{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "170104026_exp_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0VZV-bZhOHJ"
      },
      "source": [
        "#Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwUhMIPdBDF8"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "import pandas as pd\n",
        "import PIL\n",
        "import os\n",
        "import pickle\n",
        "from google.colab import files\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JQcyIo4hUIe"
      },
      "source": [
        "#Seeding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u_CAyPVB1IZ"
      },
      "source": [
        "np.random.seed(26)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INJRp8pyhZc5"
      },
      "source": [
        "#Function for loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_spSWjyB3_J"
      },
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, csv_path, images_folder, transform=torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Grayscale(num_output_channels=1),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Resize(28),\n",
        "    torchvision.transforms.Normalize(\n",
        "        mean=[0.5],\n",
        "        std=[0.5],\n",
        "    ),\n",
        "])):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.images_folder = images_folder\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        record = self.df.iloc[index]\n",
        "        path = self.images_folder + \"/\" +record[0]\n",
        "        image = PIL.Image.open(path)\n",
        "        label = record[3]\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image) \n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g9H56Aohhzf"
      },
      "source": [
        "#Function for converting tensor to image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J7ZZEUnEEMg"
      },
      "source": [
        "def tensorToImage(x):\n",
        "  trans = transforms.ToPILImage()\n",
        "  image = trans(x)\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtHh7j6dh4Dl"
      },
      "source": [
        "#Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV_KwRdNCBC6"
      },
      "source": [
        "path1 = '/content/drive/MyDrive/4.2/soft_com/Dataset/Assignment2/training-a.csv'\n",
        "path2 = '/content/drive/MyDrive/4.2/soft_com/Dataset/Assignment2/training-a'\n",
        "obj = CustomDataset(path1, path2)\n",
        "dataset = [[x[0],x[1]] for x in obj]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZXyZq_2h-yW"
      },
      "source": [
        "#Visualizing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "1a4q2HmFF1eE",
        "outputId": "7e70a59d-edc7-4ff5-fb2b-7f6bc201d95a"
      },
      "source": [
        "plt.imshow(tensorToImage(obj[0][0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fbc9ace7a50>"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHIuxo7MiE9X"
      },
      "source": [
        "#Checking the length of dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AEAdk7EWvp_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "733db898-fe1d-47c3-9d23-e0d0f5695f9d"
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19702"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5jaiTr6vO93"
      },
      "source": [
        "#Experiment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn5n65vzYa2V"
      },
      "source": [
        "#Aproach 1\n",
        "batch size = 64 <br>\n",
        "num_iters = 30000<br>\n",
        "input_dim = 1 * 28 * 28 <br>\n",
        "num_hidden = 100 <br>\n",
        "output_dim = 10 <br>\n",
        " learning_rate = 0.001 <br>\n",
        "Activation = ReLU <br>\n",
        "Criterion = Cross Entropy <br>\n",
        "Optimizar = SGD <br>\n",
        "<font color = 'red'> Accuracy = 9.79% </font><br>\n",
        "Comment: The accuracy is not increasing at all. It has been stucked in 9%, so I interrupt the training. <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bA0Q-d8xYbnO",
        "outputId": "940b1524-c522-48d1-acb2-80cfbd7783d0"
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "batch_size = 64\n",
        "num_iters = 30000\n",
        "input_dim = 1*28*28 # num_features = 784\n",
        "num_hidden = 100\n",
        "output_dim = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle= False, batch_size=batch_size)\n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "\n",
        "        self.linear_4 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_4 = nn.ReLU()\n",
        "        \n",
        "        \n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_5 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_5 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_6 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_6 = nn.ReLU()\n",
        "        \n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_4(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_4(out)\n",
        "        \n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_5(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_5(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_6(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_6(out)\n",
        "        \n",
        "\n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "iter_ = []\n",
        "loss_ = []\n",
        "accuracy_ = []\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 1*28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 100 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 1*28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "            iter_.append(iter)\n",
        "            loss_.append(loss.item())\n",
        "            accuracy_.append(accuracy)\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 100. Loss: 2.3002102375030518. Accuracy: 9.819842679522964\n",
            "Iteration: 200. Loss: 2.319030284881592. Accuracy: 9.819842679522964\n",
            "Iteration: 300. Loss: 2.3197758197784424. Accuracy: 9.819842679522964\n",
            "Iteration: 400. Loss: 2.294833183288574. Accuracy: 9.819842679522964\n",
            "Iteration: 500. Loss: 2.3092904090881348. Accuracy: 9.819842679522964\n",
            "Iteration: 600. Loss: 2.29862117767334. Accuracy: 9.819842679522964\n",
            "Iteration: 700. Loss: 2.3071072101593018. Accuracy: 9.819842679522964\n",
            "Iteration: 800. Loss: 2.319882392883301. Accuracy: 9.819842679522964\n",
            "Iteration: 900. Loss: 2.3081657886505127. Accuracy: 9.819842679522964\n",
            "Iteration: 1000. Loss: 2.3030426502227783. Accuracy: 9.819842679522964\n",
            "Iteration: 1100. Loss: 2.309328079223633. Accuracy: 9.819842679522964\n",
            "Iteration: 1200. Loss: 2.302061080932617. Accuracy: 9.819842679522964\n",
            "Iteration: 1300. Loss: 2.313614845275879. Accuracy: 9.819842679522964\n",
            "Iteration: 1400. Loss: 2.3047633171081543. Accuracy: 9.819842679522964\n",
            "Iteration: 1500. Loss: 2.3030173778533936. Accuracy: 9.819842679522964\n",
            "Iteration: 1600. Loss: 2.3058152198791504. Accuracy: 9.819842679522964\n",
            "Iteration: 1700. Loss: 2.2999818325042725. Accuracy: 9.819842679522964\n",
            "Iteration: 1800. Loss: 2.301692485809326. Accuracy: 9.819842679522964\n",
            "Iteration: 1900. Loss: 2.3036417961120605. Accuracy: 9.819842679522964\n",
            "Iteration: 2000. Loss: 2.3064842224121094. Accuracy: 9.819842679522964\n",
            "Iteration: 2100. Loss: 2.2971980571746826. Accuracy: 9.819842679522964\n",
            "Iteration: 2200. Loss: 2.3157405853271484. Accuracy: 9.819842679522964\n",
            "Iteration: 2300. Loss: 2.2986645698547363. Accuracy: 9.819842679522964\n",
            "Iteration: 2400. Loss: 2.302644729614258. Accuracy: 9.819842679522964\n",
            "Iteration: 2500. Loss: 2.3040709495544434. Accuracy: 9.819842679522964\n",
            "Iteration: 2600. Loss: 2.2986769676208496. Accuracy: 9.819842679522964\n",
            "Iteration: 2700. Loss: 2.302506685256958. Accuracy: 9.819842679522964\n",
            "Iteration: 2800. Loss: 2.3046021461486816. Accuracy: 9.819842679522964\n",
            "Iteration: 2900. Loss: 2.302572250366211. Accuracy: 9.819842679522964\n",
            "Iteration: 3000. Loss: 2.2951133251190186. Accuracy: 9.819842679522964\n",
            "Iteration: 3100. Loss: 2.2990198135375977. Accuracy: 9.819842679522964\n",
            "Iteration: 3200. Loss: 2.302097797393799. Accuracy: 9.819842679522964\n",
            "Iteration: 3300. Loss: 2.300994634628296. Accuracy: 9.819842679522964\n",
            "Iteration: 3400. Loss: 2.307002067565918. Accuracy: 9.819842679522964\n",
            "Iteration: 3500. Loss: 2.300124406814575. Accuracy: 9.819842679522964\n",
            "Iteration: 3600. Loss: 2.2970709800720215. Accuracy: 9.819842679522964\n",
            "Iteration: 3700. Loss: 2.3027024269104004. Accuracy: 9.819842679522964\n",
            "Iteration: 3800. Loss: 2.3012044429779053. Accuracy: 9.819842679522964\n",
            "Iteration: 3900. Loss: 2.3005988597869873. Accuracy: 9.819842679522964\n",
            "Iteration: 4000. Loss: 2.3067517280578613. Accuracy: 9.819842679522964\n",
            "Iteration: 4100. Loss: 2.3048176765441895. Accuracy: 9.819842679522964\n",
            "Iteration: 4200. Loss: 2.304798126220703. Accuracy: 9.819842679522964\n",
            "Iteration: 4300. Loss: 2.303614854812622. Accuracy: 9.819842679522964\n",
            "Iteration: 4400. Loss: 2.309988021850586. Accuracy: 9.819842679522964\n",
            "Iteration: 4500. Loss: 2.303898572921753. Accuracy: 9.819842679522964\n",
            "Iteration: 4600. Loss: 2.300748348236084. Accuracy: 9.819842679522964\n",
            "Iteration: 4700. Loss: 2.2976205348968506. Accuracy: 9.819842679522964\n",
            "Iteration: 4800. Loss: 2.309469223022461. Accuracy: 9.819842679522964\n",
            "Iteration: 4900. Loss: 2.3065497875213623. Accuracy: 9.819842679522964\n",
            "Iteration: 5000. Loss: 2.3012239933013916. Accuracy: 9.819842679522964\n",
            "Iteration: 5100. Loss: 2.3044028282165527. Accuracy: 9.819842679522964\n",
            "Iteration: 5200. Loss: 2.3011391162872314. Accuracy: 9.819842679522964\n",
            "Iteration: 5300. Loss: 2.3062095642089844. Accuracy: 9.819842679522964\n",
            "Iteration: 5400. Loss: 2.2953834533691406. Accuracy: 9.819842679522964\n",
            "Iteration: 5500. Loss: 2.3018712997436523. Accuracy: 9.819842679522964\n",
            "Iteration: 5600. Loss: 2.2994515895843506. Accuracy: 9.819842679522964\n",
            "Iteration: 5700. Loss: 2.3037993907928467. Accuracy: 9.819842679522964\n",
            "Iteration: 5800. Loss: 2.307191848754883. Accuracy: 9.819842679522964\n",
            "Iteration: 5900. Loss: 2.3041744232177734. Accuracy: 9.819842679522964\n",
            "Iteration: 6000. Loss: 2.3071484565734863. Accuracy: 9.819842679522964\n",
            "Iteration: 6100. Loss: 2.2998759746551514. Accuracy: 9.819842679522964\n",
            "Iteration: 6200. Loss: 2.304964542388916. Accuracy: 9.819842679522964\n",
            "Iteration: 6300. Loss: 2.306450366973877. Accuracy: 9.819842679522964\n",
            "Iteration: 6400. Loss: 2.2949371337890625. Accuracy: 9.819842679522964\n",
            "Iteration: 6500. Loss: 2.302269697189331. Accuracy: 9.819842679522964\n",
            "Iteration: 6600. Loss: 2.302905559539795. Accuracy: 9.819842679522964\n",
            "Iteration: 6700. Loss: 2.3021533489227295. Accuracy: 9.819842679522964\n",
            "Iteration: 6800. Loss: 2.3056745529174805. Accuracy: 9.819842679522964\n",
            "Iteration: 6900. Loss: 2.307168483734131. Accuracy: 9.819842679522964\n",
            "Iteration: 7000. Loss: 2.3006904125213623. Accuracy: 9.819842679522964\n",
            "Iteration: 7100. Loss: 2.301284074783325. Accuracy: 9.819842679522964\n",
            "Iteration: 7200. Loss: 2.3018486499786377. Accuracy: 9.819842679522964\n",
            "Iteration: 7300. Loss: 2.2998592853546143. Accuracy: 9.819842679522964\n",
            "Iteration: 7400. Loss: 2.3060641288757324. Accuracy: 9.819842679522964\n",
            "Iteration: 7500. Loss: 2.301295042037964. Accuracy: 9.819842679522964\n",
            "Iteration: 7600. Loss: 2.306520938873291. Accuracy: 9.819842679522964\n",
            "Iteration: 7700. Loss: 2.3057429790496826. Accuracy: 9.819842679522964\n",
            "Iteration: 7800. Loss: 2.3014578819274902. Accuracy: 9.819842679522964\n",
            "Iteration: 7900. Loss: 2.3006439208984375. Accuracy: 9.819842679522964\n",
            "Iteration: 8000. Loss: 2.303419828414917. Accuracy: 9.819842679522964\n",
            "Iteration: 8100. Loss: 2.3055379390716553. Accuracy: 9.819842679522964\n",
            "Iteration: 8200. Loss: 2.3027961254119873. Accuracy: 9.819842679522964\n",
            "Iteration: 8300. Loss: 2.3041093349456787. Accuracy: 9.819842679522964\n",
            "Iteration: 8400. Loss: 2.3035974502563477. Accuracy: 9.819842679522964\n",
            "Iteration: 8500. Loss: 2.306110382080078. Accuracy: 9.819842679522964\n",
            "Iteration: 8600. Loss: 2.304138660430908. Accuracy: 9.819842679522964\n",
            "Iteration: 8700. Loss: 2.3035905361175537. Accuracy: 9.819842679522964\n",
            "Iteration: 8800. Loss: 2.302339553833008. Accuracy: 9.819842679522964\n",
            "Iteration: 8900. Loss: 2.3000826835632324. Accuracy: 9.819842679522964\n",
            "Iteration: 9000. Loss: 2.305535078048706. Accuracy: 9.819842679522964\n",
            "Iteration: 9100. Loss: 2.30208683013916. Accuracy: 9.819842679522964\n",
            "Iteration: 9200. Loss: 2.294058322906494. Accuracy: 9.819842679522964\n",
            "Iteration: 9300. Loss: 2.3040380477905273. Accuracy: 9.819842679522964\n",
            "Iteration: 9400. Loss: 2.3043947219848633. Accuracy: 9.819842679522964\n",
            "Iteration: 9500. Loss: 2.306036949157715. Accuracy: 9.819842679522964\n",
            "Iteration: 9600. Loss: 2.3010125160217285. Accuracy: 9.819842679522964\n",
            "Iteration: 9700. Loss: 2.2977116107940674. Accuracy: 9.819842679522964\n",
            "Iteration: 9800. Loss: 2.2981317043304443. Accuracy: 9.819842679522964\n",
            "Iteration: 9900. Loss: 2.305591583251953. Accuracy: 9.819842679522964\n",
            "Iteration: 10000. Loss: 2.304238796234131. Accuracy: 9.819842679522964\n",
            "Iteration: 10100. Loss: 2.3040926456451416. Accuracy: 9.819842679522964\n",
            "Iteration: 10200. Loss: 2.304046392440796. Accuracy: 9.819842679522964\n",
            "Iteration: 10300. Loss: 2.2992827892303467. Accuracy: 9.819842679522964\n",
            "Iteration: 10400. Loss: 2.304248332977295. Accuracy: 9.819842679522964\n",
            "Iteration: 10500. Loss: 2.3043930530548096. Accuracy: 9.819842679522964\n",
            "Iteration: 10600. Loss: 2.30472993850708. Accuracy: 9.819842679522964\n",
            "Iteration: 10700. Loss: 2.302748441696167. Accuracy: 9.819842679522964\n",
            "Iteration: 10800. Loss: 2.303622245788574. Accuracy: 9.819842679522964\n",
            "Iteration: 10900. Loss: 2.3021254539489746. Accuracy: 9.819842679522964\n",
            "Iteration: 11000. Loss: 2.305774450302124. Accuracy: 9.819842679522964\n",
            "Iteration: 11100. Loss: 2.3016061782836914. Accuracy: 9.819842679522964\n",
            "Iteration: 11200. Loss: 2.3027539253234863. Accuracy: 9.819842679522964\n",
            "Iteration: 11300. Loss: 2.3029205799102783. Accuracy: 9.819842679522964\n",
            "Iteration: 11400. Loss: 2.303278684616089. Accuracy: 9.819842679522964\n",
            "Iteration: 11500. Loss: 2.302615165710449. Accuracy: 9.819842679522964\n",
            "Iteration: 11600. Loss: 2.302807569503784. Accuracy: 9.819842679522964\n",
            "Iteration: 11700. Loss: 2.3024771213531494. Accuracy: 9.819842679522964\n",
            "Iteration: 11800. Loss: 2.3038742542266846. Accuracy: 9.819842679522964\n",
            "Iteration: 11900. Loss: 2.302769184112549. Accuracy: 9.819842679522964\n",
            "Iteration: 12000. Loss: 2.301645517349243. Accuracy: 9.819842679522964\n",
            "Iteration: 12100. Loss: 2.3002431392669678. Accuracy: 9.819842679522964\n",
            "Iteration: 12200. Loss: 2.3009119033813477. Accuracy: 9.819842679522964\n",
            "Iteration: 12300. Loss: 2.304123878479004. Accuracy: 9.819842679522964\n",
            "Iteration: 12400. Loss: 2.3000800609588623. Accuracy: 9.819842679522964\n",
            "Iteration: 12500. Loss: 2.299609661102295. Accuracy: 9.819842679522964\n",
            "Iteration: 12600. Loss: 2.304081439971924. Accuracy: 9.819842679522964\n",
            "Iteration: 12700. Loss: 2.299201726913452. Accuracy: 9.819842679522964\n",
            "Iteration: 12800. Loss: 2.303283214569092. Accuracy: 9.819842679522964\n",
            "Iteration: 12900. Loss: 2.3018901348114014. Accuracy: 9.819842679522964\n",
            "Iteration: 13000. Loss: 2.3033604621887207. Accuracy: 9.819842679522964\n",
            "Iteration: 13100. Loss: 2.303110361099243. Accuracy: 9.819842679522964\n",
            "Iteration: 13200. Loss: 2.302828073501587. Accuracy: 9.819842679522964\n",
            "Iteration: 13300. Loss: 2.302525043487549. Accuracy: 9.819842679522964\n",
            "Iteration: 13400. Loss: 2.3051788806915283. Accuracy: 9.819842679522964\n",
            "Iteration: 13500. Loss: 2.2998948097229004. Accuracy: 9.819842679522964\n",
            "Iteration: 13600. Loss: 2.303961753845215. Accuracy: 9.819842679522964\n",
            "Iteration: 13700. Loss: 2.3005483150482178. Accuracy: 9.819842679522964\n",
            "Iteration: 13800. Loss: 2.3022167682647705. Accuracy: 9.819842679522964\n",
            "Iteration: 13900. Loss: 2.30362868309021. Accuracy: 9.819842679522964\n",
            "Iteration: 14000. Loss: 2.302858829498291. Accuracy: 9.819842679522964\n",
            "Iteration: 14100. Loss: 2.300319194793701. Accuracy: 9.819842679522964\n",
            "Iteration: 14200. Loss: 2.305307388305664. Accuracy: 9.819842679522964\n",
            "Iteration: 14300. Loss: 2.302607297897339. Accuracy: 9.819842679522964\n",
            "Iteration: 14400. Loss: 2.304366111755371. Accuracy: 9.819842679522964\n",
            "Iteration: 14500. Loss: 2.3042471408843994. Accuracy: 9.819842679522964\n",
            "Iteration: 14600. Loss: 2.3004188537597656. Accuracy: 9.819842679522964\n",
            "Iteration: 14700. Loss: 2.301500082015991. Accuracy: 9.819842679522964\n",
            "Iteration: 14800. Loss: 2.3010919094085693. Accuracy: 9.819842679522964\n",
            "Iteration: 14900. Loss: 2.3004486560821533. Accuracy: 9.819842679522964\n",
            "Iteration: 15000. Loss: 2.303818702697754. Accuracy: 9.819842679522964\n",
            "Iteration: 15100. Loss: 2.30043625831604. Accuracy: 9.819842679522964\n",
            "Iteration: 15200. Loss: 2.30005145072937. Accuracy: 9.819842679522964\n",
            "Iteration: 15300. Loss: 2.3029119968414307. Accuracy: 9.819842679522964\n",
            "Iteration: 15400. Loss: 2.3014729022979736. Accuracy: 9.819842679522964\n",
            "Iteration: 15500. Loss: 2.302053451538086. Accuracy: 9.819842679522964\n",
            "Iteration: 15600. Loss: 2.2998881340026855. Accuracy: 9.819842679522964\n",
            "Iteration: 15700. Loss: 2.3028507232666016. Accuracy: 9.819842679522964\n",
            "Iteration: 15800. Loss: 2.301243305206299. Accuracy: 9.819842679522964\n",
            "Iteration: 15900. Loss: 2.3040010929107666. Accuracy: 9.819842679522964\n",
            "Iteration: 16000. Loss: 2.3014824390411377. Accuracy: 9.819842679522964\n",
            "Iteration: 16100. Loss: 2.3009774684906006. Accuracy: 9.819842679522964\n",
            "Iteration: 16200. Loss: 2.303302526473999. Accuracy: 9.819842679522964\n",
            "Iteration: 16300. Loss: 2.301511287689209. Accuracy: 9.819842679522964\n",
            "Iteration: 16400. Loss: 2.3018078804016113. Accuracy: 9.819842679522964\n",
            "Iteration: 16500. Loss: 2.303565740585327. Accuracy: 9.819842679522964\n",
            "Iteration: 16600. Loss: 2.3024203777313232. Accuracy: 9.819842679522964\n",
            "Iteration: 16700. Loss: 2.3029890060424805. Accuracy: 9.819842679522964\n",
            "Iteration: 16800. Loss: 2.300464391708374. Accuracy: 9.819842679522964\n",
            "Iteration: 16900. Loss: 2.3029823303222656. Accuracy: 9.819842679522964\n",
            "Iteration: 17000. Loss: 2.302967071533203. Accuracy: 9.819842679522964\n",
            "Iteration: 17100. Loss: 2.300933599472046. Accuracy: 9.819842679522964\n",
            "Iteration: 17200. Loss: 2.3014798164367676. Accuracy: 9.819842679522964\n",
            "Iteration: 17300. Loss: 2.3040874004364014. Accuracy: 9.819842679522964\n",
            "Iteration: 17400. Loss: 2.301513195037842. Accuracy: 9.819842679522964\n",
            "Iteration: 17500. Loss: 2.302400588989258. Accuracy: 9.819842679522964\n",
            "Iteration: 17600. Loss: 2.302874803543091. Accuracy: 8.83024613042375\n",
            "Iteration: 17700. Loss: 2.3023784160614014. Accuracy: 8.703374777975133\n",
            "Iteration: 17800. Loss: 2.3031058311462402. Accuracy: 9.79446840903324\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-ed208216bdb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# Forward pass to get output/logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# Calculate Loss: softmax --> cross entropy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-ed208216bdb6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m### 3rd hidden layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mout\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;31m### Non-linearity in 3rd hidden layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1115\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_full_backward_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehY7gcORZnLP"
      },
      "source": [
        "#Aproach 2\n",
        "batch size = 64<br>\n",
        "num_iters = 30000<br>\n",
        "input_dim = 1 * 28 * 28 <br>\n",
        "num_hidden = 100 <br>\n",
        "output_dim = 10 <br>\n",
        "<font color = 'tiffani blue'> learning_rate = 0.01 </font><br>\n",
        "Activation = ReLU <br>\n",
        "Criterion = Cross Entropy <br>\n",
        "Optimizar = SGD <br>\n",
        "<font color = 'orange'> Accuracy = 24.89% </font><br>\n",
        "Comment: I have increased the learning rate. The accuracy is better than the previous one but this accuracy is not acceptable. <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOHVnMEeY0tJ",
        "outputId": "25d875e1-2e16-4330-dcf2-9370e1416fa4"
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "batch_size = 64\n",
        "num_iters = 30000\n",
        "input_dim = 1*28*28 # num_features = 784\n",
        "num_hidden = 100\n",
        "output_dim = 10\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle= False, batch_size=batch_size)\n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "\n",
        "        self.linear_4 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_4 = nn.ReLU()\n",
        "        \n",
        "        \n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_5 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_5 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_6 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_6 = nn.ReLU()\n",
        "        \n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_4(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_4(out)\n",
        "        \n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_5(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_5(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_6(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_6(out)\n",
        "        \n",
        "\n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "iter_ = []\n",
        "loss_ = []\n",
        "accuracy_ = []\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 1*28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 100 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 1*28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "            iter_.append(iter)\n",
        "            loss_.append(loss.item())\n",
        "            accuracy_.append(accuracy)\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 100. Loss: 2.3033573627471924. Accuracy: 10.175082466379092\n",
            "Iteration: 200. Loss: 2.2982337474823. Accuracy: 10.175082466379092\n",
            "Iteration: 300. Loss: 2.3028087615966797. Accuracy: 10.175082466379092\n",
            "Iteration: 400. Loss: 2.308992385864258. Accuracy: 10.175082466379092\n",
            "Iteration: 500. Loss: 2.3003430366516113. Accuracy: 12.890129408779497\n",
            "Iteration: 600. Loss: 2.3085360527038574. Accuracy: 8.67800050748541\n",
            "Iteration: 700. Loss: 2.305554151535034. Accuracy: 8.67800050748541\n",
            "Iteration: 800. Loss: 2.307187080383301. Accuracy: 8.67800050748541\n",
            "Iteration: 900. Loss: 2.3025152683258057. Accuracy: 8.67800050748541\n",
            "Iteration: 1000. Loss: 2.301551103591919. Accuracy: 8.67800050748541\n",
            "Iteration: 1100. Loss: 2.3030285835266113. Accuracy: 11.037807663029687\n",
            "Iteration: 1200. Loss: 2.3052635192871094. Accuracy: 8.67800050748541\n",
            "Iteration: 1300. Loss: 2.302161931991577. Accuracy: 8.67800050748541\n",
            "Iteration: 1400. Loss: 2.300096273422241. Accuracy: 8.67800050748541\n",
            "Iteration: 1500. Loss: 2.3007025718688965. Accuracy: 8.67800050748541\n",
            "Iteration: 1600. Loss: 2.3043782711029053. Accuracy: 8.67800050748541\n",
            "Iteration: 1700. Loss: 2.3017358779907227. Accuracy: 8.67800050748541\n",
            "Iteration: 1800. Loss: 2.3060362339019775. Accuracy: 8.67800050748541\n",
            "Iteration: 1900. Loss: 2.3041112422943115. Accuracy: 8.67800050748541\n",
            "Iteration: 2000. Loss: 2.302583932876587. Accuracy: 8.67800050748541\n",
            "Iteration: 2100. Loss: 2.302396535873413. Accuracy: 8.67800050748541\n",
            "Iteration: 2200. Loss: 2.3045828342437744. Accuracy: 8.67800050748541\n",
            "Iteration: 2300. Loss: 2.3014957904815674. Accuracy: 8.67800050748541\n",
            "Iteration: 2400. Loss: 2.300825357437134. Accuracy: 8.67800050748541\n",
            "Iteration: 2500. Loss: 2.304016590118408. Accuracy: 8.67800050748541\n",
            "Iteration: 2600. Loss: 2.299708604812622. Accuracy: 8.67800050748541\n",
            "Iteration: 2700. Loss: 2.3005244731903076. Accuracy: 8.67800050748541\n",
            "Iteration: 2800. Loss: 2.300380229949951. Accuracy: 8.67800050748541\n",
            "Iteration: 2900. Loss: 2.2999267578125. Accuracy: 8.67800050748541\n",
            "Iteration: 3000. Loss: 2.3023195266723633. Accuracy: 8.67800050748541\n",
            "Iteration: 3100. Loss: 2.3006577491760254. Accuracy: 8.67800050748541\n",
            "Iteration: 3200. Loss: 2.302530288696289. Accuracy: 8.67800050748541\n",
            "Iteration: 3300. Loss: 2.3000829219818115. Accuracy: 8.67800050748541\n",
            "Iteration: 3400. Loss: 2.302232503890991. Accuracy: 8.67800050748541\n",
            "Iteration: 3500. Loss: 2.300910234451294. Accuracy: 8.67800050748541\n",
            "Iteration: 3600. Loss: 2.303847074508667. Accuracy: 8.67800050748541\n",
            "Iteration: 3700. Loss: 2.3054862022399902. Accuracy: 8.67800050748541\n",
            "Iteration: 3800. Loss: 2.3025166988372803. Accuracy: 8.67800050748541\n",
            "Iteration: 3900. Loss: 2.299644708633423. Accuracy: 8.67800050748541\n",
            "Iteration: 4000. Loss: 2.302320957183838. Accuracy: 8.67800050748541\n",
            "Iteration: 4100. Loss: 2.3053667545318604. Accuracy: 8.67800050748541\n",
            "Iteration: 4200. Loss: 2.3045296669006348. Accuracy: 8.67800050748541\n",
            "Iteration: 4300. Loss: 2.301710844039917. Accuracy: 8.67800050748541\n",
            "Iteration: 4400. Loss: 2.303950786590576. Accuracy: 8.67800050748541\n",
            "Iteration: 4500. Loss: 2.301612615585327. Accuracy: 8.67800050748541\n",
            "Iteration: 4600. Loss: 2.300919532775879. Accuracy: 8.67800050748541\n",
            "Iteration: 4700. Loss: 2.3026857376098633. Accuracy: 8.67800050748541\n",
            "Iteration: 4800. Loss: 2.30033540725708. Accuracy: 8.67800050748541\n",
            "Iteration: 4900. Loss: 2.3042314052581787. Accuracy: 8.67800050748541\n",
            "Iteration: 5000. Loss: 2.3036324977874756. Accuracy: 8.67800050748541\n",
            "Iteration: 5100. Loss: 2.2987096309661865. Accuracy: 8.67800050748541\n",
            "Iteration: 5200. Loss: 2.3020877838134766. Accuracy: 8.67800050748541\n",
            "Iteration: 5300. Loss: 2.300645112991333. Accuracy: 8.67800050748541\n",
            "Iteration: 5400. Loss: 2.3039512634277344. Accuracy: 8.67800050748541\n",
            "Iteration: 5500. Loss: 2.301276683807373. Accuracy: 8.67800050748541\n",
            "Iteration: 5600. Loss: 2.302142381668091. Accuracy: 8.67800050748541\n",
            "Iteration: 5700. Loss: 2.300795555114746. Accuracy: 8.67800050748541\n",
            "Iteration: 5800. Loss: 2.301609516143799. Accuracy: 8.67800050748541\n",
            "Iteration: 5900. Loss: 2.3025765419006348. Accuracy: 8.67800050748541\n",
            "Iteration: 6000. Loss: 2.3036608695983887. Accuracy: 8.67800050748541\n",
            "Iteration: 6100. Loss: 2.2995193004608154. Accuracy: 8.67800050748541\n",
            "Iteration: 6200. Loss: 2.30035138130188. Accuracy: 8.67800050748541\n",
            "Iteration: 6300. Loss: 2.299797773361206. Accuracy: 8.67800050748541\n",
            "Iteration: 6400. Loss: 2.2994256019592285. Accuracy: 8.67800050748541\n",
            "Iteration: 6500. Loss: 2.3022894859313965. Accuracy: 8.67800050748541\n",
            "Iteration: 6600. Loss: 2.3022000789642334. Accuracy: 8.67800050748541\n",
            "Iteration: 6700. Loss: 2.3020589351654053. Accuracy: 8.67800050748541\n",
            "Iteration: 6800. Loss: 2.3020482063293457. Accuracy: 8.67800050748541\n",
            "Iteration: 6900. Loss: 2.301807165145874. Accuracy: 8.67800050748541\n",
            "Iteration: 7000. Loss: 2.302424907684326. Accuracy: 8.67800050748541\n",
            "Iteration: 7100. Loss: 2.299060583114624. Accuracy: 8.67800050748541\n",
            "Iteration: 7200. Loss: 2.302666664123535. Accuracy: 8.67800050748541\n",
            "Iteration: 7300. Loss: 2.299884557723999. Accuracy: 8.67800050748541\n",
            "Iteration: 7400. Loss: 2.3025565147399902. Accuracy: 8.67800050748541\n",
            "Iteration: 7500. Loss: 2.300604820251465. Accuracy: 8.67800050748541\n",
            "Iteration: 7600. Loss: 2.304185390472412. Accuracy: 8.67800050748541\n",
            "Iteration: 7700. Loss: 2.3021836280822754. Accuracy: 8.67800050748541\n",
            "Iteration: 7800. Loss: 2.298920154571533. Accuracy: 8.779497589444304\n",
            "Iteration: 7900. Loss: 2.3011515140533447. Accuracy: 8.67800050748541\n",
            "Iteration: 8000. Loss: 2.30257511138916. Accuracy: 8.67800050748541\n",
            "Iteration: 8100. Loss: 2.2992310523986816. Accuracy: 8.67800050748541\n",
            "Iteration: 8200. Loss: 2.2979202270507812. Accuracy: 8.67800050748541\n",
            "Iteration: 8300. Loss: 2.301492691040039. Accuracy: 8.67800050748541\n",
            "Iteration: 8400. Loss: 2.299341917037964. Accuracy: 8.728749048464856\n",
            "Iteration: 8500. Loss: 2.29738187789917. Accuracy: 8.67800050748541\n",
            "Iteration: 8600. Loss: 2.2987048625946045. Accuracy: 8.67800050748541\n",
            "Iteration: 8700. Loss: 2.298781156539917. Accuracy: 8.728749048464856\n",
            "Iteration: 8800. Loss: 2.298650026321411. Accuracy: 10.910936310581071\n",
            "Iteration: 8900. Loss: 2.298356056213379. Accuracy: 8.67800050748541\n",
            "Iteration: 9000. Loss: 2.2981491088867188. Accuracy: 8.67800050748541\n",
            "Iteration: 9100. Loss: 2.295779228210449. Accuracy: 8.67800050748541\n",
            "Iteration: 9200. Loss: 2.2933526039123535. Accuracy: 8.67800050748541\n",
            "Iteration: 9300. Loss: 2.2995898723602295. Accuracy: 8.75412331895458\n",
            "Iteration: 9400. Loss: 2.295830249786377. Accuracy: 10.479573712255773\n",
            "Iteration: 9500. Loss: 2.297837495803833. Accuracy: 9.616848515605176\n",
            "Iteration: 9600. Loss: 2.2993924617767334. Accuracy: 10.733316417153008\n",
            "Iteration: 9700. Loss: 2.292426824569702. Accuracy: 9.007866023851815\n",
            "Iteration: 9800. Loss: 2.295461654663086. Accuracy: 9.692971327074346\n",
            "Iteration: 9900. Loss: 2.2997336387634277. Accuracy: 10.37807663029688\n",
            "Iteration: 10000. Loss: 2.2914044857025146. Accuracy: 10.910936310581071\n",
            "Iteration: 10100. Loss: 2.2959976196289062. Accuracy: 12.915503679269221\n",
            "Iteration: 10200. Loss: 2.294532537460327. Accuracy: 9.6422227860949\n",
            "Iteration: 10300. Loss: 2.2921142578125. Accuracy: 16.163410301953817\n",
            "Iteration: 10400. Loss: 2.2907752990722656. Accuracy: 11.367673179396093\n",
            "Iteration: 10500. Loss: 2.2896556854248047. Accuracy: 12.255772646536412\n",
            "Iteration: 10600. Loss: 2.294233560562134. Accuracy: 13.118497843187008\n",
            "Iteration: 10700. Loss: 2.2950069904327393. Accuracy: 14.76782542501903\n",
            "Iteration: 10800. Loss: 2.288961887359619. Accuracy: 14.76782542501903\n",
            "Iteration: 10900. Loss: 2.28426456451416. Accuracy: 14.717076884039583\n",
            "Iteration: 11000. Loss: 2.284940481185913. Accuracy: 14.996193859426542\n",
            "Iteration: 11100. Loss: 2.2865982055664062. Accuracy: 14.53945699061152\n",
            "Iteration: 11200. Loss: 2.2775561809539795. Accuracy: 15.935041867546309\n",
            "Iteration: 11300. Loss: 2.277301788330078. Accuracy: 16.01116467901548\n",
            "Iteration: 11400. Loss: 2.2746007442474365. Accuracy: 17.43212382643999\n",
            "Iteration: 11500. Loss: 2.2729194164276123. Accuracy: 15.2753108348135\n",
            "Iteration: 11600. Loss: 2.2783498764038086. Accuracy: 17.20375539203248\n",
            "Iteration: 11700. Loss: 2.2712674140930176. Accuracy: 17.35600101497082\n",
            "Iteration: 11800. Loss: 2.2576041221618652. Accuracy: 19.2844455721898\n",
            "Iteration: 11900. Loss: 2.262136459350586. Accuracy: 15.884293326566862\n",
            "Iteration: 12000. Loss: 2.2439370155334473. Accuracy: 17.102258310073584\n",
            "Iteration: 12100. Loss: 2.260699510574341. Accuracy: 17.00076122811469\n",
            "Iteration: 12200. Loss: 2.2478270530700684. Accuracy: 16.848515605176352\n",
            "Iteration: 12300. Loss: 2.224209785461426. Accuracy: 18.573965998477544\n",
            "Iteration: 12400. Loss: 2.193028211593628. Accuracy: 18.624714539456992\n",
            "Iteration: 12500. Loss: 2.1915273666381836. Accuracy: 18.70083735092616\n",
            "Iteration: 12600. Loss: 2.2287981510162354. Accuracy: 16.92463841664552\n",
            "Iteration: 12700. Loss: 2.1627187728881836. Accuracy: 16.290281654402435\n",
            "Iteration: 12800. Loss: 2.1590287685394287. Accuracy: 19.538188277087034\n",
            "Iteration: 12900. Loss: 2.0483782291412354. Accuracy: 15.757421974118245\n",
            "Iteration: 13000. Loss: 2.1609532833099365. Accuracy: 20.147170768840397\n",
            "Iteration: 13100. Loss: 2.1460816860198975. Accuracy: 18.726211621415885\n",
            "Iteration: 13200. Loss: 2.096618413925171. Accuracy: 18.19335194113169\n",
            "Iteration: 13300. Loss: 2.1279733180999756. Accuracy: 17.609743719868053\n",
            "Iteration: 13400. Loss: 2.1282150745391846. Accuracy: 16.518650088809945\n",
            "Iteration: 13500. Loss: 2.0580995082855225. Accuracy: 21.21289012940878\n",
            "Iteration: 13600. Loss: 2.088099956512451. Accuracy: 19.51281400659731\n",
            "Iteration: 13700. Loss: 2.052385091781616. Accuracy: 17.076884039583863\n",
            "Iteration: 13800. Loss: 2.132230758666992. Accuracy: 17.102258310073584\n",
            "Iteration: 13900. Loss: 2.303859233856201. Accuracy: 9.819842679522964\n",
            "Iteration: 14000. Loss: 2.24225115776062. Accuracy: 11.900532859680284\n",
            "Iteration: 14100. Loss: 2.117387056350708. Accuracy: 18.370971834559757\n",
            "Iteration: 14200. Loss: 2.024738073348999. Accuracy: 22.55772646536412\n",
            "Iteration: 14300. Loss: 1.9914888143539429. Accuracy: 19.259071301700075\n",
            "Iteration: 14400. Loss: 2.2006912231445312. Accuracy: 11.088556204009135\n",
            "Iteration: 14500. Loss: 1.9750360250473022. Accuracy: 24.054808424257804\n",
            "Iteration: 14600. Loss: 2.296964168548584. Accuracy: 13.321492007104796\n",
            "Iteration: 14700. Loss: 2.3680551052093506. Accuracy: 10.784064958132454\n",
            "Iteration: 14800. Loss: 2.02316951751709. Accuracy: 21.847246891651864\n",
            "Iteration: 14900. Loss: 2.1074576377868652. Accuracy: 9.972088302461303\n",
            "Iteration: 15000. Loss: 2.0608766078948975. Accuracy: 21.6696269982238\n",
            "Iteration: 15100. Loss: 1.954821228981018. Accuracy: 19.05607713778229\n",
            "Iteration: 15200. Loss: 2.109276533126831. Accuracy: 17.20375539203248\n",
            "Iteration: 15300. Loss: 1.961036205291748. Accuracy: 23.67419436691195\n",
            "Iteration: 15400. Loss: 2.0310604572296143. Accuracy: 21.94874397361076\n",
            "Iteration: 15500. Loss: 2.004424810409546. Accuracy: 18.599340268967268\n",
            "Iteration: 15600. Loss: 2.583641529083252. Accuracy: 15.681299162649074\n",
            "Iteration: 15700. Loss: 2.045548915863037. Accuracy: 11.41842172037554\n",
            "Iteration: 15800. Loss: 1.9593186378479004. Accuracy: 21.97411824410048\n",
            "Iteration: 15900. Loss: 2.1956570148468018. Accuracy: 10.225831007358538\n",
            "Iteration: 16000. Loss: 1.9431869983673096. Accuracy: 18.117229129662523\n",
            "Iteration: 16100. Loss: 1.9579966068267822. Accuracy: 22.151738137528547\n",
            "Iteration: 16200. Loss: 2.364926815032959. Accuracy: 20.578533367165694\n",
            "Iteration: 16300. Loss: 2.009131669998169. Accuracy: 21.44125856381629\n",
            "Iteration: 16400. Loss: 2.1396937370300293. Accuracy: 17.00076122811469\n",
            "Iteration: 16500. Loss: 2.1577749252319336. Accuracy: 14.640954072570414\n",
            "Iteration: 16600. Loss: 2.0534913539886475. Accuracy: 17.406749555950267\n",
            "Iteration: 16700. Loss: 1.8789221048355103. Accuracy: 21.086018776960163\n",
            "Iteration: 16800. Loss: 2.3295228481292725. Accuracy: 23.648820096422227\n",
            "Iteration: 16900. Loss: 2.001246929168701. Accuracy: 20.451662014717076\n",
            "Iteration: 17000. Loss: 2.258371591567993. Accuracy: 9.616848515605176\n",
            "Iteration: 17100. Loss: 1.9194896221160889. Accuracy: 17.863486424765288\n",
            "Iteration: 17200. Loss: 2.2970831394195557. Accuracy: 14.742451154529308\n",
            "Iteration: 17300. Loss: 2.2933008670806885. Accuracy: 9.6422227860949\n",
            "Iteration: 17400. Loss: 2.113105535507202. Accuracy: 20.806901801573204\n",
            "Iteration: 17500. Loss: 2.108919143676758. Accuracy: 23.598071555442782\n",
            "Iteration: 17600. Loss: 2.016195774078369. Accuracy: 22.78609489977163\n",
            "Iteration: 17700. Loss: 2.1409144401550293. Accuracy: 23.851814260340014\n",
            "Iteration: 17800. Loss: 2.0135507583618164. Accuracy: 18.421720375539202\n",
            "Iteration: 17900. Loss: 1.8839912414550781. Accuracy: 21.872621162141588\n",
            "Iteration: 18000. Loss: 2.2818503379821777. Accuracy: 18.903831514843947\n",
            "Iteration: 18100. Loss: 2.0059101581573486. Accuracy: 12.712509515351433\n",
            "Iteration: 18200. Loss: 2.4494221210479736. Accuracy: 14.945445318447094\n",
            "Iteration: 18300. Loss: 2.2910444736480713. Accuracy: 17.45749809692971\n",
            "Iteration: 18400. Loss: 2.4968106746673584. Accuracy: 10.809439228622177\n",
            "Iteration: 18500. Loss: 2.265267848968506. Accuracy: 10.555696523724944\n",
            "Iteration: 18600. Loss: 2.031982421875. Accuracy: 21.162141588429332\n",
            "Iteration: 18700. Loss: 1.9331836700439453. Accuracy: 17.153006851053032\n",
            "Iteration: 18800. Loss: 2.215517997741699. Accuracy: 22.075615326059374\n",
            "Iteration: 18900. Loss: 1.975915789604187. Accuracy: 21.339761481857398\n",
            "Iteration: 19000. Loss: 2.315967082977295. Accuracy: 9.6422227860949\n",
            "Iteration: 19100. Loss: 2.1188769340515137. Accuracy: 22.684597817812737\n",
            "Iteration: 19200. Loss: 2.0026211738586426. Accuracy: 16.417153006851052\n",
            "Iteration: 19300. Loss: 2.1014134883880615. Accuracy: 21.415884293326567\n",
            "Iteration: 19400. Loss: 2.1455020904541016. Accuracy: 24.080182694747528\n",
            "Iteration: 19500. Loss: 2.039604902267456. Accuracy: 24.25780258817559\n",
            "Iteration: 19600. Loss: 1.9807238578796387. Accuracy: 21.111393047449887\n",
            "Iteration: 19700. Loss: 2.0121288299560547. Accuracy: 13.778228875919817\n",
            "Iteration: 19800. Loss: 2.0465335845947266. Accuracy: 13.27074346612535\n",
            "Iteration: 19900. Loss: 1.9180442094802856. Accuracy: 21.79649835067242\n",
            "Iteration: 20000. Loss: 1.9650505781173706. Accuracy: 9.972088302461303\n",
            "Iteration: 20100. Loss: 1.9340885877609253. Accuracy: 23.699568637401676\n",
            "Iteration: 20200. Loss: 2.0524840354919434. Accuracy: 9.895965490992134\n",
            "Iteration: 20300. Loss: 1.9680708646774292. Accuracy: 21.035270235980715\n",
            "Iteration: 20400. Loss: 1.961922526359558. Accuracy: 14.793199695508754\n",
            "Iteration: 20500. Loss: 2.1168553829193115. Accuracy: 18.32022329358031\n",
            "Iteration: 20600. Loss: 1.989066481590271. Accuracy: 16.0365389495052\n",
            "Iteration: 20700. Loss: 1.9904239177703857. Accuracy: 20.502410555696525\n",
            "Iteration: 20800. Loss: 2.1107800006866455. Accuracy: 10.073585384420198\n",
            "Iteration: 20900. Loss: 2.07987642288208. Accuracy: 13.955848769347881\n",
            "Iteration: 21000. Loss: 2.53511118888855. Accuracy: 11.063181933519411\n",
            "Iteration: 21100. Loss: 2.0699098110198975. Accuracy: 23.496574473483886\n",
            "Iteration: 21200. Loss: 2.5657734870910645. Accuracy: 9.591474245115453\n",
            "Iteration: 21300. Loss: 2.0431361198425293. Accuracy: 14.945445318447094\n",
            "Iteration: 21400. Loss: 2.306577682495117. Accuracy: 15.072316670895711\n",
            "Iteration: 21500. Loss: 2.0040156841278076. Accuracy: 20.654656178634863\n",
            "Iteration: 21600. Loss: 2.0016064643859863. Accuracy: 21.086018776960163\n",
            "Iteration: 21700. Loss: 2.4336628913879395. Accuracy: 21.415884293326567\n",
            "Iteration: 21800. Loss: 1.9996981620788574. Accuracy: 21.06064450647044\n",
            "Iteration: 21900. Loss: 2.2318248748779297. Accuracy: 23.598071555442782\n",
            "Iteration: 22000. Loss: 2.0147509574890137. Accuracy: 24.866785079928952\n",
            "Iteration: 22100. Loss: 1.9294013977050781. Accuracy: 24.587668104541994\n",
            "Iteration: 22200. Loss: 2.0407021045684814. Accuracy: 21.111393047449887\n",
            "Iteration: 22300. Loss: 2.1404707431793213. Accuracy: 15.73204770362852\n",
            "Iteration: 22400. Loss: 2.214933395385742. Accuracy: 16.08728749048465\n",
            "Iteration: 22500. Loss: 2.1018102169036865. Accuracy: 11.190053285968029\n",
            "Iteration: 22600. Loss: 2.057624578475952. Accuracy: 19.2844455721898\n",
            "Iteration: 22700. Loss: 2.5384464263916016. Accuracy: 12.99162649073839\n",
            "Iteration: 22800. Loss: 1.9746097326278687. Accuracy: 23.471200202994165\n",
            "Iteration: 22900. Loss: 1.9897620677947998. Accuracy: 21.187515858919056\n",
            "Iteration: 23000. Loss: 1.9239798784255981. Accuracy: 22.30398376046689\n",
            "Iteration: 23100. Loss: 1.9640413522720337. Accuracy: 24.41004821111393\n",
            "Iteration: 23200. Loss: 2.4925851821899414. Accuracy: 10.68256787617356\n",
            "Iteration: 23300. Loss: 1.9303619861602783. Accuracy: 16.290281654402435\n",
            "Iteration: 23400. Loss: 2.1141653060913086. Accuracy: 23.06521187515859\n",
            "Iteration: 23500. Loss: 2.1635231971740723. Accuracy: 20.730778990104035\n",
            "Iteration: 23600. Loss: 2.3643698692321777. Accuracy: 24.130931235726973\n",
            "Iteration: 23700. Loss: 2.1638336181640625. Accuracy: 11.87515858919056\n",
            "Iteration: 23800. Loss: 2.4378864765167236. Accuracy: 16.214158842933266\n",
            "Iteration: 23900. Loss: 2.1546707153320312. Accuracy: 11.621415884293327\n",
            "Iteration: 24000. Loss: 2.3433308601379395. Accuracy: 21.187515858919056\n",
            "Iteration: 24100. Loss: 1.9462018013000488. Accuracy: 24.53691956356255\n",
            "Iteration: 24200. Loss: 2.052863121032715. Accuracy: 15.50367926922101\n",
            "Iteration: 24300. Loss: 2.36897611618042. Accuracy: 17.330626744481098\n",
            "Iteration: 24400. Loss: 1.8461512327194214. Accuracy: 22.5323521948744\n",
            "Iteration: 24500. Loss: 1.9932414293289185. Accuracy: 22.55772646536412\n",
            "Iteration: 24600. Loss: 2.0302882194519043. Accuracy: 20.274042121289014\n",
            "Iteration: 24700. Loss: 1.7580375671386719. Accuracy: 16.31565592489216\n",
            "Iteration: 24800. Loss: 2.2072861194610596. Accuracy: 20.806901801573204\n",
            "Iteration: 24900. Loss: 2.0623505115509033. Accuracy: 19.766556711494545\n",
            "Iteration: 25000. Loss: 2.073065757751465. Accuracy: 23.420451662014717\n",
            "Iteration: 25100. Loss: 2.233722448348999. Accuracy: 10.555696523724944\n",
            "Iteration: 25200. Loss: 1.9067939519882202. Accuracy: 12.890129408779497\n",
            "Iteration: 25300. Loss: 1.980088710784912. Accuracy: 17.330626744481098\n",
            "Iteration: 25400. Loss: 1.9625420570373535. Accuracy: 19.538188277087034\n",
            "Iteration: 25500. Loss: 2.047192335128784. Accuracy: 10.75869068764273\n",
            "Iteration: 25600. Loss: 1.926511287689209. Accuracy: 23.369703121035272\n",
            "Iteration: 25700. Loss: 2.1382641792297363. Accuracy: 24.53691956356255\n",
            "Iteration: 25800. Loss: 1.9353828430175781. Accuracy: 20.147170768840397\n",
            "Iteration: 25900. Loss: 2.1544179916381836. Accuracy: 9.616848515605176\n",
            "Iteration: 26000. Loss: 1.8984709978103638. Accuracy: 25.881755899517888\n",
            "Iteration: 26100. Loss: 1.9615840911865234. Accuracy: 20.883024613042377\n",
            "Iteration: 26200. Loss: 1.9453352689743042. Accuracy: 19.96955087541233\n",
            "Iteration: 26300. Loss: 1.966748595237732. Accuracy: 23.52194874397361\n",
            "Iteration: 26400. Loss: 2.258559226989746. Accuracy: 10.555696523724944\n",
            "Iteration: 26500. Loss: 2.004040479660034. Accuracy: 13.118497843187008\n",
            "Iteration: 26600. Loss: 1.9104433059692383. Accuracy: 17.6604922608475\n",
            "Iteration: 26700. Loss: 1.86769437789917. Accuracy: 17.50824663790916\n",
            "Iteration: 26800. Loss: 1.8926033973693848. Accuracy: 22.380106571936057\n",
            "Iteration: 26900. Loss: 1.9775714874267578. Accuracy: 17.406749555950267\n",
            "Iteration: 27000. Loss: 1.9628714323043823. Accuracy: 22.227860948997716\n",
            "Iteration: 27100. Loss: 1.749542474746704. Accuracy: 20.147170768840397\n",
            "Iteration: 27200. Loss: 1.9592279195785522. Accuracy: 20.75615326059376\n",
            "Iteration: 27300. Loss: 2.1505420207977295. Accuracy: 18.97995432631312\n",
            "Iteration: 27400. Loss: 2.447920560836792. Accuracy: 10.53032225323522\n",
            "Iteration: 27500. Loss: 2.0255119800567627. Accuracy: 12.179649835067242\n",
            "Iteration: 27600. Loss: 2.0991134643554688. Accuracy: 22.912966252220247\n",
            "Iteration: 27700. Loss: 1.8731263875961304. Accuracy: 19.74118244100482\n",
            "Iteration: 27800. Loss: 2.2719149589538574. Accuracy: 23.344328850545548\n",
            "Iteration: 27900. Loss: 2.0133461952209473. Accuracy: 26.236995686374016\n",
            "Iteration: 28000. Loss: 1.9901318550109863. Accuracy: 16.97538695762497\n",
            "Iteration: 28100. Loss: 1.993688941001892. Accuracy: 22.963714793199696\n",
            "Iteration: 28200. Loss: 2.0937438011169434. Accuracy: 17.102258310073584\n",
            "Iteration: 28300. Loss: 1.8455383777618408. Accuracy: 24.02943415376808\n",
            "Iteration: 28400. Loss: 1.933226227760315. Accuracy: 24.4861710225831\n",
            "Iteration: 28500. Loss: 1.9048045873641968. Accuracy: 25.044404973357015\n",
            "Iteration: 28600. Loss: 2.0731072425842285. Accuracy: 22.20248667850799\n",
            "Iteration: 28700. Loss: 2.126553773880005. Accuracy: 13.6513575234712\n",
            "Iteration: 28800. Loss: 1.954308271408081. Accuracy: 19.487439736107586\n",
            "Iteration: 28900. Loss: 2.2985026836395264. Accuracy: 12.661760974371987\n",
            "Iteration: 29000. Loss: 2.1199700832366943. Accuracy: 23.623445825932503\n",
            "Iteration: 29100. Loss: 2.011049270629883. Accuracy: 24.30855112915504\n",
            "Iteration: 29200. Loss: 1.8535990715026855. Accuracy: 24.10555696523725\n",
            "Iteration: 29300. Loss: 1.8949075937271118. Accuracy: 16.721644252727735\n",
            "Iteration: 29400. Loss: 2.1093859672546387. Accuracy: 22.583100735853844\n",
            "Iteration: 29500. Loss: 1.8587963581085205. Accuracy: 17.73661507231667\n",
            "Iteration: 29600. Loss: 2.1001007556915283. Accuracy: 19.2844455721898\n",
            "Iteration: 29700. Loss: 1.640364408493042. Accuracy: 27.07434661253489\n",
            "Iteration: 29800. Loss: 2.256768226623535. Accuracy: 24.892159350418677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8UTZRHLaa0n"
      },
      "source": [
        "#Aproach 3\n",
        "batch size = 64<br>\n",
        "num_iters = 30000<br>\n",
        "input_dim = 1 * 28 * 28 <br>\n",
        "num_hidden = 100 <br>\n",
        "output_dim = 10 <br>\n",
        "<font color = 'tiffani blue'> learning_rate = 0.1 </font><br>\n",
        "Activation = ReLU <br>\n",
        "Criterion = Cross Entropy <br>\n",
        "Optimizar = SGD <br>\n",
        "<font color = 'green'> Accuracy = 75.61% </font><br>\n",
        "Comment: After increasing the learning rate even more, the accuracy has increased a lot.<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qad099TWZPDL",
        "outputId": "1d14182e-2de2-4d09-baac-ec1adb70a2a0"
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "batch_size = 64\n",
        "num_iters = 30000\n",
        "input_dim = 1*28*28 # num_features = 784\n",
        "num_hidden = 100\n",
        "output_dim = 10\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle= False, batch_size=batch_size)\n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "\n",
        "        self.linear_4 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_4 = nn.ReLU()\n",
        "        \n",
        "        \n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_5 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_5 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_6 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_6 = nn.ReLU()\n",
        "        \n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_4(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_4(out)\n",
        "        \n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_5(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_5(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_6(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_6(out)\n",
        "        \n",
        "\n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "iter_ = []\n",
        "loss_ = []\n",
        "accuracy_ = []\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 1*28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 100 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 1*28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "            iter_.append(iter)\n",
        "            loss_.append(loss.item())\n",
        "            accuracy_.append(accuracy)\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 100. Loss: 2.2985525131225586. Accuracy: 9.261608728749048\n",
            "Iteration: 200. Loss: 2.3117992877960205. Accuracy: 9.261608728749048\n",
            "Iteration: 300. Loss: 2.297316074371338. Accuracy: 9.79446840903324\n",
            "Iteration: 400. Loss: 2.3112902641296387. Accuracy: 10.327328089317431\n",
            "Iteration: 500. Loss: 2.3096959590911865. Accuracy: 9.261608728749048\n",
            "Iteration: 600. Loss: 2.3038060665130615. Accuracy: 9.261608728749048\n",
            "Iteration: 700. Loss: 2.298801898956299. Accuracy: 9.540725704136007\n",
            "Iteration: 800. Loss: 2.296647548675537. Accuracy: 9.79446840903324\n",
            "Iteration: 900. Loss: 2.3037259578704834. Accuracy: 9.261608728749048\n",
            "Iteration: 1000. Loss: 2.2994396686553955. Accuracy: 9.388480081197665\n",
            "Iteration: 1100. Loss: 2.297450304031372. Accuracy: 18.218726211621416\n",
            "Iteration: 1200. Loss: 2.2870306968688965. Accuracy: 16.54402435929967\n",
            "Iteration: 1300. Loss: 2.2846486568450928. Accuracy: 12.661760974371987\n",
            "Iteration: 1400. Loss: 2.2578957080841064. Accuracy: 10.75869068764273\n",
            "Iteration: 1500. Loss: 2.22713303565979. Accuracy: 10.707942146663283\n",
            "Iteration: 1600. Loss: 2.266296863555908. Accuracy: 18.269474752600864\n",
            "Iteration: 1700. Loss: 2.228085994720459. Accuracy: 10.327328089317431\n",
            "Iteration: 1800. Loss: 2.291170358657837. Accuracy: 15.681299162649074\n",
            "Iteration: 1900. Loss: 2.1866743564605713. Accuracy: 17.761989342806395\n",
            "Iteration: 2000. Loss: 2.3377490043640137. Accuracy: 13.87972595787871\n",
            "Iteration: 2100. Loss: 2.217987060546875. Accuracy: 14.46333417914235\n",
            "Iteration: 2200. Loss: 2.1822564601898193. Accuracy: 18.447094646028926\n",
            "Iteration: 2300. Loss: 2.232919454574585. Accuracy: 21.390510022836843\n",
            "Iteration: 2400. Loss: 2.256077289581299. Accuracy: 17.076884039583863\n",
            "Iteration: 2500. Loss: 2.2272708415985107. Accuracy: 18.32022329358031\n",
            "Iteration: 2600. Loss: 2.1389544010162354. Accuracy: 17.50824663790916\n",
            "Iteration: 2700. Loss: 2.25726580619812. Accuracy: 10.8348134991119\n",
            "Iteration: 2800. Loss: 2.221632242202759. Accuracy: 13.321492007104796\n",
            "Iteration: 2900. Loss: 2.24444842338562. Accuracy: 10.860187769601623\n",
            "Iteration: 3000. Loss: 2.232123851776123. Accuracy: 14.260340015224562\n",
            "Iteration: 3100. Loss: 2.18212890625. Accuracy: 18.041106318193354\n",
            "Iteration: 3200. Loss: 2.212510585784912. Accuracy: 15.833544785587414\n",
            "Iteration: 3300. Loss: 2.2758255004882812. Accuracy: 20.375539203247907\n",
            "Iteration: 3400. Loss: 2.1122193336486816. Accuracy: 18.24410048211114\n",
            "Iteration: 3500. Loss: 2.37690806388855. Accuracy: 10.403450900786602\n",
            "Iteration: 3600. Loss: 2.1146771907806396. Accuracy: 11.976655671149455\n",
            "Iteration: 3700. Loss: 2.1239497661590576. Accuracy: 21.415884293326567\n",
            "Iteration: 3800. Loss: 2.191312074661255. Accuracy: 20.045673686881504\n",
            "Iteration: 3900. Loss: 2.091445207595825. Accuracy: 19.05607713778229\n",
            "Iteration: 4000. Loss: 2.0956344604492188. Accuracy: 19.893428063943162\n",
            "Iteration: 4100. Loss: 2.083336114883423. Accuracy: 20.730778990104035\n",
            "Iteration: 4200. Loss: 2.071439266204834. Accuracy: 17.43212382643999\n",
            "Iteration: 4300. Loss: 2.1388447284698486. Accuracy: 13.676731793960924\n",
            "Iteration: 4400. Loss: 2.1088449954986572. Accuracy: 19.639685359045927\n",
            "Iteration: 4500. Loss: 2.2736024856567383. Accuracy: 14.793199695508754\n",
            "Iteration: 4600. Loss: 2.2216567993164062. Accuracy: 17.609743719868053\n",
            "Iteration: 4700. Loss: 2.2707736492156982. Accuracy: 17.863486424765288\n",
            "Iteration: 4800. Loss: 2.1279845237731934. Accuracy: 21.97411824410048\n",
            "Iteration: 4900. Loss: 2.139716863632202. Accuracy: 19.715808170515096\n",
            "Iteration: 5000. Loss: 2.3637571334838867. Accuracy: 19.182948490230906\n",
            "Iteration: 5100. Loss: 2.0940420627593994. Accuracy: 13.828977416899264\n",
            "Iteration: 5200. Loss: 2.4609179496765137. Accuracy: 18.01573204770363\n",
            "Iteration: 5300. Loss: 2.044637680053711. Accuracy: 20.071047957371224\n",
            "Iteration: 5400. Loss: 2.318094491958618. Accuracy: 17.863486424765288\n",
            "Iteration: 5500. Loss: 2.18328595161438. Accuracy: 14.717076884039583\n",
            "Iteration: 5600. Loss: 2.292695999145508. Accuracy: 15.529053539710734\n",
            "Iteration: 5700. Loss: 2.250457763671875. Accuracy: 23.090586145648313\n",
            "Iteration: 5800. Loss: 2.140014886856079. Accuracy: 23.75031717838112\n",
            "Iteration: 5900. Loss: 2.1293914318084717. Accuracy: 12.864755138289775\n",
            "Iteration: 6000. Loss: 2.1136391162872314. Accuracy: 16.873889875666073\n",
            "Iteration: 6100. Loss: 2.2082128524780273. Accuracy: 10.60644506470439\n",
            "Iteration: 6200. Loss: 2.250415325164795. Accuracy: 14.742451154529308\n",
            "Iteration: 6300. Loss: 2.0889346599578857. Accuracy: 23.395077391524993\n",
            "Iteration: 6400. Loss: 2.1208813190460205. Accuracy: 26.084750063435678\n",
            "Iteration: 6500. Loss: 2.176121234893799. Accuracy: 26.94747526008627\n",
            "Iteration: 6600. Loss: 2.105454921722412. Accuracy: 19.05607713778229\n",
            "Iteration: 6700. Loss: 1.948016881942749. Accuracy: 18.624714539456992\n",
            "Iteration: 6800. Loss: 2.025578022003174. Accuracy: 16.214158842933266\n",
            "Iteration: 6900. Loss: 2.066709280014038. Accuracy: 25.374270489723422\n",
            "Iteration: 7000. Loss: 1.9619601964950562. Accuracy: 28.444557218979956\n",
            "Iteration: 7100. Loss: 2.240262746810913. Accuracy: 18.70083735092616\n",
            "Iteration: 7200. Loss: 2.080669403076172. Accuracy: 19.157574219741182\n",
            "Iteration: 7300. Loss: 2.0466978549957275. Accuracy: 22.17711240801827\n",
            "Iteration: 7400. Loss: 1.8598690032958984. Accuracy: 23.902562801319462\n",
            "Iteration: 7500. Loss: 2.0377700328826904. Accuracy: 23.268206039076375\n",
            "Iteration: 7600. Loss: 2.0708024501800537. Accuracy: 23.67419436691195\n",
            "Iteration: 7700. Loss: 2.1168787479400635. Accuracy: 24.30855112915504\n",
            "Iteration: 7800. Loss: 1.9728482961654663. Accuracy: 29.916264907383912\n",
            "Iteration: 7900. Loss: 2.053208351135254. Accuracy: 27.708703374777976\n",
            "Iteration: 8000. Loss: 1.7942839860916138. Accuracy: 30.70286729256534\n",
            "Iteration: 8100. Loss: 2.368321418762207. Accuracy: 16.77239279370718\n",
            "Iteration: 8200. Loss: 2.1315720081329346. Accuracy: 23.090586145648313\n",
            "Iteration: 8300. Loss: 2.0557897090911865. Accuracy: 18.47246891651865\n",
            "Iteration: 8400. Loss: 2.1226675510406494. Accuracy: 27.58183202232936\n",
            "Iteration: 8500. Loss: 1.88237726688385. Accuracy: 29.15503679269221\n",
            "Iteration: 8600. Loss: 2.067795753479004. Accuracy: 13.194620654656179\n",
            "Iteration: 8700. Loss: 1.836098551750183. Accuracy: 21.314387211367674\n",
            "Iteration: 8800. Loss: 2.0089004039764404. Accuracy: 18.75158589190561\n",
            "Iteration: 8900. Loss: 2.0701935291290283. Accuracy: 34.9911190053286\n",
            "Iteration: 9000. Loss: 2.0125772953033447. Accuracy: 35.82846993148947\n",
            "Iteration: 9100. Loss: 2.0671234130859375. Accuracy: 23.80106571936057\n",
            "Iteration: 9200. Loss: 2.0422849655151367. Accuracy: 26.5414869322507\n",
            "Iteration: 9300. Loss: 1.9654380083084106. Accuracy: 32.47906622684598\n",
            "Iteration: 9400. Loss: 2.0376551151275635. Accuracy: 34.9911190053286\n",
            "Iteration: 9500. Loss: 1.9729068279266357. Accuracy: 24.917533620908397\n",
            "Iteration: 9600. Loss: 1.8263850212097168. Accuracy: 35.39710733316417\n",
            "Iteration: 9700. Loss: 1.7617287635803223. Accuracy: 38.848008119766554\n",
            "Iteration: 9800. Loss: 1.7615379095077515. Accuracy: 30.93123572697285\n",
            "Iteration: 9900. Loss: 2.079481601715088. Accuracy: 28.698299923877187\n",
            "Iteration: 10000. Loss: 1.985742211341858. Accuracy: 31.51484394823649\n",
            "Iteration: 10100. Loss: 2.078829050064087. Accuracy: 34.71200202994164\n",
            "Iteration: 10200. Loss: 2.756422519683838. Accuracy: 25.425019030702867\n",
            "Iteration: 10300. Loss: 1.7522832155227661. Accuracy: 36.48820096422228\n",
            "Iteration: 10400. Loss: 1.5465329885482788. Accuracy: 39.304744988581575\n",
            "Iteration: 10500. Loss: 1.8911223411560059. Accuracy: 22.633849276833292\n",
            "Iteration: 10600. Loss: 1.6296026706695557. Accuracy: 36.46282669373255\n",
            "Iteration: 10700. Loss: 2.159902334213257. Accuracy: 25.653387465110377\n",
            "Iteration: 10800. Loss: 1.891082525253296. Accuracy: 31.920832276072062\n",
            "Iteration: 10900. Loss: 2.3833396434783936. Accuracy: 24.71453945699061\n",
            "Iteration: 11000. Loss: 1.882131814956665. Accuracy: 17.863486424765288\n",
            "Iteration: 11100. Loss: 1.8855458498001099. Accuracy: 28.77442273534636\n",
            "Iteration: 11200. Loss: 1.8498005867004395. Accuracy: 22.963714793199696\n",
            "Iteration: 11300. Loss: 1.6854395866394043. Accuracy: 24.638416645521442\n",
            "Iteration: 11400. Loss: 1.627546787261963. Accuracy: 35.194113169246386\n",
            "Iteration: 11500. Loss: 2.2875123023986816. Accuracy: 36.1837097183456\n",
            "Iteration: 11600. Loss: 1.6937154531478882. Accuracy: 27.099720883024613\n",
            "Iteration: 11700. Loss: 2.3515098094940186. Accuracy: 25.831007358538443\n",
            "Iteration: 11800. Loss: 1.7196221351623535. Accuracy: 36.00608982491753\n",
            "Iteration: 11900. Loss: 2.3765616416931152. Accuracy: 16.163410301953817\n",
            "Iteration: 12000. Loss: 1.6996484994888306. Accuracy: 34.86424765287998\n",
            "Iteration: 12100. Loss: 1.9413132667541504. Accuracy: 26.439989850291806\n",
            "Iteration: 12200. Loss: 1.8280885219573975. Accuracy: 41.53768079167724\n",
            "Iteration: 12300. Loss: 1.827880859375. Accuracy: 38.72113676731794\n",
            "Iteration: 12400. Loss: 1.8379679918289185. Accuracy: 28.444557218979956\n",
            "Iteration: 12500. Loss: 3.33227276802063. Accuracy: 10.09895965490992\n",
            "Iteration: 12600. Loss: 1.7962522506713867. Accuracy: 33.41791423496574\n",
            "Iteration: 12700. Loss: 1.932164192199707. Accuracy: 33.697031210352705\n",
            "Iteration: 12800. Loss: 1.8337079286575317. Accuracy: 35.06724181679777\n",
            "Iteration: 12900. Loss: 1.7264983654022217. Accuracy: 32.732808931743214\n",
            "Iteration: 13000. Loss: 2.1416056156158447. Accuracy: 27.251966505962955\n",
            "Iteration: 13100. Loss: 1.9609160423278809. Accuracy: 42.52727734077645\n",
            "Iteration: 13200. Loss: 2.097757577896118. Accuracy: 29.07891398122304\n",
            "Iteration: 13300. Loss: 1.7177584171295166. Accuracy: 36.91956356254758\n",
            "Iteration: 13400. Loss: 1.8789516687393188. Accuracy: 33.08804871859934\n",
            "Iteration: 13500. Loss: 1.8149652481079102. Accuracy: 37.45242324283177\n",
            "Iteration: 13600. Loss: 1.6719529628753662. Accuracy: 20.24866785079929\n",
            "Iteration: 13700. Loss: 2.2053065299987793. Accuracy: 28.85054554681553\n",
            "Iteration: 13800. Loss: 1.6812928915023804. Accuracy: 44.93783303730018\n",
            "Iteration: 13900. Loss: 1.597800850868225. Accuracy: 40.11672164425273\n",
            "Iteration: 14000. Loss: 1.4961036443710327. Accuracy: 42.1466632834306\n",
            "Iteration: 14100. Loss: 1.8886302709579468. Accuracy: 34.10301953818828\n",
            "Iteration: 14200. Loss: 1.5354472398757935. Accuracy: 46.790154783049985\n",
            "Iteration: 14300. Loss: 1.6101382970809937. Accuracy: 29.58639939101751\n",
            "Iteration: 14400. Loss: 1.5059117078781128. Accuracy: 33.97614818573966\n",
            "Iteration: 14500. Loss: 1.5507464408874512. Accuracy: 42.65414869322507\n",
            "Iteration: 14600. Loss: 1.594504714012146. Accuracy: 47.246891651865006\n",
            "Iteration: 14700. Loss: 1.5237772464752197. Accuracy: 46.257295102765795\n",
            "Iteration: 14800. Loss: 1.6797171831130981. Accuracy: 41.41080943922862\n",
            "Iteration: 14900. Loss: 2.180318593978882. Accuracy: 38.54351687388988\n",
            "Iteration: 15000. Loss: 1.3850514888763428. Accuracy: 44.04973357015986\n",
            "Iteration: 15100. Loss: 1.369917869567871. Accuracy: 44.30347627505709\n",
            "Iteration: 15200. Loss: 1.4151332378387451. Accuracy: 43.795990865262624\n",
            "Iteration: 15300. Loss: 2.266740322113037. Accuracy: 30.1192590713017\n",
            "Iteration: 15400. Loss: 1.654757022857666. Accuracy: 51.4590205531591\n",
            "Iteration: 15500. Loss: 1.6054147481918335. Accuracy: 47.652879979700586\n",
            "Iteration: 15600. Loss: 1.617542028427124. Accuracy: 49.809692971327074\n",
            "Iteration: 15700. Loss: 1.6868177652359009. Accuracy: 50.95153514336463\n",
            "Iteration: 15800. Loss: 1.4554824829101562. Accuracy: 50.11418421720376\n",
            "Iteration: 15900. Loss: 1.3631104230880737. Accuracy: 28.26693732555189\n",
            "Iteration: 16000. Loss: 1.4341543912887573. Accuracy: 45.03933011925907\n",
            "Iteration: 16100. Loss: 1.3318307399749756. Accuracy: 42.45115452930728\n",
            "Iteration: 16200. Loss: 1.4242476224899292. Accuracy: 48.05886830753616\n",
            "Iteration: 16300. Loss: 1.3866987228393555. Accuracy: 46.51103780766303\n",
            "Iteration: 16400. Loss: 1.688808560371399. Accuracy: 28.77442273534636\n",
            "Iteration: 16500. Loss: 1.3369096517562866. Accuracy: 50.748540979446844\n",
            "Iteration: 16600. Loss: 1.5708929300308228. Accuracy: 50.03806140573459\n",
            "Iteration: 16700. Loss: 1.5925486087799072. Accuracy: 37.09718345597564\n",
            "Iteration: 16800. Loss: 3.117723226547241. Accuracy: 31.616341030195382\n",
            "Iteration: 16900. Loss: 1.6215347051620483. Accuracy: 54.47855874143618\n",
            "Iteration: 17000. Loss: 1.584037184715271. Accuracy: 50.266429840142095\n",
            "Iteration: 17100. Loss: 1.4283078908920288. Accuracy: 53.61583354478559\n",
            "Iteration: 17200. Loss: 1.0851130485534668. Accuracy: 49.55595026642984\n",
            "Iteration: 17300. Loss: 1.3575998544692993. Accuracy: 54.88454706927176\n",
            "Iteration: 17400. Loss: 2.257101535797119. Accuracy: 45.41994417660492\n",
            "Iteration: 17500. Loss: 1.593464732170105. Accuracy: 47.67825425019031\n",
            "Iteration: 17600. Loss: 1.6165815591812134. Accuracy: 53.311342298908905\n",
            "Iteration: 17700. Loss: 1.5691553354263306. Accuracy: 29.129662522202487\n",
            "Iteration: 17800. Loss: 1.2807196378707886. Accuracy: 53.260593757929456\n",
            "Iteration: 17900. Loss: 1.2881115674972534. Accuracy: 56.00101497081959\n",
            "Iteration: 18000. Loss: 1.4096612930297852. Accuracy: 43.99898502918041\n",
            "Iteration: 18100. Loss: 1.4074828624725342. Accuracy: 52.347120020299414\n",
            "Iteration: 18200. Loss: 1.8723448514938354. Accuracy: 46.00355239786856\n",
            "Iteration: 18300. Loss: 1.1943962574005127. Accuracy: 54.706927175843695\n",
            "Iteration: 18400. Loss: 1.8852168321609497. Accuracy: 45.87668104541994\n",
            "Iteration: 18500. Loss: 2.92659592628479. Accuracy: 20.93377315402182\n",
            "Iteration: 18600. Loss: 1.2226066589355469. Accuracy: 56.63537173306268\n",
            "Iteration: 18700. Loss: 1.3181649446487427. Accuracy: 58.58919056077138\n",
            "Iteration: 18800. Loss: 1.4543943405151367. Accuracy: 54.40243592996701\n",
            "Iteration: 18900. Loss: 1.2520697116851807. Accuracy: 52.67698553666582\n",
            "Iteration: 19000. Loss: 1.3005266189575195. Accuracy: 47.830499873128645\n",
            "Iteration: 19100. Loss: 1.2835884094238281. Accuracy: 41.46155798020807\n",
            "Iteration: 19200. Loss: 1.403099536895752. Accuracy: 54.6054300938848\n",
            "Iteration: 19300. Loss: 1.4440640211105347. Accuracy: 53.742704897234205\n",
            "Iteration: 19400. Loss: 1.1818252801895142. Accuracy: 47.196143110885565\n",
            "Iteration: 19500. Loss: 1.6770669221878052. Accuracy: 48.337985282923114\n",
            "Iteration: 19600. Loss: 1.3159534931182861. Accuracy: 53.8442019791931\n",
            "Iteration: 19700. Loss: 2.078073024749756. Accuracy: 50.266429840142095\n",
            "Iteration: 19800. Loss: 1.1393593549728394. Accuracy: 46.9170261354986\n",
            "Iteration: 19900. Loss: 1.7053571939468384. Accuracy: 20.121796498350673\n",
            "Iteration: 20000. Loss: 1.4586880207061768. Accuracy: 42.9078913981223\n",
            "Iteration: 20100. Loss: 2.0862441062927246. Accuracy: 45.31844709464603\n",
            "Iteration: 20200. Loss: 1.3971184492111206. Accuracy: 53.46358792184725\n",
            "Iteration: 20300. Loss: 2.0989174842834473. Accuracy: 50.748540979446844\n",
            "Iteration: 20400. Loss: 2.6216423511505127. Accuracy: 34.661253488962195\n",
            "Iteration: 20500. Loss: 1.2225816249847412. Accuracy: 52.169500126871355\n",
            "Iteration: 20600. Loss: 1.528760552406311. Accuracy: 40.29434153768079\n",
            "Iteration: 20700. Loss: 1.2503771781921387. Accuracy: 53.15909667597057\n",
            "Iteration: 20800. Loss: 1.4943292140960693. Accuracy: 44.45572189799543\n",
            "Iteration: 20900. Loss: 1.3347249031066895. Accuracy: 40.72570413600609\n",
            "Iteration: 21000. Loss: 1.5064008235931396. Accuracy: 34.686627759451916\n",
            "Iteration: 21100. Loss: 1.3928086757659912. Accuracy: 50.95153514336463\n",
            "Iteration: 21200. Loss: 2.344010353088379. Accuracy: 37.90916011164679\n",
            "Iteration: 21300. Loss: 1.8261266946792603. Accuracy: 42.62877442273535\n",
            "Iteration: 21400. Loss: 1.1641639471054077. Accuracy: 43.237756914488706\n",
            "Iteration: 21500. Loss: 2.2254300117492676. Accuracy: 24.384673940624207\n",
            "Iteration: 21600. Loss: 1.6459323167800903. Accuracy: 48.00811976655671\n",
            "Iteration: 21700. Loss: 1.246172547340393. Accuracy: 48.236488200964224\n",
            "Iteration: 21800. Loss: 0.9725164771080017. Accuracy: 59.85790408525755\n",
            "Iteration: 21900. Loss: 0.9275099039077759. Accuracy: 62.57295102765796\n",
            "Iteration: 22000. Loss: 1.1376835107803345. Accuracy: 55.64577518396346\n",
            "Iteration: 22100. Loss: 0.9185608625411987. Accuracy: 61.38036031464095\n",
            "Iteration: 22200. Loss: 1.7769709825515747. Accuracy: 48.261862471453945\n",
            "Iteration: 22300. Loss: 1.5253444910049438. Accuracy: 27.3027150469424\n",
            "Iteration: 22400. Loss: 1.0947879552841187. Accuracy: 54.174067495559505\n",
            "Iteration: 22500. Loss: 1.2291784286499023. Accuracy: 52.42324283176859\n",
            "Iteration: 22600. Loss: 0.8769243955612183. Accuracy: 60.61913219994925\n",
            "Iteration: 22700. Loss: 1.9790358543395996. Accuracy: 44.35422481603654\n",
            "Iteration: 22800. Loss: 0.8234939575195312. Accuracy: 65.23724942907891\n",
            "Iteration: 22900. Loss: 1.18062424659729. Accuracy: 58.208576503425526\n",
            "Iteration: 23000. Loss: 1.1682270765304565. Accuracy: 64.17153006851053\n",
            "Iteration: 23100. Loss: 1.2079440355300903. Accuracy: 63.13118497843187\n",
            "Iteration: 23200. Loss: 1.4731509685516357. Accuracy: 25.6787617356001\n",
            "Iteration: 23300. Loss: 1.3326475620269775. Accuracy: 53.94569906115199\n",
            "Iteration: 23400. Loss: 1.0985233783721924. Accuracy: 65.33874651103781\n",
            "Iteration: 23500. Loss: 1.6057794094085693. Accuracy: 64.93275818320224\n",
            "Iteration: 23600. Loss: 0.9178855419158936. Accuracy: 61.837097183455974\n",
            "Iteration: 23700. Loss: 1.2164828777313232. Accuracy: 61.5326059375793\n",
            "Iteration: 23800. Loss: 1.5357202291488647. Accuracy: 56.762243085511294\n",
            "Iteration: 23900. Loss: 0.8580098748207092. Accuracy: 67.49555950266429\n",
            "Iteration: 24000. Loss: 1.1858861446380615. Accuracy: 68.48515605176351\n",
            "Iteration: 24100. Loss: 1.062866449356079. Accuracy: 54.07257041360061\n",
            "Iteration: 24200. Loss: 0.8493751287460327. Accuracy: 70.7434661253489\n",
            "Iteration: 24300. Loss: 1.4665262699127197. Accuracy: 57.142857142857146\n",
            "Iteration: 24400. Loss: 0.9892763495445251. Accuracy: 42.47652879979701\n",
            "Iteration: 24500. Loss: 0.9602901935577393. Accuracy: 66.25222024866785\n",
            "Iteration: 24600. Loss: 1.0009044408798218. Accuracy: 58.23395077391525\n",
            "Iteration: 24700. Loss: 0.6903761029243469. Accuracy: 44.684090332402945\n",
            "Iteration: 24800. Loss: 0.8799328804016113. Accuracy: 70.79421466632834\n",
            "Iteration: 24900. Loss: 0.8665039539337158. Accuracy: 65.03425526516112\n",
            "Iteration: 25000. Loss: 0.7921475172042847. Accuracy: 66.3029687896473\n",
            "Iteration: 25100. Loss: 1.7200483083724976. Accuracy: 62.6236995686374\n",
            "Iteration: 25200. Loss: 0.6718454360961914. Accuracy: 72.49429078913981\n",
            "Iteration: 25300. Loss: 1.14655601978302. Accuracy: 57.295102765795484\n",
            "Iteration: 25400. Loss: 0.8487714529037476. Accuracy: 71.22557726465364\n",
            "Iteration: 25500. Loss: 0.4853569269180298. Accuracy: 69.2210098959655\n",
            "Iteration: 25600. Loss: 0.8762059807777405. Accuracy: 69.27175843694494\n",
            "Iteration: 25700. Loss: 1.1259911060333252. Accuracy: 57.90408525754884\n",
            "Iteration: 25800. Loss: 1.0126928091049194. Accuracy: 67.49555950266429\n",
            "Iteration: 25900. Loss: 0.6509426236152649. Accuracy: 68.0284191829485\n",
            "Iteration: 26000. Loss: 0.8106861114501953. Accuracy: 71.37782288759198\n",
            "Iteration: 26100. Loss: 0.8437271118164062. Accuracy: 72.01217964983506\n",
            "Iteration: 26200. Loss: 1.558044195175171. Accuracy: 51.662014717076886\n",
            "Iteration: 26300. Loss: 1.0007256269454956. Accuracy: 63.3849276833291\n",
            "Iteration: 26400. Loss: 0.9447040557861328. Accuracy: 69.14488708449632\n",
            "Iteration: 26500. Loss: 0.7372853755950928. Accuracy: 69.24638416645521\n",
            "Iteration: 26600. Loss: 0.7930365800857544. Accuracy: 68.256787617356\n",
            "Iteration: 26700. Loss: 0.7458497285842896. Accuracy: 58.741436183709716\n",
            "Iteration: 26800. Loss: 0.957484781742096. Accuracy: 50.799289520426285\n",
            "Iteration: 26900. Loss: 0.9813163876533508. Accuracy: 71.75843694493783\n",
            "Iteration: 27000. Loss: 2.524725914001465. Accuracy: 44.40497335701598\n",
            "Iteration: 27100. Loss: 0.8252381682395935. Accuracy: 74.37198680537935\n",
            "Iteration: 27200. Loss: 0.9006470441818237. Accuracy: 71.834559756407\n",
            "Iteration: 27300. Loss: 0.9645185470581055. Accuracy: 64.42527277340777\n",
            "Iteration: 27400. Loss: 0.7338175773620605. Accuracy: 74.67647805125603\n",
            "Iteration: 27500. Loss: 1.2799800634384155. Accuracy: 61.228114691702615\n",
            "Iteration: 27600. Loss: 0.6747026443481445. Accuracy: 74.42273534635879\n",
            "Iteration: 27700. Loss: 1.0791475772857666. Accuracy: 67.9269221009896\n",
            "Iteration: 27800. Loss: 0.7173565626144409. Accuracy: 71.09870591220502\n",
            "Iteration: 27900. Loss: 0.6174857020378113. Accuracy: 65.71936056838366\n",
            "Iteration: 28000. Loss: 0.43778496980667114. Accuracy: 70.66734331387973\n",
            "Iteration: 28100. Loss: 0.8965610861778259. Accuracy: 71.58081705150977\n",
            "Iteration: 28200. Loss: 1.0432181358337402. Accuracy: 65.54174067495559\n",
            "Iteration: 28300. Loss: 0.7689730525016785. Accuracy: 76.55417406749557\n",
            "Iteration: 28400. Loss: 0.452470988035202. Accuracy: 74.6511037807663\n",
            "Iteration: 28500. Loss: 0.7631136178970337. Accuracy: 77.0362852068003\n",
            "Iteration: 28600. Loss: 0.4768335223197937. Accuracy: 75.69144887084497\n",
            "Iteration: 28700. Loss: 0.8886609077453613. Accuracy: 68.78964729764019\n",
            "Iteration: 28800. Loss: 0.922834038734436. Accuracy: 70.81958893681806\n",
            "Iteration: 28900. Loss: 0.6821444034576416. Accuracy: 69.44937833037301\n",
            "Iteration: 29000. Loss: 0.5998485088348389. Accuracy: 72.24054808424258\n",
            "Iteration: 29100. Loss: 2.5508885383605957. Accuracy: 65.64323775691449\n",
            "Iteration: 29200. Loss: 0.7706242799758911. Accuracy: 70.46434914996193\n",
            "Iteration: 29300. Loss: 0.7925088405609131. Accuracy: 70.15985790408526\n",
            "Iteration: 29400. Loss: 0.7154897451400757. Accuracy: 72.62116214158843\n",
            "Iteration: 29500. Loss: 0.5218918919563293. Accuracy: 77.51839634610505\n",
            "Iteration: 29600. Loss: 0.9210778474807739. Accuracy: 61.50723166708957\n",
            "Iteration: 29700. Loss: 0.6428622007369995. Accuracy: 70.33747779751332\n",
            "Iteration: 29800. Loss: 0.5746650695800781. Accuracy: 75.61532605937579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AozePbcZbQH8"
      },
      "source": [
        "#Aproach 4\n",
        "batch size = 64<br>\n",
        "<font color = 'tiffani blue'> num_iters = 60000 </font><br>\n",
        "input_dim = 1 * 28 * 28 <br>\n",
        "num_hidden = 100 <br>\n",
        "output_dim = 10 <br>\n",
        "learning_rate = 0.1 <br>\n",
        "Activation = ReLU <br>\n",
        "Criterion = Cross Entropy <br>\n",
        "Optimizar = SGD <br>\n",
        "<font color = 'green'> Accuracy = 86.62% </font><br>\n",
        "Comment: I have doubled the iteration and the accuracy has increased about 11%.<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ray-7GRtZocE",
        "outputId": "0ebcb39b-711a-4a62-d025-99f10d0bce51"
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "batch_size = 64\n",
        "num_iters = 60000\n",
        "input_dim = 1*28*28 # num_features = 784\n",
        "num_hidden = 100\n",
        "output_dim = 10\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle= False, batch_size=batch_size)\n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "\n",
        "        self.linear_4 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_4 = nn.ReLU()\n",
        "        \n",
        "        \n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_5 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_5 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_6 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_6 = nn.ReLU()\n",
        "        \n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_4(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_4(out)\n",
        "        \n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_5(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_5(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_6(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_6(out)\n",
        "        \n",
        "\n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "iter_ = []\n",
        "loss_ = []\n",
        "accuracy_ = []\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 1*28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 100 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 1*28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "            iter_.append(iter)\n",
        "            loss_.append(loss.item())\n",
        "            accuracy_.append(accuracy)\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 100. Loss: 2.3023600578308105. Accuracy: 9.769094138543517\n",
            "Iteration: 200. Loss: 2.304642915725708. Accuracy: 9.997462572951028\n",
            "Iteration: 300. Loss: 2.303281545639038. Accuracy: 10.175082466379092\n",
            "Iteration: 400. Loss: 2.3033735752105713. Accuracy: 13.778228875919817\n",
            "Iteration: 500. Loss: 2.3057358264923096. Accuracy: 9.286982999238772\n",
            "Iteration: 600. Loss: 2.301929473876953. Accuracy: 9.769094138543517\n",
            "Iteration: 700. Loss: 2.30350923538208. Accuracy: 9.997462572951028\n",
            "Iteration: 800. Loss: 2.305283546447754. Accuracy: 15.935041867546309\n",
            "Iteration: 900. Loss: 2.3078770637512207. Accuracy: 17.43212382643999\n",
            "Iteration: 1000. Loss: 2.3017938137054443. Accuracy: 19.538188277087034\n",
            "Iteration: 1100. Loss: 2.2981760501861572. Accuracy: 11.64679015478305\n",
            "Iteration: 1200. Loss: 2.2953550815582275. Accuracy: 9.79446840903324\n",
            "Iteration: 1300. Loss: 2.2913131713867188. Accuracy: 10.352702359807155\n",
            "Iteration: 1400. Loss: 2.2804601192474365. Accuracy: 14.209591474245116\n",
            "Iteration: 1500. Loss: 2.257512331008911. Accuracy: 17.711240801826946\n",
            "Iteration: 1600. Loss: 2.260601282119751. Accuracy: 10.403450900786602\n",
            "Iteration: 1700. Loss: 2.221946954727173. Accuracy: 14.717076884039583\n",
            "Iteration: 1800. Loss: 2.213855028152466. Accuracy: 12.10352702359807\n",
            "Iteration: 1900. Loss: 2.1812732219696045. Accuracy: 18.218726211621416\n",
            "Iteration: 2000. Loss: 2.281588315963745. Accuracy: 9.667597056584624\n",
            "Iteration: 2100. Loss: 2.199860095977783. Accuracy: 17.45749809692971\n",
            "Iteration: 2200. Loss: 2.310452461242676. Accuracy: 14.640954072570414\n",
            "Iteration: 2300. Loss: 2.168254852294922. Accuracy: 9.540725704136007\n",
            "Iteration: 2400. Loss: 2.4220035076141357. Accuracy: 11.64679015478305\n",
            "Iteration: 2500. Loss: 2.163846015930176. Accuracy: 20.883024613042377\n",
            "Iteration: 2600. Loss: 2.1681690216064453. Accuracy: 17.635117990357777\n",
            "Iteration: 2700. Loss: 2.3053414821624756. Accuracy: 12.940877949758944\n",
            "Iteration: 2800. Loss: 2.2269365787506104. Accuracy: 11.621415884293327\n",
            "Iteration: 2900. Loss: 2.201260566711426. Accuracy: 14.234965744734838\n",
            "Iteration: 3000. Loss: 2.355410099029541. Accuracy: 14.76782542501903\n",
            "Iteration: 3100. Loss: 2.1384546756744385. Accuracy: 18.903831514843947\n",
            "Iteration: 3200. Loss: 2.1985561847686768. Accuracy: 18.54859172798782\n",
            "Iteration: 3300. Loss: 2.1214818954467773. Accuracy: 12.179649835067242\n",
            "Iteration: 3400. Loss: 2.2130279541015625. Accuracy: 14.920071047957371\n",
            "Iteration: 3500. Loss: 2.2370333671569824. Accuracy: 10.124333925399645\n",
            "Iteration: 3600. Loss: 2.1478052139282227. Accuracy: 10.809439228622177\n",
            "Iteration: 3700. Loss: 2.1305558681488037. Accuracy: 14.61557980208069\n",
            "Iteration: 3800. Loss: 2.347766637802124. Accuracy: 9.895965490992134\n",
            "Iteration: 3900. Loss: 2.076450824737549. Accuracy: 20.375539203247907\n",
            "Iteration: 4000. Loss: 2.1656386852264404. Accuracy: 19.487439736107586\n",
            "Iteration: 4100. Loss: 2.0390617847442627. Accuracy: 20.883024613042377\n",
            "Iteration: 4200. Loss: 2.220240354537964. Accuracy: 18.117229129662523\n",
            "Iteration: 4300. Loss: 2.215399742126465. Accuracy: 15.529053539710734\n",
            "Iteration: 4400. Loss: 2.2507004737854004. Accuracy: 20.908398883532097\n",
            "Iteration: 4500. Loss: 2.047436237335205. Accuracy: 20.603907637655418\n",
            "Iteration: 4600. Loss: 2.190450429916382. Accuracy: 11.748287236741943\n",
            "Iteration: 4700. Loss: 2.192781448364258. Accuracy: 20.857650342552652\n",
            "Iteration: 4800. Loss: 2.1785316467285156. Accuracy: 20.09642222786095\n",
            "Iteration: 4900. Loss: 2.1055898666381836. Accuracy: 19.36056838365897\n",
            "Iteration: 5000. Loss: 2.1087160110473633. Accuracy: 12.002029941639178\n",
            "Iteration: 5100. Loss: 1.997220516204834. Accuracy: 21.111393047449887\n",
            "Iteration: 5200. Loss: 2.6069283485412598. Accuracy: 13.448363359553413\n",
            "Iteration: 5300. Loss: 2.1442291736602783. Accuracy: 15.65592489215935\n",
            "Iteration: 5400. Loss: 2.092585802078247. Accuracy: 22.659223547323016\n",
            "Iteration: 5500. Loss: 2.0218541622161865. Accuracy: 17.305252473991374\n",
            "Iteration: 5600. Loss: 2.0122463703155518. Accuracy: 22.126363867038823\n",
            "Iteration: 5700. Loss: 2.0438663959503174. Accuracy: 20.55315909667597\n",
            "Iteration: 5800. Loss: 2.0986201763153076. Accuracy: 19.994925145902055\n",
            "Iteration: 5900. Loss: 2.1718385219573975. Accuracy: 14.742451154529308\n",
            "Iteration: 6000. Loss: 2.090289831161499. Accuracy: 12.53488962192337\n",
            "Iteration: 6100. Loss: 2.071481227874756. Accuracy: 14.53945699061152\n",
            "Iteration: 6200. Loss: 2.012829065322876. Accuracy: 17.533620908398884\n",
            "Iteration: 6300. Loss: 2.2957558631896973. Accuracy: 22.659223547323016\n",
            "Iteration: 6400. Loss: 2.240135669708252. Accuracy: 15.935041867546309\n",
            "Iteration: 6500. Loss: 2.1736886501312256. Accuracy: 24.613042375031718\n",
            "Iteration: 6600. Loss: 2.135633707046509. Accuracy: 24.232428317685866\n",
            "Iteration: 6700. Loss: 2.135911226272583. Accuracy: 25.247399137274805\n",
            "Iteration: 6800. Loss: 2.275834560394287. Accuracy: 14.361837097183455\n",
            "Iteration: 6900. Loss: 2.1842803955078125. Accuracy: 17.43212382643999\n",
            "Iteration: 7000. Loss: 2.3709816932678223. Accuracy: 24.02943415376808\n",
            "Iteration: 7100. Loss: 2.3456976413726807. Accuracy: 20.24866785079929\n",
            "Iteration: 7200. Loss: 2.143794059753418. Accuracy: 25.780258817558995\n",
            "Iteration: 7300. Loss: 1.9774492979049683. Accuracy: 20.654656178634863\n",
            "Iteration: 7400. Loss: 1.9411044120788574. Accuracy: 16.493275818320225\n",
            "Iteration: 7500. Loss: 2.085461378097534. Accuracy: 27.987820350164935\n",
            "Iteration: 7600. Loss: 2.0842082500457764. Accuracy: 28.216188784572445\n",
            "Iteration: 7700. Loss: 2.02524733543396. Accuracy: 16.23953311342299\n",
            "Iteration: 7800. Loss: 2.1981256008148193. Accuracy: 19.842679522963714\n",
            "Iteration: 7900. Loss: 2.2270987033843994. Accuracy: 13.702106064450646\n",
            "Iteration: 8000. Loss: 2.003568649291992. Accuracy: 24.84141080943923\n",
            "Iteration: 8100. Loss: 2.010594129562378. Accuracy: 27.22659223547323\n",
            "Iteration: 8200. Loss: 2.211602210998535. Accuracy: 15.35143364628267\n",
            "Iteration: 8300. Loss: 1.881484866142273. Accuracy: 28.698299923877187\n",
            "Iteration: 8400. Loss: 1.8412752151489258. Accuracy: 24.689165186500887\n",
            "Iteration: 8500. Loss: 1.9011365175247192. Accuracy: 28.41918294849023\n",
            "Iteration: 8600. Loss: 2.2987682819366455. Accuracy: 18.624714539456992\n",
            "Iteration: 8700. Loss: 1.9559539556503296. Accuracy: 30.753615833544785\n",
            "Iteration: 8800. Loss: 2.09309983253479. Accuracy: 22.78609489977163\n",
            "Iteration: 8900. Loss: 2.028536558151245. Accuracy: 31.667089571174827\n",
            "Iteration: 9000. Loss: 1.971199631690979. Accuracy: 25.45039330119259\n",
            "Iteration: 9100. Loss: 1.9086432456970215. Accuracy: 29.30728241563055\n",
            "Iteration: 9200. Loss: 1.8190884590148926. Accuracy: 30.347627505709212\n",
            "Iteration: 9300. Loss: 2.274649143218994. Accuracy: 16.417153006851052\n",
            "Iteration: 9400. Loss: 1.974104404449463. Accuracy: 34.255265161126616\n",
            "Iteration: 9500. Loss: 1.964785099029541. Accuracy: 31.870083735092617\n",
            "Iteration: 9600. Loss: 1.67789626121521. Accuracy: 29.358030956609998\n",
            "Iteration: 9700. Loss: 2.130441665649414. Accuracy: 35.16873889875666\n",
            "Iteration: 9800. Loss: 2.0062413215637207. Accuracy: 23.57269728495306\n",
            "Iteration: 9900. Loss: 1.9106191396713257. Accuracy: 28.03856889114438\n",
            "Iteration: 10000. Loss: 2.1845552921295166. Accuracy: 20.603907637655418\n",
            "Iteration: 10100. Loss: 1.8586139678955078. Accuracy: 31.184978431870082\n",
            "Iteration: 10200. Loss: 1.9559403657913208. Accuracy: 36.234458259325045\n",
            "Iteration: 10300. Loss: 2.078677177429199. Accuracy: 30.32225323521949\n",
            "Iteration: 10400. Loss: 2.274937868118286. Accuracy: 15.123065211875158\n",
            "Iteration: 10500. Loss: 1.9068315029144287. Accuracy: 39.05100228368435\n",
            "Iteration: 10600. Loss: 2.6726081371307373. Accuracy: 22.050241055569654\n",
            "Iteration: 10700. Loss: 1.9150125980377197. Accuracy: 31.210352702359806\n",
            "Iteration: 10800. Loss: 1.8436740636825562. Accuracy: 28.977416899264146\n",
            "Iteration: 10900. Loss: 1.7874550819396973. Accuracy: 38.61963968535905\n",
            "Iteration: 11000. Loss: 1.68662691116333. Accuracy: 37.883785841157064\n",
            "Iteration: 11100. Loss: 1.9108318090438843. Accuracy: 31.23572697284953\n",
            "Iteration: 11200. Loss: 2.010951042175293. Accuracy: 36.234458259325045\n",
            "Iteration: 11300. Loss: 1.7847239971160889. Accuracy: 35.11799035777722\n",
            "Iteration: 11400. Loss: 2.019721746444702. Accuracy: 25.80563308804872\n",
            "Iteration: 11500. Loss: 1.9603594541549683. Accuracy: 39.35549352956102\n",
            "Iteration: 11600. Loss: 1.743685007095337. Accuracy: 37.274803349403705\n",
            "Iteration: 11700. Loss: 1.8588616847991943. Accuracy: 33.798528292311595\n",
            "Iteration: 11800. Loss: 2.3472213745117188. Accuracy: 35.879218472468914\n",
            "Iteration: 11900. Loss: 1.6928809881210327. Accuracy: 23.699568637401676\n",
            "Iteration: 12000. Loss: 1.6948065757751465. Accuracy: 42.273534635879216\n",
            "Iteration: 12100. Loss: 2.015747308731079. Accuracy: 19.639685359045927\n",
            "Iteration: 12200. Loss: 1.9315675497055054. Accuracy: 29.383405227099722\n",
            "Iteration: 12300. Loss: 1.7106976509094238. Accuracy: 26.160872874904847\n",
            "Iteration: 12400. Loss: 1.641528844833374. Accuracy: 34.255265161126616\n",
            "Iteration: 12500. Loss: 1.7793337106704712. Accuracy: 41.13169246384167\n",
            "Iteration: 12600. Loss: 1.6367908716201782. Accuracy: 41.66455214412586\n",
            "Iteration: 12700. Loss: 1.8462024927139282. Accuracy: 37.90916011164679\n",
            "Iteration: 12800. Loss: 1.6306976079940796. Accuracy: 29.358030956609998\n",
            "Iteration: 12900. Loss: 1.6438982486724854. Accuracy: 29.00279116975387\n",
            "Iteration: 13000. Loss: 1.5796079635620117. Accuracy: 39.253996447602134\n",
            "Iteration: 13100. Loss: 1.979517936706543. Accuracy: 36.513575234712\n",
            "Iteration: 13200. Loss: 1.7192498445510864. Accuracy: 38.72113676731794\n",
            "Iteration: 13300. Loss: 2.1642863750457764. Accuracy: 33.95077391524994\n",
            "Iteration: 13400. Loss: 1.7610068321228027. Accuracy: 32.45369195635625\n",
            "Iteration: 13500. Loss: 2.0060806274414062. Accuracy: 31.058107079421468\n",
            "Iteration: 13600. Loss: 1.4833848476409912. Accuracy: 45.03933011925907\n",
            "Iteration: 13700. Loss: 4.039529323577881. Accuracy: 10.09895965490992\n",
            "Iteration: 13800. Loss: 2.1253607273101807. Accuracy: 26.668358284699316\n",
            "Iteration: 13900. Loss: 1.6746360063552856. Accuracy: 40.319715808170514\n",
            "Iteration: 14000. Loss: 1.748596429824829. Accuracy: 42.88251712763258\n",
            "Iteration: 14100. Loss: 2.0549466609954834. Accuracy: 39.58386196396854\n",
            "Iteration: 14200. Loss: 1.8741724491119385. Accuracy: 19.106825678761737\n",
            "Iteration: 14300. Loss: 1.6675372123718262. Accuracy: 30.19538188277087\n",
            "Iteration: 14400. Loss: 1.6103957891464233. Accuracy: 45.36919563562547\n",
            "Iteration: 14500. Loss: 1.669266939163208. Accuracy: 41.96904339000254\n",
            "Iteration: 14600. Loss: 1.6814864873886108. Accuracy: 48.261862471453945\n",
            "Iteration: 14700. Loss: 1.791687250137329. Accuracy: 31.51484394823649\n",
            "Iteration: 14800. Loss: 2.3473095893859863. Accuracy: 25.374270489723422\n",
            "Iteration: 14900. Loss: 1.7565116882324219. Accuracy: 35.270235980715555\n",
            "Iteration: 15000. Loss: 1.5787503719329834. Accuracy: 35.244861710225834\n",
            "Iteration: 15100. Loss: 1.4626747369766235. Accuracy: 46.181172291296626\n",
            "Iteration: 15200. Loss: 2.3093433380126953. Accuracy: 35.65085003806141\n",
            "Iteration: 15300. Loss: 1.333086371421814. Accuracy: 48.03349403704644\n",
            "Iteration: 15400. Loss: 1.345241665840149. Accuracy: 44.88708449632073\n",
            "Iteration: 15500. Loss: 2.3244683742523193. Accuracy: 12.357269728495305\n",
            "Iteration: 15600. Loss: 1.4593268632888794. Accuracy: 47.57675716823141\n",
            "Iteration: 15700. Loss: 1.883388638496399. Accuracy: 26.31311849784319\n",
            "Iteration: 15800. Loss: 1.9373503923416138. Accuracy: 39.86297893935549\n",
            "Iteration: 15900. Loss: 2.2596516609191895. Accuracy: 30.880487185993402\n",
            "Iteration: 16000. Loss: 2.942938804626465. Accuracy: 17.35600101497082\n",
            "Iteration: 16100. Loss: 1.4680086374282837. Accuracy: 48.89621923369703\n",
            "Iteration: 16200. Loss: 2.2858290672302246. Accuracy: 18.142603400152247\n",
            "Iteration: 16300. Loss: 1.8841205835342407. Accuracy: 39.203247906622686\n",
            "Iteration: 16400. Loss: 1.75139582157135. Accuracy: 39.45699061151992\n",
            "Iteration: 16500. Loss: 1.7357261180877686. Accuracy: 30.42375031717838\n",
            "Iteration: 16600. Loss: 1.7493846416473389. Accuracy: 43.06013702106065\n",
            "Iteration: 16700. Loss: 1.8554143905639648. Accuracy: 41.740674955595026\n",
            "Iteration: 16800. Loss: 2.1011528968811035. Accuracy: 35.34635879218472\n",
            "Iteration: 16900. Loss: 1.8844618797302246. Accuracy: 40.04059883278356\n",
            "Iteration: 17000. Loss: 1.6211901903152466. Accuracy: 45.216950012687136\n",
            "Iteration: 17100. Loss: 1.8643776178359985. Accuracy: 37.35092616087287\n",
            "Iteration: 17200. Loss: 1.6911669969558716. Accuracy: 35.98071555442781\n",
            "Iteration: 17300. Loss: 1.5658679008483887. Accuracy: 48.185739659984776\n",
            "Iteration: 17400. Loss: 2.061251640319824. Accuracy: 33.13879725957879\n",
            "Iteration: 17500. Loss: 1.9240479469299316. Accuracy: 36.10758690687643\n",
            "Iteration: 17600. Loss: 1.9627822637557983. Accuracy: 31.261101243339255\n",
            "Iteration: 17700. Loss: 1.653678059577942. Accuracy: 40.06597310327328\n",
            "Iteration: 17800. Loss: 2.4643900394439697. Accuracy: 26.516112661760975\n",
            "Iteration: 17900. Loss: 1.5701311826705933. Accuracy: 24.99365643237757\n",
            "Iteration: 18000. Loss: 1.6293749809265137. Accuracy: 50.64704389748795\n",
            "Iteration: 18100. Loss: 2.0270111560821533. Accuracy: 46.53641207815275\n",
            "Iteration: 18200. Loss: 1.0697563886642456. Accuracy: 49.98731286475514\n",
            "Iteration: 18300. Loss: 1.7614128589630127. Accuracy: 48.10961684851561\n",
            "Iteration: 18400. Loss: 1.4213967323303223. Accuracy: 54.88454706927176\n",
            "Iteration: 18500. Loss: 1.1851177215576172. Accuracy: 44.12585638162903\n",
            "Iteration: 18600. Loss: 1.5361154079437256. Accuracy: 49.45445318447095\n",
            "Iteration: 18700. Loss: 1.5385303497314453. Accuracy: 39.50773915249937\n",
            "Iteration: 18800. Loss: 1.6943703889846802. Accuracy: 37.17330626744481\n",
            "Iteration: 18900. Loss: 2.4317619800567627. Accuracy: 29.840142095914743\n",
            "Iteration: 19000. Loss: 1.3804042339324951. Accuracy: 47.55138289774169\n",
            "Iteration: 19100. Loss: 1.3008298873901367. Accuracy: 52.09337731540218\n",
            "Iteration: 19200. Loss: 1.7982919216156006. Accuracy: 35.65085003806141\n",
            "Iteration: 19300. Loss: 1.4846725463867188. Accuracy: 46.714031971580816\n",
            "Iteration: 19400. Loss: 1.2060537338256836. Accuracy: 58.33544785587414\n",
            "Iteration: 19500. Loss: 1.3484567403793335. Accuracy: 54.45318447094646\n",
            "Iteration: 19600. Loss: 2.1003170013427734. Accuracy: 28.089317432123828\n",
            "Iteration: 19700. Loss: 2.569654703140259. Accuracy: 40.97944684090332\n",
            "Iteration: 19800. Loss: 1.6170316934585571. Accuracy: 49.47982745496067\n",
            "Iteration: 19900. Loss: 2.9036433696746826. Accuracy: 25.831007358538443\n",
            "Iteration: 20000. Loss: 1.6275392770767212. Accuracy: 39.48236488200964\n",
            "Iteration: 20100. Loss: 1.2633469104766846. Accuracy: 55.64577518396346\n",
            "Iteration: 20200. Loss: 1.5539427995681763. Accuracy: 49.14996193859427\n",
            "Iteration: 20300. Loss: 1.3263975381851196. Accuracy: 53.717330626744484\n",
            "Iteration: 20400. Loss: 1.8643113374710083. Accuracy: 33.39253996447602\n",
            "Iteration: 20500. Loss: 1.327459454536438. Accuracy: 49.63207307789901\n",
            "Iteration: 20600. Loss: 1.3053420782089233. Accuracy: 56.63537173306268\n",
            "Iteration: 20700. Loss: 1.7319414615631104. Accuracy: 38.01065719360568\n",
            "Iteration: 20800. Loss: 1.2763967514038086. Accuracy: 53.818827708703374\n",
            "Iteration: 20900. Loss: 1.3228265047073364. Accuracy: 51.738137528546055\n",
            "Iteration: 21000. Loss: 1.1077154874801636. Accuracy: 57.65034255265161\n",
            "Iteration: 21100. Loss: 2.6115283966064453. Accuracy: 32.98655163664045\n",
            "Iteration: 21200. Loss: 1.3420801162719727. Accuracy: 57.777213905100226\n",
            "Iteration: 21300. Loss: 1.2909190654754639. Accuracy: 55.671149454453186\n",
            "Iteration: 21400. Loss: 1.1150455474853516. Accuracy: 61.837097183455974\n",
            "Iteration: 21500. Loss: 2.002382516860962. Accuracy: 39.98985029180411\n",
            "Iteration: 21600. Loss: 1.0330615043640137. Accuracy: 62.293834052270995\n",
            "Iteration: 21700. Loss: 1.3762478828430176. Accuracy: 39.736107586906876\n",
            "Iteration: 21800. Loss: 1.1193212270736694. Accuracy: 59.40116721644253\n",
            "Iteration: 21900. Loss: 1.2114754915237427. Accuracy: 61.48185739659985\n",
            "Iteration: 22000. Loss: 1.196203351020813. Accuracy: 60.97437198680538\n",
            "Iteration: 22100. Loss: 0.8133163452148438. Accuracy: 60.08627251966506\n",
            "Iteration: 22200. Loss: 1.709713101387024. Accuracy: 32.04770362852068\n",
            "Iteration: 22300. Loss: 1.380393147468567. Accuracy: 51.89038315148439\n",
            "Iteration: 22400. Loss: 1.6045339107513428. Accuracy: 49.17533620908399\n",
            "Iteration: 22500. Loss: 1.3033039569854736. Accuracy: 60.61913219994925\n",
            "Iteration: 22600. Loss: 1.278024435043335. Accuracy: 65.13575234712002\n",
            "Iteration: 22700. Loss: 1.154076337814331. Accuracy: 54.250190307028674\n",
            "Iteration: 22800. Loss: 1.4958546161651611. Accuracy: 61.735600101497084\n",
            "Iteration: 22900. Loss: 1.0159960985183716. Accuracy: 63.20730778990104\n",
            "Iteration: 23000. Loss: 1.2120424509048462. Accuracy: 41.68992641461558\n",
            "Iteration: 23100. Loss: 0.9661064743995667. Accuracy: 47.196143110885565\n",
            "Iteration: 23200. Loss: 0.8218508958816528. Accuracy: 61.329611773661505\n",
            "Iteration: 23300. Loss: 1.2473138570785522. Accuracy: 61.151991880233446\n",
            "Iteration: 23400. Loss: 1.4906893968582153. Accuracy: 63.51179903577772\n",
            "Iteration: 23500. Loss: 1.012865662574768. Accuracy: 61.17736615072317\n",
            "Iteration: 23600. Loss: 1.1045327186584473. Accuracy: 61.735600101497084\n",
            "Iteration: 23700. Loss: 1.6651815176010132. Accuracy: 52.803856889114435\n",
            "Iteration: 23800. Loss: 1.30001699924469. Accuracy: 58.690687642730275\n",
            "Iteration: 23900. Loss: 2.0525498390197754. Accuracy: 28.901294087794977\n",
            "Iteration: 24000. Loss: 1.5066442489624023. Accuracy: 48.38873382390256\n",
            "Iteration: 24100. Loss: 1.1626938581466675. Accuracy: 60.08627251966506\n",
            "Iteration: 24200. Loss: 1.8719799518585205. Accuracy: 55.97564070032987\n",
            "Iteration: 24300. Loss: 1.1990697383880615. Accuracy: 35.11799035777722\n",
            "Iteration: 24400. Loss: 1.074602723121643. Accuracy: 54.58005582339508\n",
            "Iteration: 24500. Loss: 1.2531425952911377. Accuracy: 62.95356508500381\n",
            "Iteration: 24600. Loss: 1.0067579746246338. Accuracy: 63.3849276833291\n",
            "Iteration: 24700. Loss: 1.1267024278640747. Accuracy: 51.00228368434408\n",
            "Iteration: 24800. Loss: 2.238642692565918. Accuracy: 36.46282669373255\n",
            "Iteration: 24900. Loss: 1.065293312072754. Accuracy: 53.46358792184725\n",
            "Iteration: 25000. Loss: 1.2820004224777222. Accuracy: 51.306774930220755\n",
            "Iteration: 25100. Loss: 2.132174253463745. Accuracy: 26.795229637147933\n",
            "Iteration: 25200. Loss: 1.1586966514587402. Accuracy: 44.02435929967013\n",
            "Iteration: 25300. Loss: 0.9083213806152344. Accuracy: 62.09083988835321\n",
            "Iteration: 25400. Loss: 1.3945655822753906. Accuracy: 64.19690433900026\n",
            "Iteration: 25500. Loss: 1.1362199783325195. Accuracy: 68.12991626490738\n",
            "Iteration: 25600. Loss: 0.8635057806968689. Accuracy: 70.03298655163664\n",
            "Iteration: 25700. Loss: 0.7370586395263672. Accuracy: 70.28672925653387\n",
            "Iteration: 25800. Loss: 1.264100432395935. Accuracy: 58.360822126363864\n",
            "Iteration: 25900. Loss: 0.9801105260848999. Accuracy: 58.48769347881248\n",
            "Iteration: 26000. Loss: 1.0711406469345093. Accuracy: 60.695255011418425\n",
            "Iteration: 26100. Loss: 1.4346067905426025. Accuracy: 56.58462319208323\n",
            "Iteration: 26200. Loss: 0.8197926878929138. Accuracy: 68.66277594519157\n",
            "Iteration: 26300. Loss: 1.1945881843566895. Accuracy: 65.26262369956864\n",
            "Iteration: 26400. Loss: 1.199661135673523. Accuracy: 62.47145394569906\n",
            "Iteration: 26500. Loss: 0.9823172092437744. Accuracy: 71.22557726465364\n",
            "Iteration: 26600. Loss: 0.8989335298538208. Accuracy: 64.70438974879472\n",
            "Iteration: 26700. Loss: 1.0328024625778198. Accuracy: 67.08957117482872\n",
            "Iteration: 26800. Loss: 2.6245572566986084. Accuracy: 37.02106064450647\n",
            "Iteration: 26900. Loss: 0.6883506178855896. Accuracy: 72.97640192844456\n",
            "Iteration: 27000. Loss: 0.9232374429702759. Accuracy: 73.05252473991372\n",
            "Iteration: 27100. Loss: 1.1740132570266724. Accuracy: 69.14488708449632\n",
            "Iteration: 27200. Loss: 1.3562158346176147. Accuracy: 52.37249429078914\n",
            "Iteration: 27300. Loss: 0.7390703558921814. Accuracy: 71.47931996955087\n",
            "Iteration: 27400. Loss: 0.7181808948516846. Accuracy: 71.27632580563309\n",
            "Iteration: 27500. Loss: 1.2638139724731445. Accuracy: 59.70565846231921\n",
            "Iteration: 27600. Loss: 1.2467159032821655. Accuracy: 67.64780512560264\n",
            "Iteration: 27700. Loss: 0.8927765488624573. Accuracy: 68.256787617356\n",
            "Iteration: 27800. Loss: 0.9349422454833984. Accuracy: 72.51966505962953\n",
            "Iteration: 27900. Loss: 0.9937158823013306. Accuracy: 58.716061913219995\n",
            "Iteration: 28000. Loss: 1.266097068786621. Accuracy: 57.726465364120784\n",
            "Iteration: 28100. Loss: 1.283318281173706. Accuracy: 61.5326059375793\n",
            "Iteration: 28200. Loss: 0.7761359214782715. Accuracy: 65.71936056838366\n",
            "Iteration: 28300. Loss: 0.971972644329071. Accuracy: 72.95102765795484\n",
            "Iteration: 28400. Loss: 1.1673403978347778. Accuracy: 66.07460035523978\n",
            "Iteration: 28500. Loss: 1.1143163442611694. Accuracy: 71.14945445318448\n",
            "Iteration: 28600. Loss: 1.3109794855117798. Accuracy: 50.46942400405988\n",
            "Iteration: 28700. Loss: 0.7378475666046143. Accuracy: 72.21517381375286\n",
            "Iteration: 28800. Loss: 0.7027952075004578. Accuracy: 72.18979954326313\n",
            "Iteration: 28900. Loss: 1.3942526578903198. Accuracy: 52.95610251205278\n",
            "Iteration: 29000. Loss: 1.0291481018066406. Accuracy: 53.8442019791931\n",
            "Iteration: 29100. Loss: 0.6573586463928223. Accuracy: 70.99720883024614\n",
            "Iteration: 29200. Loss: 0.5589898824691772. Accuracy: 69.62699822380107\n",
            "Iteration: 29300. Loss: 1.4444878101348877. Accuracy: 67.90154783049988\n",
            "Iteration: 29400. Loss: 1.5301026105880737. Accuracy: 61.101243339254\n",
            "Iteration: 29500. Loss: 0.7865658402442932. Accuracy: 75.31083481349911\n",
            "Iteration: 29600. Loss: 1.1242414712905884. Accuracy: 71.09870591220502\n",
            "Iteration: 29700. Loss: 0.7356849908828735. Accuracy: 71.14945445318448\n",
            "Iteration: 29800. Loss: 1.4886224269866943. Accuracy: 54.40243592996701\n",
            "Iteration: 29900. Loss: 0.5349820852279663. Accuracy: 76.75716823141335\n",
            "Iteration: 30000. Loss: 0.8675417304039001. Accuracy: 72.44354224816037\n",
            "Iteration: 30100. Loss: 0.6017410159111023. Accuracy: 77.44227353463587\n",
            "Iteration: 30200. Loss: 1.587308406829834. Accuracy: 30.398376046688657\n",
            "Iteration: 30300. Loss: 0.8019139766693115. Accuracy: 74.67647805125603\n",
            "Iteration: 30400. Loss: 0.7348679900169373. Accuracy: 73.38239025628013\n",
            "Iteration: 30500. Loss: 1.0413563251495361. Accuracy: 69.06876427302716\n",
            "Iteration: 30600. Loss: 1.3535312414169312. Accuracy: 57.0921086018777\n",
            "Iteration: 30700. Loss: 0.7579873204231262. Accuracy: 71.88530829738644\n",
            "Iteration: 30800. Loss: 0.640057384967804. Accuracy: 75.58995178888607\n",
            "Iteration: 30900. Loss: 0.8389783501625061. Accuracy: 76.30043136259833\n",
            "Iteration: 31000. Loss: 1.330923318862915. Accuracy: 63.28343060137021\n",
            "Iteration: 31100. Loss: 0.868582010269165. Accuracy: 69.72849530575996\n",
            "Iteration: 31200. Loss: 0.7306246757507324. Accuracy: 70.84496320730779\n",
            "Iteration: 31300. Loss: 0.9823600053787231. Accuracy: 70.66734331387973\n",
            "Iteration: 31400. Loss: 0.6230682730674744. Accuracy: 78.990104034509\n",
            "Iteration: 31500. Loss: 0.5898167490959167. Accuracy: 74.72722659223547\n",
            "Iteration: 31600. Loss: 0.959291398525238. Accuracy: 71.1748287236742\n",
            "Iteration: 31700. Loss: 0.9697911739349365. Accuracy: 69.82999238771885\n",
            "Iteration: 31800. Loss: 0.33776676654815674. Accuracy: 78.45724435422481\n",
            "Iteration: 31900. Loss: 1.730436086654663. Accuracy: 44.50647043897488\n",
            "Iteration: 32000. Loss: 1.0393961668014526. Accuracy: 74.7526008627252\n",
            "Iteration: 32100. Loss: 0.5946346521377563. Accuracy: 77.11240801826948\n",
            "Iteration: 32200. Loss: 0.7661563754081726. Accuracy: 77.26465364120781\n",
            "Iteration: 32300. Loss: 1.2789828777313232. Accuracy: 69.0180157320477\n",
            "Iteration: 32400. Loss: 0.6384047269821167. Accuracy: 76.3765541740675\n",
            "Iteration: 32500. Loss: 0.8709362149238586. Accuracy: 76.07206292819082\n",
            "Iteration: 32600. Loss: 0.6883281469345093. Accuracy: 75.41233189545801\n",
            "Iteration: 32700. Loss: 1.4042744636535645. Accuracy: 71.65693986297894\n",
            "Iteration: 32800. Loss: 0.6559960246086121. Accuracy: 73.48388733823903\n",
            "Iteration: 32900. Loss: 0.7726714611053467. Accuracy: 76.88403958386196\n",
            "Iteration: 33000. Loss: 0.7064146995544434. Accuracy: 74.72722659223547\n",
            "Iteration: 33100. Loss: 1.1814005374908447. Accuracy: 76.57954833798529\n",
            "Iteration: 33200. Loss: 0.6714287400245667. Accuracy: 78.58411570667343\n",
            "Iteration: 33300. Loss: 1.5811256170272827. Accuracy: 39.634610504947986\n",
            "Iteration: 33400. Loss: 0.5087872743606567. Accuracy: 74.42273534635879\n",
            "Iteration: 33500. Loss: 0.9223431944847107. Accuracy: 66.12534889621924\n",
            "Iteration: 33600. Loss: 0.6430296897888184. Accuracy: 79.11697538695762\n",
            "Iteration: 33700. Loss: 0.997084379196167. Accuracy: 70.9718345597564\n",
            "Iteration: 33800. Loss: 0.9411905407905579. Accuracy: 75.46308043643745\n",
            "Iteration: 33900. Loss: 0.9730790853500366. Accuracy: 73.027150469424\n",
            "Iteration: 34000. Loss: 0.5065056681632996. Accuracy: 80.84242578025882\n",
            "Iteration: 34100. Loss: 0.4571971297264099. Accuracy: 78.12737883785842\n",
            "Iteration: 34200. Loss: 0.49366989731788635. Accuracy: 81.93351941131692\n",
            "Iteration: 34300. Loss: 0.8342969417572021. Accuracy: 78.78710987059122\n",
            "Iteration: 34400. Loss: 0.6175916790962219. Accuracy: 81.78127378837858\n",
            "Iteration: 34500. Loss: 0.4983564615249634. Accuracy: 77.94975894443034\n",
            "Iteration: 34600. Loss: 0.7722389698028564. Accuracy: 73.86450139558488\n",
            "Iteration: 34700. Loss: 0.418967604637146. Accuracy: 73.61075869068765\n",
            "Iteration: 34800. Loss: 0.5151521563529968. Accuracy: 76.52879979700583\n",
            "Iteration: 34900. Loss: 0.6797056198120117. Accuracy: 73.45851306774931\n",
            "Iteration: 35000. Loss: 1.3884801864624023. Accuracy: 66.58208576503425\n",
            "Iteration: 35100. Loss: 0.5639258623123169. Accuracy: 79.24384673940624\n",
            "Iteration: 35200. Loss: 0.9038545489311218. Accuracy: 77.54377061659477\n",
            "Iteration: 35300. Loss: 0.3570728003978729. Accuracy: 82.36488200964223\n",
            "Iteration: 35400. Loss: 0.4184325635433197. Accuracy: 80.91854859172798\n",
            "Iteration: 35500. Loss: 0.5651933550834656. Accuracy: 74.52423242831769\n",
            "Iteration: 35600. Loss: 0.5717517733573914. Accuracy: 79.92895204262878\n",
            "Iteration: 35700. Loss: 0.6340278387069702. Accuracy: 77.29002791169754\n",
            "Iteration: 35800. Loss: 0.4652177393436432. Accuracy: 78.27962446079675\n",
            "Iteration: 35900. Loss: 0.5157747268676758. Accuracy: 82.28875919817305\n",
            "Iteration: 36000. Loss: 0.566726803779602. Accuracy: 76.96016239533114\n",
            "Iteration: 36100. Loss: 0.35672080516815186. Accuracy: 83.2022329358031\n",
            "Iteration: 36200. Loss: 0.553926944732666. Accuracy: 73.48388733823903\n",
            "Iteration: 36300. Loss: 0.6399568319320679. Accuracy: 73.48388733823903\n",
            "Iteration: 36400. Loss: 0.28403469920158386. Accuracy: 83.83658969804618\n",
            "Iteration: 36500. Loss: 0.34454819560050964. Accuracy: 82.16188784572444\n",
            "Iteration: 36600. Loss: 1.0218852758407593. Accuracy: 72.59578787109871\n",
            "Iteration: 36700. Loss: 0.6326196193695068. Accuracy: 79.31996955087541\n",
            "Iteration: 36800. Loss: 0.7190983891487122. Accuracy: 75.91981730525248\n",
            "Iteration: 36900. Loss: 0.3822803795337677. Accuracy: 81.78127378837858\n",
            "Iteration: 37000. Loss: 0.26607558131217957. Accuracy: 82.97386450139558\n",
            "Iteration: 37100. Loss: 1.654159665107727. Accuracy: 46.790154783049985\n",
            "Iteration: 37200. Loss: 0.4160548448562622. Accuracy: 79.9543263131185\n",
            "Iteration: 37300. Loss: 0.6788535118103027. Accuracy: 76.80791677239279\n",
            "Iteration: 37400. Loss: 0.5526310205459595. Accuracy: 74.29586399391017\n",
            "Iteration: 37500. Loss: 0.42840656638145447. Accuracy: 80.3856889114438\n",
            "Iteration: 37600. Loss: 0.47527027130126953. Accuracy: 76.78254250190307\n",
            "Iteration: 37700. Loss: 0.5510925054550171. Accuracy: 77.54377061659477\n",
            "Iteration: 37800. Loss: 0.47143417596817017. Accuracy: 80.61405734585131\n",
            "Iteration: 37900. Loss: 0.4102051556110382. Accuracy: 81.67977670641969\n",
            "Iteration: 38000. Loss: 0.35251951217651367. Accuracy: 82.87236741943669\n",
            "Iteration: 38100. Loss: 0.9490903615951538. Accuracy: 70.56584623192083\n",
            "Iteration: 38200. Loss: 0.6891807913780212. Accuracy: 80.48718599340269\n",
            "Iteration: 38300. Loss: 0.463381290435791. Accuracy: 79.87820350164932\n",
            "Iteration: 38400. Loss: 0.7264370918273926. Accuracy: 72.34204516620147\n",
            "Iteration: 38500. Loss: 0.4384174644947052. Accuracy: 80.20806901801573\n",
            "Iteration: 38600. Loss: 0.5777114629745483. Accuracy: 79.57371225577265\n",
            "Iteration: 38700. Loss: 0.38023000955581665. Accuracy: 84.0649581324537\n",
            "Iteration: 38800. Loss: 0.550359308719635. Accuracy: 74.72722659223547\n",
            "Iteration: 38900. Loss: 0.28644075989723206. Accuracy: 79.24384673940624\n",
            "Iteration: 39000. Loss: 0.6075527667999268. Accuracy: 82.06039076376554\n",
            "Iteration: 39100. Loss: 0.5620116591453552. Accuracy: 79.14234965744735\n",
            "Iteration: 39200. Loss: 0.7988981008529663. Accuracy: 78.78710987059122\n",
            "Iteration: 39300. Loss: 0.30662718415260315. Accuracy: 84.44557218979955\n",
            "Iteration: 39400. Loss: 0.35081905126571655. Accuracy: 78.6602385181426\n",
            "Iteration: 39500. Loss: 0.913520336151123. Accuracy: 71.834559756407\n",
            "Iteration: 39600. Loss: 0.4678407609462738. Accuracy: 81.09616848515606\n",
            "Iteration: 39700. Loss: 0.41553249955177307. Accuracy: 78.2288759198173\n",
            "Iteration: 39800. Loss: 0.7810125350952148. Accuracy: 78.83785841157066\n",
            "Iteration: 39900. Loss: 0.8666735887527466. Accuracy: 64.93275818320224\n",
            "Iteration: 40000. Loss: 0.5587670207023621. Accuracy: 85.28292311596041\n",
            "Iteration: 40100. Loss: 0.512374997138977. Accuracy: 78.43187008373509\n",
            "Iteration: 40200. Loss: 0.6820361018180847. Accuracy: 73.2301446333418\n",
            "Iteration: 40300. Loss: 0.7037785053253174. Accuracy: 74.54960669880741\n",
            "Iteration: 40400. Loss: 0.15921670198440552. Accuracy: 84.16645521441258\n",
            "Iteration: 40500. Loss: 0.4328625798225403. Accuracy: 82.74549606698807\n",
            "Iteration: 40600. Loss: 0.3111608624458313. Accuracy: 84.47094646028927\n",
            "Iteration: 40700. Loss: 0.41822344064712524. Accuracy: 82.64399898502919\n",
            "Iteration: 40800. Loss: 0.36309927701950073. Accuracy: 85.2575488454707\n",
            "Iteration: 40900. Loss: 0.3672293424606323. Accuracy: 83.17685866531338\n",
            "Iteration: 41000. Loss: 0.2645231783390045. Accuracy: 84.87693478812484\n",
            "Iteration: 41100. Loss: 0.2044711709022522. Accuracy: 83.83658969804618\n",
            "Iteration: 41200. Loss: 0.455990195274353. Accuracy: 74.06749555950266\n",
            "Iteration: 41300. Loss: 0.5976462364196777. Accuracy: 76.07206292819082\n",
            "Iteration: 41400. Loss: 0.31353282928466797. Accuracy: 83.27835574727227\n",
            "Iteration: 41500. Loss: 0.4439338743686676. Accuracy: 83.12611012433392\n",
            "Iteration: 41600. Loss: 0.2517971098423004. Accuracy: 84.59781781273789\n",
            "Iteration: 41700. Loss: 0.33264273405075073. Accuracy: 85.00380614057346\n",
            "Iteration: 41800. Loss: 0.26500561833381653. Accuracy: 84.87693478812484\n",
            "Iteration: 41900. Loss: 1.2422086000442505. Accuracy: 71.35244861710225\n",
            "Iteration: 42000. Loss: 0.8082838654518127. Accuracy: 80.79167723927937\n",
            "Iteration: 42100. Loss: 0.20867455005645752. Accuracy: 85.2575488454707\n",
            "Iteration: 42200. Loss: 0.5172545909881592. Accuracy: 79.5229637147932\n",
            "Iteration: 42300. Loss: 0.308513343334198. Accuracy: 84.14108094392286\n",
            "Iteration: 42400. Loss: 0.3924918472766876. Accuracy: 84.26795229637148\n",
            "Iteration: 42500. Loss: 0.39689919352531433. Accuracy: 81.75589951788886\n",
            "Iteration: 42600. Loss: 0.21579928696155548. Accuracy: 84.97843187008374\n",
            "Iteration: 42700. Loss: 0.45138218998908997. Accuracy: 79.97970058360822\n",
            "Iteration: 42800. Loss: 0.37250739336013794. Accuracy: 79.26922100989597\n",
            "Iteration: 42900. Loss: 0.20817360281944275. Accuracy: 85.99340268967268\n",
            "Iteration: 43000. Loss: 0.2921275496482849. Accuracy: 85.8157827962446\n",
            "Iteration: 43100. Loss: 0.20329415798187256. Accuracy: 82.69474752600863\n",
            "Iteration: 43200. Loss: 0.24579249322414398. Accuracy: 85.33367165693986\n",
            "Iteration: 43300. Loss: 0.21576936542987823. Accuracy: 85.20680030449125\n",
            "Iteration: 43400. Loss: 0.1462317556142807. Accuracy: 85.73965998477544\n",
            "Iteration: 43500. Loss: 0.19866901636123657. Accuracy: 86.17102258310074\n",
            "Iteration: 43600. Loss: 0.20735538005828857. Accuracy: 86.83075361583354\n",
            "Iteration: 43700. Loss: 0.3157060444355011. Accuracy: 85.40979446840903\n",
            "Iteration: 43800. Loss: 0.4596714973449707. Accuracy: 74.80334940370464\n",
            "Iteration: 43900. Loss: 0.5390398502349854. Accuracy: 83.760466886577\n",
            "Iteration: 44000. Loss: 0.39448660612106323. Accuracy: 83.43060137021061\n",
            "Iteration: 44100. Loss: 0.5430586934089661. Accuracy: 73.94062420705404\n",
            "Iteration: 44200. Loss: 0.41770532727241516. Accuracy: 81.9081451408272\n",
            "Iteration: 44300. Loss: 0.39985576272010803. Accuracy: 80.84242578025882\n",
            "Iteration: 44400. Loss: 0.42012539505958557. Accuracy: 77.26465364120781\n",
            "Iteration: 44500. Loss: 0.21231307089328766. Accuracy: 81.45140827201217\n",
            "Iteration: 44600. Loss: 0.2722552418708801. Accuracy: 84.67394062420705\n",
            "Iteration: 44700. Loss: 0.1424017697572708. Accuracy: 83.60822126363867\n",
            "Iteration: 44800. Loss: 0.18053396046161652. Accuracy: 86.72925653387465\n",
            "Iteration: 44900. Loss: 0.3084927201271057. Accuracy: 83.65896980461812\n",
            "Iteration: 45000. Loss: 0.3438868224620819. Accuracy: 84.64856635371733\n",
            "Iteration: 45100. Loss: 0.3986360430717468. Accuracy: 82.111139304745\n",
            "Iteration: 45200. Loss: 0.26034435629844666. Accuracy: 85.76503425526516\n",
            "Iteration: 45300. Loss: 0.23220252990722656. Accuracy: 86.50088809946715\n",
            "Iteration: 45400. Loss: 0.30194446444511414. Accuracy: 85.58741436183709\n",
            "Iteration: 45500. Loss: 0.36039119958877563. Accuracy: 85.02918041106318\n",
            "Iteration: 45600. Loss: 0.20475097000598907. Accuracy: 83.81121542755646\n",
            "Iteration: 45700. Loss: 0.5103365182876587. Accuracy: 81.65440243592997\n",
            "Iteration: 45800. Loss: 0.16558027267456055. Accuracy: 86.04415123065212\n",
            "Iteration: 45900. Loss: 0.1294693946838379. Accuracy: 85.38442019791931\n",
            "Iteration: 46000. Loss: 0.2420504242181778. Accuracy: 85.73965998477544\n",
            "Iteration: 46100. Loss: 0.23827166855335236. Accuracy: 85.48591727987821\n",
            "Iteration: 46200. Loss: 0.26497316360473633. Accuracy: 86.19639685359046\n",
            "Iteration: 46300. Loss: 0.29843634366989136. Accuracy: 86.52626236995687\n",
            "Iteration: 46400. Loss: 0.43713220953941345. Accuracy: 82.49175336209083\n",
            "Iteration: 46500. Loss: 0.30312058329582214. Accuracy: 85.07992895204264\n",
            "Iteration: 46600. Loss: 0.12215874344110489. Accuracy: 87.0083735092616\n",
            "Iteration: 46700. Loss: 0.14956508576869965. Accuracy: 86.93225069779244\n",
            "Iteration: 46800. Loss: 0.5411964058876038. Accuracy: 81.75589951788886\n",
            "Iteration: 46900. Loss: 0.4318546652793884. Accuracy: 86.45013955848769\n",
            "Iteration: 47000. Loss: 0.4617786407470703. Accuracy: 82.18726211621416\n",
            "Iteration: 47100. Loss: 0.19310930371284485. Accuracy: 87.0083735092616\n",
            "Iteration: 47200. Loss: 0.1909514218568802. Accuracy: 86.14564831261102\n",
            "Iteration: 47300. Loss: 0.3821732699871063. Accuracy: 84.19182948490231\n",
            "Iteration: 47400. Loss: 0.22443170845508575. Accuracy: 87.08449632073078\n",
            "Iteration: 47500. Loss: 0.5514881014823914. Accuracy: 84.52169500126871\n",
            "Iteration: 47600. Loss: 0.5575846433639526. Accuracy: 87.31286475513829\n",
            "Iteration: 47700. Loss: 0.188191220164299. Accuracy: 85.89190560771378\n",
            "Iteration: 47800. Loss: 0.2814634442329407. Accuracy: 86.17102258310074\n",
            "Iteration: 47900. Loss: 0.37793946266174316. Accuracy: 82.92311596041614\n",
            "Iteration: 48000. Loss: 0.36811327934265137. Accuracy: 85.99340268967268\n",
            "Iteration: 48100. Loss: 0.1912800669670105. Accuracy: 86.24714539456991\n",
            "Iteration: 48200. Loss: 0.3641217350959778. Accuracy: 81.8066480588683\n",
            "Iteration: 48300. Loss: 0.3836638927459717. Accuracy: 82.06039076376554\n",
            "Iteration: 48400. Loss: 0.17070168256759644. Accuracy: 86.85612788632326\n",
            "Iteration: 48500. Loss: 0.20391303300857544. Accuracy: 85.38442019791931\n",
            "Iteration: 48600. Loss: 0.10029033571481705. Accuracy: 87.2113676731794\n",
            "Iteration: 48700. Loss: 0.6944267153739929. Accuracy: 77.56914488708449\n",
            "Iteration: 48800. Loss: 0.36464646458625793. Accuracy: 80.0558233950774\n",
            "Iteration: 48900. Loss: 0.14928779006004333. Accuracy: 87.49048464856635\n",
            "Iteration: 49000. Loss: 0.2142256200313568. Accuracy: 87.1098705912205\n",
            "Iteration: 49100. Loss: 0.18627387285232544. Accuracy: 86.95762496828216\n",
            "Iteration: 49200. Loss: 0.22022446990013123. Accuracy: 85.8157827962446\n",
            "Iteration: 49300. Loss: 0.3078387677669525. Accuracy: 79.47221517381375\n",
            "Iteration: 49400. Loss: 0.23452378809452057. Accuracy: 83.32910428825171\n",
            "Iteration: 49500. Loss: 2.6005284786224365. Accuracy: 33.773154021821874\n",
            "Iteration: 49600. Loss: 0.3198796808719635. Accuracy: 84.03958386196396\n",
            "Iteration: 49700. Loss: 0.1248687133193016. Accuracy: 86.85612788632326\n",
            "Iteration: 49800. Loss: 0.19380706548690796. Accuracy: 87.7696016239533\n",
            "Iteration: 49900. Loss: 0.07025469839572906. Accuracy: 86.85612788632326\n",
            "Iteration: 50000. Loss: 0.2994987666606903. Accuracy: 86.37401674701853\n",
            "Iteration: 50100. Loss: 1.0087785720825195. Accuracy: 50.164932758183205\n",
            "Iteration: 50200. Loss: 0.3039301931858063. Accuracy: 80.91854859172798\n",
            "Iteration: 50300. Loss: 0.1669406294822693. Accuracy: 87.23674194366912\n",
            "Iteration: 50400. Loss: 0.08702011406421661. Accuracy: 87.59198173052525\n",
            "Iteration: 50500. Loss: 0.17924201488494873. Accuracy: 83.98883532098452\n",
            "Iteration: 50600. Loss: 0.3158273994922638. Accuracy: 80.66480588683075\n",
            "Iteration: 50700. Loss: 0.3101087510585785. Accuracy: 87.05912205024106\n",
            "Iteration: 50800. Loss: 0.15710367262363434. Accuracy: 87.79497589444304\n",
            "Iteration: 50900. Loss: 0.07589869201183319. Accuracy: 87.1098705912205\n",
            "Iteration: 51000. Loss: 0.31280404329299927. Accuracy: 83.81121542755646\n",
            "Iteration: 51100. Loss: 0.20496924221515656. Accuracy: 87.5412331895458\n",
            "Iteration: 51200. Loss: 0.23796799778938293. Accuracy: 84.7246891651865\n",
            "Iteration: 51300. Loss: 0.09539583325386047. Accuracy: 87.28749048464857\n",
            "Iteration: 51400. Loss: 0.13019925355911255. Accuracy: 87.84572443542248\n",
            "Iteration: 51500. Loss: 0.21267729997634888. Accuracy: 85.76503425526516\n",
            "Iteration: 51600. Loss: 0.7980433702468872. Accuracy: 76.80791677239279\n",
            "Iteration: 51700. Loss: 0.2264285385608673. Accuracy: 84.16645521441258\n",
            "Iteration: 51800. Loss: 0.22184313833713531. Accuracy: 83.58284699314895\n",
            "Iteration: 51900. Loss: 0.5207588076591492. Accuracy: 80.03044912458766\n",
            "Iteration: 52000. Loss: 0.3449042737483978. Accuracy: 86.83075361583354\n",
            "Iteration: 52100. Loss: 0.1730923056602478. Accuracy: 85.9172798782035\n",
            "Iteration: 52200. Loss: 0.415608674287796. Accuracy: 81.40065973103273\n",
            "Iteration: 52300. Loss: 0.14957785606384277. Accuracy: 88.4039583861964\n",
            "Iteration: 52400. Loss: 0.2742410898208618. Accuracy: 85.07992895204264\n",
            "Iteration: 52500. Loss: 0.264849454164505. Accuracy: 87.0083735092616\n",
            "Iteration: 52600. Loss: 0.051491767168045044. Accuracy: 87.46511037807663\n",
            "Iteration: 52700. Loss: 0.11778143048286438. Accuracy: 86.90687642730272\n",
            "Iteration: 52800. Loss: 0.14807918667793274. Accuracy: 87.28749048464857\n",
            "Iteration: 52900. Loss: 0.14378327131271362. Accuracy: 87.33823902562801\n",
            "Iteration: 53000. Loss: 0.16472584009170532. Accuracy: 83.40522709972089\n",
            "Iteration: 53100. Loss: 0.7054913640022278. Accuracy: 77.41689926414615\n",
            "Iteration: 53200. Loss: 0.12271400541067123. Accuracy: 86.17102258310074\n",
            "Iteration: 53300. Loss: 0.23986461758613586. Accuracy: 85.1560517635118\n",
            "Iteration: 53400. Loss: 0.06483877450227737. Accuracy: 88.37858411570667\n",
            "Iteration: 53500. Loss: 0.3053613305091858. Accuracy: 83.8619639685359\n",
            "Iteration: 53600. Loss: 1.0804476737976074. Accuracy: 67.64780512560264\n",
            "Iteration: 53700. Loss: 0.17320027947425842. Accuracy: 83.45597564070033\n",
            "Iteration: 53800. Loss: 0.11136012524366379. Accuracy: 88.20096422227861\n",
            "Iteration: 53900. Loss: 0.041526585817337036. Accuracy: 87.41436183709719\n",
            "Iteration: 54000. Loss: 0.22015602886676788. Accuracy: 85.56204009134737\n",
            "Iteration: 54100. Loss: 0.09510865062475204. Accuracy: 87.38898756660745\n",
            "Iteration: 54200. Loss: 0.16995100677013397. Accuracy: 86.98299923877188\n",
            "Iteration: 54300. Loss: 0.2750070095062256. Accuracy: 84.87693478812484\n",
            "Iteration: 54400. Loss: 0.07970317453145981. Accuracy: 85.94265414869322\n",
            "Iteration: 54500. Loss: 0.13732561469078064. Accuracy: 87.18599340268968\n",
            "Iteration: 54600. Loss: 0.13606038689613342. Accuracy: 86.98299923877188\n",
            "Iteration: 54700. Loss: 0.13186930119991302. Accuracy: 86.27251966505963\n",
            "Iteration: 54800. Loss: 0.3560478389263153. Accuracy: 81.29916264907384\n",
            "Iteration: 54900. Loss: 0.27759110927581787. Accuracy: 85.35904592742959\n",
            "Iteration: 55000. Loss: 0.10728088766336441. Accuracy: 87.49048464856635\n",
            "Iteration: 55100. Loss: 0.13421204686164856. Accuracy: 87.31286475513829\n",
            "Iteration: 55200. Loss: 0.2048237919807434. Accuracy: 86.50088809946715\n",
            "Iteration: 55300. Loss: 0.16820885241031647. Accuracy: 86.04415123065212\n",
            "Iteration: 55400. Loss: 0.2228628545999527. Accuracy: 86.22177112408018\n",
            "Iteration: 55500. Loss: 1.4095066785812378. Accuracy: 71.14945445318448\n",
            "Iteration: 55600. Loss: 0.09066752344369888. Accuracy: 86.37401674701853\n",
            "Iteration: 55700. Loss: 0.14573483169078827. Accuracy: 86.55163664044659\n",
            "Iteration: 55800. Loss: 0.3160729706287384. Accuracy: 84.67394062420705\n",
            "Iteration: 55900. Loss: 0.15508872270584106. Accuracy: 84.7246891651865\n",
            "Iteration: 56000. Loss: 0.10073426365852356. Accuracy: 87.7696016239533\n",
            "Iteration: 56100. Loss: 0.7568053007125854. Accuracy: 72.64653641207815\n",
            "Iteration: 56200. Loss: 0.336129754781723. Accuracy: 84.14108094392286\n",
            "Iteration: 56300. Loss: 0.307685524225235. Accuracy: 78.58411570667343\n",
            "Iteration: 56400. Loss: 0.28320640325546265. Accuracy: 86.3486424765288\n",
            "Iteration: 56500. Loss: 0.26311051845550537. Accuracy: 84.3948236488201\n",
            "Iteration: 56600. Loss: 0.10761085897684097. Accuracy: 87.5412331895458\n",
            "Iteration: 56700. Loss: 0.1969599425792694. Accuracy: 88.25171276325806\n",
            "Iteration: 56800. Loss: 0.18956075608730316. Accuracy: 86.09489977163156\n",
            "Iteration: 56900. Loss: 0.09652349352836609. Accuracy: 87.59198173052525\n",
            "Iteration: 57000. Loss: 0.34775036573410034. Accuracy: 80.89317432123826\n",
            "Iteration: 57100. Loss: 0.06233125925064087. Accuracy: 87.9725957878711\n",
            "Iteration: 57200. Loss: 0.20395927131175995. Accuracy: 87.08449632073078\n",
            "Iteration: 57300. Loss: 0.06595046818256378. Accuracy: 87.64273027150469\n",
            "Iteration: 57400. Loss: 0.19731517136096954. Accuracy: 85.66353717330627\n",
            "Iteration: 57500. Loss: 0.1731559932231903. Accuracy: 87.08449632073078\n",
            "Iteration: 57600. Loss: 0.08802541345357895. Accuracy: 87.08449632073078\n",
            "Iteration: 57700. Loss: 0.3122641146183014. Accuracy: 83.58284699314895\n",
            "Iteration: 57800. Loss: 1.9509005546569824. Accuracy: 62.52220248667851\n",
            "Iteration: 57900. Loss: 0.13978035748004913. Accuracy: 86.1202740421213\n",
            "Iteration: 58000. Loss: 0.18552562594413757. Accuracy: 87.56660746003553\n",
            "Iteration: 58100. Loss: 0.05775734782218933. Accuracy: 86.62775945191575\n",
            "Iteration: 58200. Loss: 0.15919533371925354. Accuracy: 86.60238518142603\n",
            "Iteration: 58300. Loss: 0.23938316106796265. Accuracy: 86.50088809946715\n",
            "Iteration: 58400. Loss: 0.1325848251581192. Accuracy: 86.22177112408018\n",
            "Iteration: 58500. Loss: 0.12914222478866577. Accuracy: 87.36361329611773\n",
            "Iteration: 58600. Loss: 0.06866493821144104. Accuracy: 88.45470692717585\n",
            "Iteration: 58700. Loss: 0.08418508619070053. Accuracy: 87.66810454199442\n",
            "Iteration: 58800. Loss: 0.16489481925964355. Accuracy: 86.75463080436437\n",
            "Iteration: 58900. Loss: 0.08017375320196152. Accuracy: 87.92184724689166\n",
            "Iteration: 59000. Loss: 0.45724180340766907. Accuracy: 79.21847246891652\n",
            "Iteration: 59100. Loss: 0.15422764420509338. Accuracy: 85.63816290281655\n",
            "Iteration: 59200. Loss: 0.15461964905261993. Accuracy: 85.68891144379599\n",
            "Iteration: 59300. Loss: 0.48053544759750366. Accuracy: 73.8898756660746\n",
            "Iteration: 59400. Loss: 0.2249632477760315. Accuracy: 84.92768332910428\n",
            "Iteration: 59500. Loss: 0.21462024748325348. Accuracy: 85.96802841918294\n",
            "Iteration: 59600. Loss: 0.18154625594615936. Accuracy: 84.75006343567622\n",
            "Iteration: 59700. Loss: 0.2531633973121643. Accuracy: 84.16645521441258\n",
            "Iteration: 59800. Loss: 0.06242666766047478. Accuracy: 87.43973610758691\n",
            "Iteration: 59900. Loss: 0.22536426782608032. Accuracy: 84.57244354224817\n",
            "Iteration: 60000. Loss: 0.1485472023487091. Accuracy: 86.62775945191575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWZ5i0fzOlPb"
      },
      "source": [
        "#Aproach 5\n",
        "batch size = 64<br>\n",
        "<font color = 'tiffani blue'> num_iters = 90000 </font><br>\n",
        "input_dim = 1 * 28 * 28 <br>\n",
        "num_hidden = 100 <br>\n",
        "output_dim = 10 <br>\n",
        "learning_rate = 0.1 <br>\n",
        "Activation = ReLU <br>\n",
        "Criterion = Cross Entropy <br>\n",
        "Optimizar = SGD <br>\n",
        "<font color = 'green'> Accuracy = 88.17% </font><br>\n",
        "Comment: I have increased the iteration even more and the accuracy has increased a little bit but it has not crossed 90% yet. <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNnF5862aZCj",
        "outputId": "cd8982ab-54eb-4284-c7ca-5572f036c505"
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "batch_size = 64\n",
        "num_iters = 90000\n",
        "input_dim = 1*28*28 # num_features = 784\n",
        "num_hidden = 100\n",
        "output_dim = 10\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle= False, batch_size=batch_size)\n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "\n",
        "        self.linear_4 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_4 = nn.ReLU()\n",
        "        \n",
        "        \n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_5 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_5 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_6 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_6 = nn.ReLU()\n",
        "        \n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_4(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_4(out)\n",
        "        \n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_5(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_5(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_6(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_6(out)\n",
        "        \n",
        "\n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "iter_ = []\n",
        "loss_ = []\n",
        "accuracy_ = []\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 1*28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 100 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 1*28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "            iter_.append(iter)\n",
        "            loss_.append(loss.item())\n",
        "            accuracy_.append(accuracy)\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 100. Loss: 2.304543972015381. Accuracy: 10.37807663029688\n",
            "Iteration: 200. Loss: 2.2992143630981445. Accuracy: 9.058614564831261\n",
            "Iteration: 300. Loss: 2.3066811561584473. Accuracy: 9.895965490992134\n",
            "Iteration: 400. Loss: 2.305482864379883. Accuracy: 9.058614564831261\n",
            "Iteration: 500. Loss: 2.3069839477539062. Accuracy: 9.48997716315656\n",
            "Iteration: 600. Loss: 2.3088536262512207. Accuracy: 9.48997716315656\n",
            "Iteration: 700. Loss: 2.3056230545043945. Accuracy: 9.6422227860949\n",
            "Iteration: 800. Loss: 2.302320718765259. Accuracy: 10.403450900786602\n",
            "Iteration: 900. Loss: 2.2975409030914307. Accuracy: 9.6422227860949\n",
            "Iteration: 1000. Loss: 2.296914577484131. Accuracy: 10.68256787617356\n",
            "Iteration: 1100. Loss: 2.300281286239624. Accuracy: 9.48997716315656\n",
            "Iteration: 1200. Loss: 2.299525260925293. Accuracy: 9.058614564831261\n",
            "Iteration: 1300. Loss: 2.3082797527313232. Accuracy: 9.058614564831261\n",
            "Iteration: 1400. Loss: 2.299199104309082. Accuracy: 10.68256787617356\n",
            "Iteration: 1500. Loss: 2.2992186546325684. Accuracy: 10.8348134991119\n",
            "Iteration: 1600. Loss: 2.3005988597869873. Accuracy: 15.452930728241563\n",
            "Iteration: 1700. Loss: 2.3014822006225586. Accuracy: 10.454199441766049\n",
            "Iteration: 1800. Loss: 2.290055513381958. Accuracy: 16.54402435929967\n",
            "Iteration: 1900. Loss: 2.2667901515960693. Accuracy: 9.058614564831261\n",
            "Iteration: 2000. Loss: 2.2578811645507812. Accuracy: 9.895965490992134\n",
            "Iteration: 2100. Loss: 2.3993711471557617. Accuracy: 9.058614564831261\n",
            "Iteration: 2200. Loss: 2.2375285625457764. Accuracy: 10.784064958132454\n",
            "Iteration: 2300. Loss: 2.247758150100708. Accuracy: 9.363105810707943\n",
            "Iteration: 2400. Loss: 2.263340950012207. Accuracy: 11.037807663029687\n",
            "Iteration: 2500. Loss: 2.3138926029205322. Accuracy: 20.52778482618625\n",
            "Iteration: 2600. Loss: 2.2284047603607178. Accuracy: 10.225831007358538\n",
            "Iteration: 2700. Loss: 2.262042284011841. Accuracy: 15.782796244607967\n",
            "Iteration: 2800. Loss: 2.147141218185425. Accuracy: 17.254503933011925\n",
            "Iteration: 2900. Loss: 2.108607292175293. Accuracy: 20.24866785079929\n",
            "Iteration: 3000. Loss: 2.2157671451568604. Accuracy: 18.39634610504948\n",
            "Iteration: 3100. Loss: 2.2463631629943848. Accuracy: 10.428825171276326\n",
            "Iteration: 3200. Loss: 2.122699022293091. Accuracy: 16.69626998223801\n",
            "Iteration: 3300. Loss: 2.2981297969818115. Accuracy: 18.650088809946713\n",
            "Iteration: 3400. Loss: 2.1339926719665527. Accuracy: 15.072316670895711\n",
            "Iteration: 3500. Loss: 2.2913079261779785. Accuracy: 13.219994925145903\n",
            "Iteration: 3600. Loss: 2.1820201873779297. Accuracy: 11.215427556457751\n",
            "Iteration: 3700. Loss: 2.173586845397949. Accuracy: 19.081451408272013\n",
            "Iteration: 3800. Loss: 2.204596519470215. Accuracy: 16.54402435929967\n",
            "Iteration: 3900. Loss: 2.006635904312134. Accuracy: 15.960416138036031\n",
            "Iteration: 4000. Loss: 2.123990058898926. Accuracy: 18.01573204770363\n",
            "Iteration: 4100. Loss: 2.310580015182495. Accuracy: 12.433392539964476\n",
            "Iteration: 4200. Loss: 2.0987846851348877. Accuracy: 22.40548084242578\n",
            "Iteration: 4300. Loss: 2.274482250213623. Accuracy: 13.930474498858159\n",
            "Iteration: 4400. Loss: 2.271092176437378. Accuracy: 13.169246384166454\n",
            "Iteration: 4500. Loss: 2.1596710681915283. Accuracy: 12.357269728495305\n",
            "Iteration: 4600. Loss: 2.0842201709747314. Accuracy: 16.366404465871607\n",
            "Iteration: 4700. Loss: 2.0141029357910156. Accuracy: 19.030702867292565\n",
            "Iteration: 4800. Loss: 2.1277918815612793. Accuracy: 22.25323521948744\n",
            "Iteration: 4900. Loss: 2.060441732406616. Accuracy: 20.680030449124587\n",
            "Iteration: 5000. Loss: 2.0515596866607666. Accuracy: 27.7340776452677\n",
            "Iteration: 5100. Loss: 2.033468008041382. Accuracy: 21.00989596549099\n",
            "Iteration: 5200. Loss: 2.11395001411438. Accuracy: 26.363867038822633\n",
            "Iteration: 5300. Loss: 2.0022058486938477. Accuracy: 14.61557980208069\n",
            "Iteration: 5400. Loss: 1.9750581979751587. Accuracy: 22.32935803095661\n",
            "Iteration: 5500. Loss: 2.1164658069610596. Accuracy: 28.216188784572445\n",
            "Iteration: 5600. Loss: 1.8830523490905762. Accuracy: 22.278609489977164\n",
            "Iteration: 5700. Loss: 2.0140771865844727. Accuracy: 16.620147170768842\n",
            "Iteration: 5800. Loss: 2.251038074493408. Accuracy: 18.75158589190561\n",
            "Iteration: 5900. Loss: 1.9316620826721191. Accuracy: 21.695001268713526\n",
            "Iteration: 6000. Loss: 1.895843744277954. Accuracy: 31.15960416138036\n",
            "Iteration: 6100. Loss: 1.8363051414489746. Accuracy: 25.196650596295356\n",
            "Iteration: 6200. Loss: 1.984985589981079. Accuracy: 27.835574727226593\n",
            "Iteration: 6300. Loss: 2.249169111251831. Accuracy: 17.58436944937833\n",
            "Iteration: 6400. Loss: 2.001722812652588. Accuracy: 24.054808424257804\n",
            "Iteration: 6500. Loss: 2.2331669330596924. Accuracy: 17.96498350672418\n",
            "Iteration: 6600. Loss: 2.0593690872192383. Accuracy: 32.70743466125349\n",
            "Iteration: 6700. Loss: 2.038958787918091. Accuracy: 23.826439989850293\n",
            "Iteration: 6800. Loss: 1.9015270471572876. Accuracy: 28.140065973103273\n",
            "Iteration: 6900. Loss: 2.1222755908966064. Accuracy: 22.963714793199696\n",
            "Iteration: 7000. Loss: 1.9812874794006348. Accuracy: 32.656686120274045\n",
            "Iteration: 7100. Loss: 1.871950626373291. Accuracy: 25.14590205531591\n",
            "Iteration: 7200. Loss: 1.8587478399276733. Accuracy: 36.310581070794214\n",
            "Iteration: 7300. Loss: 1.9964224100112915. Accuracy: 26.896726719106827\n",
            "Iteration: 7400. Loss: 1.9755748510360718. Accuracy: 36.6150723166709\n",
            "Iteration: 7500. Loss: 1.8717468976974487. Accuracy: 32.37756914488708\n",
            "Iteration: 7600. Loss: 1.76856529712677. Accuracy: 33.722405480842426\n",
            "Iteration: 7700. Loss: 1.9137167930603027. Accuracy: 35.57472722659224\n",
            "Iteration: 7800. Loss: 2.102621078491211. Accuracy: 24.892159350418677\n",
            "Iteration: 7900. Loss: 2.591683864593506. Accuracy: 11.037807663029687\n",
            "Iteration: 8000. Loss: 1.9445642232894897. Accuracy: 28.368434407510783\n",
            "Iteration: 8100. Loss: 1.9619650840759277. Accuracy: 32.52981476782543\n",
            "Iteration: 8200. Loss: 1.7573927640914917. Accuracy: 36.64044658716062\n",
            "Iteration: 8300. Loss: 1.7752207517623901. Accuracy: 34.58513067749302\n",
            "Iteration: 8400. Loss: 1.83877694606781. Accuracy: 34.61050494798275\n",
            "Iteration: 8500. Loss: 2.0792300701141357. Accuracy: 17.406749555950267\n",
            "Iteration: 8600. Loss: 2.008662700653076. Accuracy: 24.283176858665314\n",
            "Iteration: 8700. Loss: 2.716430902481079. Accuracy: 9.921339761481857\n",
            "Iteration: 8800. Loss: 1.7709248065948486. Accuracy: 36.81806648058868\n",
            "Iteration: 8900. Loss: 2.14084529876709. Accuracy: 16.873889875666073\n",
            "Iteration: 9000. Loss: 2.0637693405151367. Accuracy: 36.97031210352702\n",
            "Iteration: 9100. Loss: 2.0740418434143066. Accuracy: 31.184978431870082\n",
            "Iteration: 9200. Loss: 2.2125930786132812. Accuracy: 17.00076122811469\n",
            "Iteration: 9300. Loss: 1.9544488191604614. Accuracy: 38.03603146409541\n",
            "Iteration: 9400. Loss: 1.7993168830871582. Accuracy: 38.5688911443796\n",
            "Iteration: 9500. Loss: 1.9471328258514404. Accuracy: 25.52651611266176\n",
            "Iteration: 9600. Loss: 1.8155046701431274. Accuracy: 37.07180918548592\n",
            "Iteration: 9700. Loss: 1.6776593923568726. Accuracy: 42.324283176858664\n",
            "Iteration: 9800. Loss: 2.0883443355560303. Accuracy: 27.657954833798527\n",
            "Iteration: 9900. Loss: 1.8672257661819458. Accuracy: 39.3301192590713\n",
            "Iteration: 10000. Loss: 2.076695203781128. Accuracy: 17.076884039583863\n",
            "Iteration: 10100. Loss: 1.774438738822937. Accuracy: 35.44785587414362\n",
            "Iteration: 10200. Loss: 1.5942052602767944. Accuracy: 44.45572189799543\n",
            "Iteration: 10300. Loss: 1.7872612476348877. Accuracy: 35.44785587414362\n",
            "Iteration: 10400. Loss: 1.8043962717056274. Accuracy: 36.08221263638671\n",
            "Iteration: 10500. Loss: 2.0025386810302734. Accuracy: 24.20705404719614\n",
            "Iteration: 10600. Loss: 1.6619000434875488. Accuracy: 31.920832276072062\n",
            "Iteration: 10700. Loss: 1.7773866653442383. Accuracy: 34.58513067749302\n",
            "Iteration: 10800. Loss: 1.698598861694336. Accuracy: 29.992387718853085\n",
            "Iteration: 10900. Loss: 1.8318887948989868. Accuracy: 44.37959908652626\n",
            "Iteration: 11000. Loss: 1.5287525653839111. Accuracy: 38.315148439482364\n",
            "Iteration: 11100. Loss: 2.4947657585144043. Accuracy: 28.622177112408018\n",
            "Iteration: 11200. Loss: 1.667338252067566. Accuracy: 30.017761989342805\n",
            "Iteration: 11300. Loss: 2.065286159515381. Accuracy: 28.698299923877187\n",
            "Iteration: 11400. Loss: 1.6721558570861816. Accuracy: 40.47196143110886\n",
            "Iteration: 11500. Loss: 1.9463937282562256. Accuracy: 37.90916011164679\n",
            "Iteration: 11600. Loss: 1.6439342498779297. Accuracy: 37.98528292311596\n",
            "Iteration: 11700. Loss: 1.5313096046447754. Accuracy: 39.91372748033494\n",
            "Iteration: 11800. Loss: 1.6409437656402588. Accuracy: 46.43491499619386\n",
            "Iteration: 11900. Loss: 1.3673545122146606. Accuracy: 26.92210098959655\n",
            "Iteration: 12000. Loss: 1.7888708114624023. Accuracy: 32.80893174321238\n",
            "Iteration: 12100. Loss: 1.69869863986969. Accuracy: 48.84547069271758\n",
            "Iteration: 12200. Loss: 2.3321197032928467. Accuracy: 25.628013194620653\n",
            "Iteration: 12300. Loss: 1.5153473615646362. Accuracy: 42.95863993910175\n",
            "Iteration: 12400. Loss: 1.9744799137115479. Accuracy: 30.804364374524233\n",
            "Iteration: 12500. Loss: 1.290290355682373. Accuracy: 48.6424765287998\n",
            "Iteration: 12600. Loss: 1.7374476194381714. Accuracy: 29.15503679269221\n",
            "Iteration: 12700. Loss: 1.5682346820831299. Accuracy: 27.708703374777976\n",
            "Iteration: 12800. Loss: 1.8573453426361084. Accuracy: 34.661253488962195\n",
            "Iteration: 12900. Loss: 1.3936655521392822. Accuracy: 46.61253488962193\n",
            "Iteration: 13000. Loss: 1.5392870903015137. Accuracy: 42.47652879979701\n",
            "Iteration: 13100. Loss: 1.6267492771148682. Accuracy: 30.068510530322254\n",
            "Iteration: 13200. Loss: 1.80131196975708. Accuracy: 45.69906115199188\n",
            "Iteration: 13300. Loss: 1.6565734148025513. Accuracy: 40.268967267191066\n",
            "Iteration: 13400. Loss: 1.8501266241073608. Accuracy: 50.49479827454961\n",
            "Iteration: 13500. Loss: 1.274149775505066. Accuracy: 48.10961684851561\n",
            "Iteration: 13600. Loss: 1.5080653429031372. Accuracy: 49.835067241816795\n",
            "Iteration: 13700. Loss: 1.5587836503982544. Accuracy: 50.241055569652374\n",
            "Iteration: 13800. Loss: 1.6772547960281372. Accuracy: 39.50773915249937\n",
            "Iteration: 13900. Loss: 1.8063645362854004. Accuracy: 49.657447348388736\n",
            "Iteration: 14000. Loss: 1.4131853580474854. Accuracy: 50.41867546308043\n",
            "Iteration: 14100. Loss: 1.2812504768371582. Accuracy: 46.02892666835829\n",
            "Iteration: 14200. Loss: 1.72120201587677. Accuracy: 45.49606698807409\n",
            "Iteration: 14300. Loss: 1.846824288368225. Accuracy: 41.182441004821115\n",
            "Iteration: 14400. Loss: 1.4347865581512451. Accuracy: 39.279370718091855\n",
            "Iteration: 14500. Loss: 1.4000781774520874. Accuracy: 47.67825425019031\n",
            "Iteration: 14600. Loss: 2.329958438873291. Accuracy: 27.860948997716317\n",
            "Iteration: 14700. Loss: 1.177284598350525. Accuracy: 54.47855874143618\n",
            "Iteration: 14800. Loss: 1.5132026672363281. Accuracy: 48.92159350418675\n",
            "Iteration: 14900. Loss: 1.2753212451934814. Accuracy: 51.17990357777214\n",
            "Iteration: 15000. Loss: 1.1707006692886353. Accuracy: 53.92032479066227\n",
            "Iteration: 15100. Loss: 1.4114923477172852. Accuracy: 52.85460543009388\n",
            "Iteration: 15200. Loss: 1.2580671310424805. Accuracy: 55.315909667597055\n",
            "Iteration: 15300. Loss: 1.416042685508728. Accuracy: 55.138289774168996\n",
            "Iteration: 15400. Loss: 1.3411403894424438. Accuracy: 51.05303222532352\n",
            "Iteration: 15500. Loss: 1.5675028562545776. Accuracy: 43.97361075869069\n",
            "Iteration: 15600. Loss: 1.5715150833129883. Accuracy: 52.90535397107333\n",
            "Iteration: 15700. Loss: 1.7211607694625854. Accuracy: 50.723166708957116\n",
            "Iteration: 15800. Loss: 2.553220272064209. Accuracy: 29.028165440243594\n",
            "Iteration: 15900. Loss: 1.210700273513794. Accuracy: 55.39203247906622\n",
            "Iteration: 16000. Loss: 1.3939281702041626. Accuracy: 54.58005582339508\n",
            "Iteration: 16100. Loss: 1.5904403924942017. Accuracy: 52.14412585638163\n",
            "Iteration: 16200. Loss: 1.3727126121520996. Accuracy: 45.77518396346105\n",
            "Iteration: 16300. Loss: 1.5865025520324707. Accuracy: 54.14869322506978\n",
            "Iteration: 16400. Loss: 1.356217622756958. Accuracy: 43.03476275057092\n",
            "Iteration: 16500. Loss: 1.1255278587341309. Accuracy: 58.56381629028166\n",
            "Iteration: 16600. Loss: 1.3656017780303955. Accuracy: 34.00152245622938\n",
            "Iteration: 16700. Loss: 1.212639570236206. Accuracy: 48.9723420451662\n",
            "Iteration: 16800. Loss: 1.1572587490081787. Accuracy: 52.753108348134994\n",
            "Iteration: 16900. Loss: 1.5066378116607666. Accuracy: 53.209845216950015\n",
            "Iteration: 17000. Loss: 1.3592472076416016. Accuracy: 54.88454706927176\n",
            "Iteration: 17100. Loss: 2.1041111946105957. Accuracy: 25.75488454706927\n",
            "Iteration: 17200. Loss: 1.4220072031021118. Accuracy: 55.56965237249429\n",
            "Iteration: 17300. Loss: 1.7621670961380005. Accuracy: 45.34382136513575\n",
            "Iteration: 17400. Loss: 1.2199472188949585. Accuracy: 59.52803856889114\n",
            "Iteration: 17500. Loss: 1.0851606130599976. Accuracy: 58.208576503425526\n",
            "Iteration: 17600. Loss: 1.2319564819335938. Accuracy: 56.9906115199188\n",
            "Iteration: 17700. Loss: 1.1942116022109985. Accuracy: 41.61380360314641\n",
            "Iteration: 17800. Loss: 1.1718744039535522. Accuracy: 54.83379852829231\n",
            "Iteration: 17900. Loss: 1.3368732929229736. Accuracy: 39.38086780005075\n",
            "Iteration: 18000. Loss: 1.189721941947937. Accuracy: 63.689418929205786\n",
            "Iteration: 18100. Loss: 1.587425947189331. Accuracy: 58.46231920832276\n",
            "Iteration: 18200. Loss: 1.4682722091674805. Accuracy: 50.49479827454961\n",
            "Iteration: 18300. Loss: 1.2723324298858643. Accuracy: 54.732301446333416\n",
            "Iteration: 18400. Loss: 1.4378833770751953. Accuracy: 47.17076884039584\n",
            "Iteration: 18500. Loss: 1.010715365409851. Accuracy: 56.762243085511294\n",
            "Iteration: 18600. Loss: 0.9500229954719543. Accuracy: 57.49809692971327\n",
            "Iteration: 18700. Loss: 1.2883479595184326. Accuracy: 55.18903831514844\n",
            "Iteration: 18800. Loss: 0.9703234434127808. Accuracy: 64.2476528799797\n",
            "Iteration: 18900. Loss: 1.1267801523208618. Accuracy: 57.320477036285205\n",
            "Iteration: 19000. Loss: 1.1866399049758911. Accuracy: 60.720629281908145\n",
            "Iteration: 19100. Loss: 1.1904935836791992. Accuracy: 49.0738391271251\n",
            "Iteration: 19200. Loss: 1.0719276666641235. Accuracy: 63.20730778990104\n",
            "Iteration: 19300. Loss: 1.0677692890167236. Accuracy: 61.05049479827455\n",
            "Iteration: 19400. Loss: 1.3548327684402466. Accuracy: 34.35676224308551\n",
            "Iteration: 19500. Loss: 1.387861728668213. Accuracy: 58.89368180664806\n",
            "Iteration: 19600. Loss: 1.2456846237182617. Accuracy: 54.09794468409033\n",
            "Iteration: 19700. Loss: 1.4083338975906372. Accuracy: 43.821365135752345\n",
            "Iteration: 19800. Loss: 0.971250593662262. Accuracy: 53.4128393808678\n",
            "Iteration: 19900. Loss: 1.2630202770233154. Accuracy: 57.01598579040853\n",
            "Iteration: 20000. Loss: 1.353384017944336. Accuracy: 62.90281654402436\n",
            "Iteration: 20100. Loss: 1.137505054473877. Accuracy: 56.93986297893935\n",
            "Iteration: 20200. Loss: 0.8578656911849976. Accuracy: 60.6698807409287\n",
            "Iteration: 20300. Loss: 1.0640801191329956. Accuracy: 61.837097183455974\n",
            "Iteration: 20400. Loss: 1.0512571334838867. Accuracy: 49.93656432377569\n",
            "Iteration: 20500. Loss: 1.4548343420028687. Accuracy: 47.246891651865006\n",
            "Iteration: 20600. Loss: 0.9017391800880432. Accuracy: 66.32834306013702\n",
            "Iteration: 20700. Loss: 1.0410805940628052. Accuracy: 60.49226084750063\n",
            "Iteration: 20800. Loss: 1.1755008697509766. Accuracy: 62.52220248667851\n",
            "Iteration: 20900. Loss: 1.2258164882659912. Accuracy: 65.23724942907891\n",
            "Iteration: 21000. Loss: 1.2698752880096436. Accuracy: 64.27302715046942\n",
            "Iteration: 21100. Loss: 1.130255937576294. Accuracy: 57.11748287236742\n",
            "Iteration: 21200. Loss: 1.1399980783462524. Accuracy: 60.92362344582593\n",
            "Iteration: 21300. Loss: 1.5820233821868896. Accuracy: 31.00735853844202\n",
            "Iteration: 21400. Loss: 1.0578449964523315. Accuracy: 69.1195128140066\n",
            "Iteration: 21500. Loss: 1.4018551111221313. Accuracy: 59.60416138036032\n",
            "Iteration: 21600. Loss: 0.9456499218940735. Accuracy: 64.62826693732555\n",
            "Iteration: 21700. Loss: 1.0579051971435547. Accuracy: 68.48515605176351\n",
            "Iteration: 21800. Loss: 0.9023255109786987. Accuracy: 69.32250697792439\n",
            "Iteration: 21900. Loss: 1.0738030672073364. Accuracy: 66.40446587160619\n",
            "Iteration: 22000. Loss: 0.8689481019973755. Accuracy: 63.94316163410302\n",
            "Iteration: 22100. Loss: 1.7654576301574707. Accuracy: 46.1557980208069\n",
            "Iteration: 22200. Loss: 0.8052438497543335. Accuracy: 69.93148946967774\n",
            "Iteration: 22300. Loss: 1.186611294746399. Accuracy: 39.93910175082466\n",
            "Iteration: 22400. Loss: 0.8047997951507568. Accuracy: 70.10910936310582\n",
            "Iteration: 22500. Loss: 0.9015786647796631. Accuracy: 62.95356508500381\n",
            "Iteration: 22600. Loss: 0.68745356798172. Accuracy: 70.36285206800305\n",
            "Iteration: 22700. Loss: 0.9533861875534058. Accuracy: 51.7888860695255\n",
            "Iteration: 22800. Loss: 0.631502628326416. Accuracy: 69.80461811722913\n",
            "Iteration: 22900. Loss: 1.0367769002914429. Accuracy: 65.03425526516112\n",
            "Iteration: 23000. Loss: 0.9614253044128418. Accuracy: 49.55595026642984\n",
            "Iteration: 23100. Loss: 1.035781979560852. Accuracy: 66.98807409286982\n",
            "Iteration: 23200. Loss: 1.0943318605422974. Accuracy: 67.7239279370718\n",
            "Iteration: 23300. Loss: 0.9018459916114807. Accuracy: 71.47931996955087\n",
            "Iteration: 23400. Loss: 0.9927014708518982. Accuracy: 68.45978178127379\n",
            "Iteration: 23500. Loss: 1.069105863571167. Accuracy: 66.48058868307537\n",
            "Iteration: 23600. Loss: 0.681612491607666. Accuracy: 65.2118751585892\n",
            "Iteration: 23700. Loss: 0.7034549117088318. Accuracy: 60.06089824917534\n",
            "Iteration: 23800. Loss: 0.9374020099639893. Accuracy: 69.42400405988327\n",
            "Iteration: 23900. Loss: 1.4476633071899414. Accuracy: 57.87871098705912\n",
            "Iteration: 24000. Loss: 0.9025573134422302. Accuracy: 62.217711240801826\n",
            "Iteration: 24100. Loss: 0.5543010830879211. Accuracy: 76.19893428063943\n",
            "Iteration: 24200. Loss: 0.8203735947608948. Accuracy: 76.42730271504695\n",
            "Iteration: 24300. Loss: 1.2754278182983398. Accuracy: 74.09286982999238\n",
            "Iteration: 24400. Loss: 0.6712455153465271. Accuracy: 70.43897487947221\n",
            "Iteration: 24500. Loss: 0.7197765707969666. Accuracy: 72.9256533874651\n",
            "Iteration: 24600. Loss: 1.0544135570526123. Accuracy: 65.44024359299671\n",
            "Iteration: 24700. Loss: 0.9718797206878662. Accuracy: 68.51053032225323\n",
            "Iteration: 24800. Loss: 1.2011513710021973. Accuracy: 58.716061913219995\n",
            "Iteration: 24900. Loss: 0.5271556377410889. Accuracy: 76.14818573965998\n",
            "Iteration: 25000. Loss: 1.141953468322754. Accuracy: 31.895458005582338\n",
            "Iteration: 25100. Loss: 0.6293298006057739. Accuracy: 72.6972849530576\n",
            "Iteration: 25200. Loss: 0.7923926711082458. Accuracy: 74.9809692971327\n",
            "Iteration: 25300. Loss: 0.5483983755111694. Accuracy: 78.71098705912205\n",
            "Iteration: 25400. Loss: 0.8076867461204529. Accuracy: 71.70768840395839\n",
            "Iteration: 25500. Loss: 1.1025800704956055. Accuracy: 35.49860441512307\n",
            "Iteration: 25600. Loss: 1.3421870470046997. Accuracy: 63.71479319969551\n",
            "Iteration: 25700. Loss: 0.964140772819519. Accuracy: 73.58538442019793\n",
            "Iteration: 25800. Loss: 0.8549830317497253. Accuracy: 58.23395077391525\n",
            "Iteration: 25900. Loss: 1.5509968996047974. Accuracy: 60.111646790154786\n",
            "Iteration: 26000. Loss: 0.6877571940422058. Accuracy: 76.80791677239279\n",
            "Iteration: 26100. Loss: 0.7687553763389587. Accuracy: 67.39406242070541\n",
            "Iteration: 26200. Loss: 0.8307133913040161. Accuracy: 53.4128393808678\n",
            "Iteration: 26300. Loss: 1.796397089958191. Accuracy: 55.239786856127886\n",
            "Iteration: 26400. Loss: 0.7730792760848999. Accuracy: 71.20020299416392\n",
            "Iteration: 26500. Loss: 0.562440037727356. Accuracy: 61.71022583100736\n",
            "Iteration: 26600. Loss: 0.5420089364051819. Accuracy: 71.02258310073586\n",
            "Iteration: 26700. Loss: 1.1455274820327759. Accuracy: 70.43897487947221\n",
            "Iteration: 26800. Loss: 0.6206037998199463. Accuracy: 75.58995178888607\n",
            "Iteration: 26900. Loss: 1.4130909442901611. Accuracy: 57.98020806901802\n",
            "Iteration: 27000. Loss: 0.4226197302341461. Accuracy: 77.2392793707181\n",
            "Iteration: 27100. Loss: 0.9845492839813232. Accuracy: 68.05379345343822\n",
            "Iteration: 27200. Loss: 0.626394510269165. Accuracy: 76.35117990357777\n",
            "Iteration: 27300. Loss: 0.5764483213424683. Accuracy: 70.94646028926668\n",
            "Iteration: 27400. Loss: 0.7516292333602905. Accuracy: 76.65567114945445\n",
            "Iteration: 27500. Loss: 0.8835315704345703. Accuracy: 76.2750570921086\n",
            "Iteration: 27600. Loss: 0.5274563431739807. Accuracy: 79.26922100989597\n",
            "Iteration: 27700. Loss: 0.69078528881073. Accuracy: 71.96143110885562\n",
            "Iteration: 27800. Loss: 0.6018617749214172. Accuracy: 75.38695762496828\n",
            "Iteration: 27900. Loss: 1.340569257736206. Accuracy: 59.98477543770617\n",
            "Iteration: 28000. Loss: 0.5038744807243347. Accuracy: 77.67064196904339\n",
            "Iteration: 28100. Loss: 0.5683454871177673. Accuracy: 68.07916772392794\n",
            "Iteration: 28200. Loss: 0.6365185976028442. Accuracy: 75.1839634610505\n",
            "Iteration: 28300. Loss: 0.5374417304992676. Accuracy: 72.2659223547323\n",
            "Iteration: 28400. Loss: 0.5817192792892456. Accuracy: 77.94975894443034\n",
            "Iteration: 28500. Loss: 0.9550605416297913. Accuracy: 78.58411570667343\n",
            "Iteration: 28600. Loss: 0.833678126335144. Accuracy: 72.24054808424258\n",
            "Iteration: 28700. Loss: 0.7441698908805847. Accuracy: 64.62826693732555\n",
            "Iteration: 28800. Loss: 0.6848746538162231. Accuracy: 78.58411570667343\n",
            "Iteration: 28900. Loss: 0.8949450254440308. Accuracy: 77.46764780512561\n",
            "Iteration: 29000. Loss: 0.8305675387382507. Accuracy: 73.58538442019793\n",
            "Iteration: 29100. Loss: 0.2733669579029083. Accuracy: 78.15275310834814\n",
            "Iteration: 29200. Loss: 0.6803160309791565. Accuracy: 74.70185232174575\n",
            "Iteration: 29300. Loss: 0.714687168598175. Accuracy: 75.97056584623192\n",
            "Iteration: 29400. Loss: 0.8002164959907532. Accuracy: 72.2659223547323\n",
            "Iteration: 29500. Loss: 1.0123564004898071. Accuracy: 63.86703882263385\n",
            "Iteration: 29600. Loss: 0.8305575251579285. Accuracy: 74.90484648566354\n",
            "Iteration: 29700. Loss: 0.5794872045516968. Accuracy: 47.50063435676224\n",
            "Iteration: 29800. Loss: 0.5139191746711731. Accuracy: 81.19766556711494\n",
            "Iteration: 29900. Loss: 0.41457366943359375. Accuracy: 77.18853082973864\n",
            "Iteration: 30000. Loss: 0.7373765110969543. Accuracy: 78.73636132961177\n",
            "Iteration: 30100. Loss: 0.32724207639694214. Accuracy: 79.8528292311596\n",
            "Iteration: 30200. Loss: 0.8072033524513245. Accuracy: 74.80334940370464\n",
            "Iteration: 30300. Loss: 0.658592164516449. Accuracy: 73.73763004313626\n",
            "Iteration: 30400. Loss: 1.0881041288375854. Accuracy: 67.1656939862979\n",
            "Iteration: 30500. Loss: 0.6158913969993591. Accuracy: 62.64907383912713\n",
            "Iteration: 30600. Loss: 0.49795398116111755. Accuracy: 78.50799289520427\n",
            "Iteration: 30700. Loss: 0.4496369957923889. Accuracy: 79.82745496066988\n",
            "Iteration: 30800. Loss: 0.3636730909347534. Accuracy: 78.68561278863233\n",
            "Iteration: 30900. Loss: 0.7392873167991638. Accuracy: 74.04212128901294\n",
            "Iteration: 31000. Loss: 0.6679245233535767. Accuracy: 79.31996955087541\n",
            "Iteration: 31100. Loss: 0.4659041464328766. Accuracy: 78.27962446079675\n",
            "Iteration: 31200. Loss: 0.4038506746292114. Accuracy: 77.2392793707181\n",
            "Iteration: 31300. Loss: 0.9888166189193726. Accuracy: 74.44810961684851\n",
            "Iteration: 31400. Loss: 0.43753838539123535. Accuracy: 76.30043136259833\n",
            "Iteration: 31500. Loss: 0.4767020344734192. Accuracy: 81.22303983760467\n",
            "Iteration: 31600. Loss: 1.442491054534912. Accuracy: 62.04009134737376\n",
            "Iteration: 31700. Loss: 0.4436302185058594. Accuracy: 79.7513321492007\n",
            "Iteration: 31800. Loss: 0.9460198879241943. Accuracy: 72.24054808424258\n",
            "Iteration: 31900. Loss: 0.45128875970840454. Accuracy: 75.97056584623192\n",
            "Iteration: 32000. Loss: 0.3797421157360077. Accuracy: 81.60365389495053\n",
            "Iteration: 32100. Loss: 1.1145193576812744. Accuracy: 70.48972342045167\n",
            "Iteration: 32200. Loss: 0.48275530338287354. Accuracy: 83.27835574727227\n",
            "Iteration: 32300. Loss: 0.8517482280731201. Accuracy: 68.96726719106826\n",
            "Iteration: 32400. Loss: 0.38808438181877136. Accuracy: 80.99467140319716\n",
            "Iteration: 32500. Loss: 0.7941959500312805. Accuracy: 75.23471200202994\n",
            "Iteration: 32600. Loss: 0.5956833362579346. Accuracy: 83.12611012433392\n",
            "Iteration: 32700. Loss: 0.4626602530479431. Accuracy: 78.88860695255012\n",
            "Iteration: 32800. Loss: 0.5386521220207214. Accuracy: 78.15275310834814\n",
            "Iteration: 32900. Loss: 0.4105576276779175. Accuracy: 78.27962446079675\n",
            "Iteration: 33000. Loss: 2.0893073081970215. Accuracy: 56.280131946206545\n",
            "Iteration: 33100. Loss: 0.47930917143821716. Accuracy: 84.01420959147424\n",
            "Iteration: 33200. Loss: 0.5763588547706604. Accuracy: 79.0916011164679\n",
            "Iteration: 33300. Loss: 0.6183579564094543. Accuracy: 81.29916264907384\n",
            "Iteration: 33400. Loss: 0.5101872086524963. Accuracy: 77.49302207561533\n",
            "Iteration: 33500. Loss: 0.6316095590591431. Accuracy: 82.46637909160111\n",
            "Iteration: 33600. Loss: 0.9480947256088257. Accuracy: 76.50342552651611\n",
            "Iteration: 33700. Loss: 0.3511594235897064. Accuracy: 83.02461304237504\n",
            "Iteration: 33800. Loss: 0.3033381402492523. Accuracy: 83.30373001776199\n",
            "Iteration: 33900. Loss: 0.41613903641700745. Accuracy: 83.37985282923115\n",
            "Iteration: 34000. Loss: 0.6563717126846313. Accuracy: 82.44100482111139\n",
            "Iteration: 34100. Loss: 0.5640308260917664. Accuracy: 78.83785841157066\n",
            "Iteration: 34200. Loss: 0.3780978322029114. Accuracy: 81.32453691956356\n",
            "Iteration: 34300. Loss: 0.6198374032974243. Accuracy: 73.63613296117737\n",
            "Iteration: 34400. Loss: 0.9993656873703003. Accuracy: 69.8807409286983\n",
            "Iteration: 34500. Loss: 0.767314612865448. Accuracy: 74.57498096929713\n",
            "Iteration: 34600. Loss: 0.3255089521408081. Accuracy: 84.24257802588176\n",
            "Iteration: 34700. Loss: 1.2571172714233398. Accuracy: 71.09870591220502\n",
            "Iteration: 34800. Loss: 0.35277289152145386. Accuracy: 82.46637909160111\n",
            "Iteration: 34900. Loss: 0.43485888838768005. Accuracy: 80.94392286221772\n",
            "Iteration: 35000. Loss: 0.5445743799209595. Accuracy: 78.48261862471453\n",
            "Iteration: 35100. Loss: 0.8073733448982239. Accuracy: 76.60492260847501\n",
            "Iteration: 35200. Loss: 0.32494956254959106. Accuracy: 83.73509261608729\n",
            "Iteration: 35300. Loss: 0.6897258758544922. Accuracy: 68.9165186500888\n",
            "Iteration: 35400. Loss: 2.129486560821533. Accuracy: 19.614311088556203\n",
            "Iteration: 35500. Loss: 0.2686409652233124. Accuracy: 82.111139304745\n",
            "Iteration: 35600. Loss: 0.3684335947036743. Accuracy: 82.54250190307029\n",
            "Iteration: 35700. Loss: 0.31501060724258423. Accuracy: 82.51712763258057\n",
            "Iteration: 35800. Loss: 1.0101596117019653. Accuracy: 51.839634610504945\n",
            "Iteration: 35900. Loss: 0.40218523144721985. Accuracy: 74.72722659223547\n",
            "Iteration: 36000. Loss: 0.5628390312194824. Accuracy: 79.49758944430347\n",
            "Iteration: 36100. Loss: 0.4303967356681824. Accuracy: 81.73052524739914\n",
            "Iteration: 36200. Loss: 0.4138702154159546. Accuracy: 78.48261862471453\n",
            "Iteration: 36300. Loss: 0.3248685896396637. Accuracy: 83.45597564070033\n",
            "Iteration: 36400. Loss: 0.1588493436574936. Accuracy: 83.93808678000508\n",
            "Iteration: 36500. Loss: 0.41431930661201477. Accuracy: 83.27835574727227\n",
            "Iteration: 36600. Loss: 0.3562402129173279. Accuracy: 80.53793453438213\n",
            "Iteration: 36700. Loss: 0.556658923625946. Accuracy: 83.22760720629282\n",
            "Iteration: 36800. Loss: 0.5930405855178833. Accuracy: 79.80208069018016\n",
            "Iteration: 36900. Loss: 0.4378330111503601. Accuracy: 80.74092869829992\n",
            "Iteration: 37000. Loss: 0.4011945426464081. Accuracy: 83.1007358538442\n",
            "Iteration: 37100. Loss: 0.6834339499473572. Accuracy: 79.47221517381375\n",
            "Iteration: 37200. Loss: 0.6069238781929016. Accuracy: 82.18726211621416\n",
            "Iteration: 37300. Loss: 0.6302534341812134. Accuracy: 79.39609236234459\n",
            "Iteration: 37400. Loss: 0.4767378568649292. Accuracy: 80.74092869829992\n",
            "Iteration: 37500. Loss: 0.6835794448852539. Accuracy: 77.21390510022837\n",
            "Iteration: 37600. Loss: 0.3361580967903137. Accuracy: 77.0362852068003\n",
            "Iteration: 37700. Loss: 0.5393508076667786. Accuracy: 81.50215681299163\n",
            "Iteration: 37800. Loss: 0.4426751732826233. Accuracy: 82.26338492768333\n",
            "Iteration: 37900. Loss: 0.5742809772491455. Accuracy: 81.98426795229638\n",
            "Iteration: 38000. Loss: 0.35951846837997437. Accuracy: 78.02588175589952\n",
            "Iteration: 38100. Loss: 0.8325393199920654. Accuracy: 75.38695762496828\n",
            "Iteration: 38200. Loss: 0.45464015007019043. Accuracy: 81.09616848515606\n",
            "Iteration: 38300. Loss: 0.2982262670993805. Accuracy: 84.64856635371733\n",
            "Iteration: 38400. Loss: 0.23256812989711761. Accuracy: 84.97843187008374\n",
            "Iteration: 38500. Loss: 0.24447374045848846. Accuracy: 83.60822126363867\n",
            "Iteration: 38600. Loss: 0.15062855184078217. Accuracy: 83.760466886577\n",
            "Iteration: 38700. Loss: 0.3084568977355957. Accuracy: 84.3948236488201\n",
            "Iteration: 38800. Loss: 0.2105165272951126. Accuracy: 84.42019791930981\n",
            "Iteration: 38900. Loss: 0.5474115610122681. Accuracy: 82.16188784572444\n",
            "Iteration: 39000. Loss: 0.43305912613868713. Accuracy: 70.89571174828724\n",
            "Iteration: 39100. Loss: 0.2584424316883087. Accuracy: 84.64856635371733\n",
            "Iteration: 39200. Loss: 0.1918897032737732. Accuracy: 85.89190560771378\n",
            "Iteration: 39300. Loss: 0.5445886254310608. Accuracy: 74.52423242831769\n",
            "Iteration: 39400. Loss: 0.30134156346321106. Accuracy: 85.02918041106318\n",
            "Iteration: 39500. Loss: 0.5936766266822815. Accuracy: 71.37782288759198\n",
            "Iteration: 39600. Loss: 0.2514774203300476. Accuracy: 86.3486424765288\n",
            "Iteration: 39700. Loss: 0.30380284786224365. Accuracy: 84.26795229637148\n",
            "Iteration: 39800. Loss: 0.34320390224456787. Accuracy: 82.23801065719361\n",
            "Iteration: 39900. Loss: 0.19946999847888947. Accuracy: 83.83658969804618\n",
            "Iteration: 40000. Loss: 0.20917782187461853. Accuracy: 82.26338492768333\n",
            "Iteration: 40100. Loss: 0.18202713131904602. Accuracy: 85.66353717330627\n",
            "Iteration: 40200. Loss: 0.17588287591934204. Accuracy: 86.32326820603907\n",
            "Iteration: 40300. Loss: 0.26938509941101074. Accuracy: 83.37985282923115\n",
            "Iteration: 40400. Loss: 0.20349325239658356. Accuracy: 84.8261862471454\n",
            "Iteration: 40500. Loss: 0.15125446021556854. Accuracy: 86.14564831261102\n",
            "Iteration: 40600. Loss: 0.5707659721374512. Accuracy: 83.17685866531338\n",
            "Iteration: 40700. Loss: 0.26653599739074707. Accuracy: 84.87693478812484\n",
            "Iteration: 40800. Loss: 0.46003028750419617. Accuracy: 72.95102765795484\n",
            "Iteration: 40900. Loss: 0.3212156593799591. Accuracy: 82.94849023090586\n",
            "Iteration: 41000. Loss: 0.6645253896713257. Accuracy: 81.27378837858411\n",
            "Iteration: 41100. Loss: 0.17770537734031677. Accuracy: 84.21720375539203\n",
            "Iteration: 41200. Loss: 0.3473086953163147. Accuracy: 86.22177112408018\n",
            "Iteration: 41300. Loss: 0.1551869958639145. Accuracy: 85.40979446840903\n",
            "Iteration: 41400. Loss: 0.23501865565776825. Accuracy: 86.47551382897741\n",
            "Iteration: 41500. Loss: 0.17505145072937012. Accuracy: 84.19182948490231\n",
            "Iteration: 41600. Loss: 0.18064019083976746. Accuracy: 85.99340268967268\n",
            "Iteration: 41700. Loss: 0.7911741137504578. Accuracy: 59.70565846231921\n",
            "Iteration: 41800. Loss: 0.1689850389957428. Accuracy: 83.40522709972089\n",
            "Iteration: 41900. Loss: 0.3096577823162079. Accuracy: 81.95889368180664\n",
            "Iteration: 42000. Loss: 0.12300962209701538. Accuracy: 85.00380614057346\n",
            "Iteration: 42100. Loss: 0.07938851416110992. Accuracy: 87.03374777975134\n",
            "Iteration: 42200. Loss: 0.343949556350708. Accuracy: 72.13905100228368\n",
            "Iteration: 42300. Loss: 0.47879308462142944. Accuracy: 84.34407510784065\n",
            "Iteration: 42400. Loss: 0.17260165512561798. Accuracy: 86.7800050748541\n",
            "Iteration: 42500. Loss: 0.1427152305841446. Accuracy: 83.50672418167977\n",
            "Iteration: 42600. Loss: 0.15370075404644012. Accuracy: 86.19639685359046\n",
            "Iteration: 42700. Loss: 0.365261048078537. Accuracy: 80.86780005074854\n",
            "Iteration: 42800. Loss: 0.1899174153804779. Accuracy: 87.31286475513829\n",
            "Iteration: 42900. Loss: 0.4256907105445862. Accuracy: 82.18726211621416\n",
            "Iteration: 43000. Loss: 0.11155164986848831. Accuracy: 86.70388226338493\n",
            "Iteration: 43100. Loss: 0.1255187839269638. Accuracy: 86.57701091093631\n",
            "Iteration: 43200. Loss: 0.4098030924797058. Accuracy: 70.05836082212636\n",
            "Iteration: 43300. Loss: 0.2534278631210327. Accuracy: 85.28292311596041\n",
            "Iteration: 43400. Loss: 0.36064812541007996. Accuracy: 85.40979446840903\n",
            "Iteration: 43500. Loss: 0.15794208645820618. Accuracy: 86.57701091093631\n",
            "Iteration: 43600. Loss: 0.1795572191476822. Accuracy: 85.89190560771378\n",
            "Iteration: 43700. Loss: 0.144308403134346. Accuracy: 86.85612788632326\n",
            "Iteration: 43800. Loss: 0.6714460253715515. Accuracy: 81.9081451408272\n",
            "Iteration: 43900. Loss: 0.25627511739730835. Accuracy: 81.78127378837858\n",
            "Iteration: 44000. Loss: 0.1636957973241806. Accuracy: 87.41436183709719\n",
            "Iteration: 44100. Loss: 0.3447902500629425. Accuracy: 85.18142603400152\n",
            "Iteration: 44200. Loss: 0.25906091928482056. Accuracy: 85.8157827962446\n",
            "Iteration: 44300. Loss: 0.3552868068218231. Accuracy: 85.56204009134737\n",
            "Iteration: 44400. Loss: 0.26469656825065613. Accuracy: 86.37401674701853\n",
            "Iteration: 44500. Loss: 0.07409817725419998. Accuracy: 86.27251966505963\n",
            "Iteration: 44600. Loss: 0.35560843348503113. Accuracy: 76.55417406749557\n",
            "Iteration: 44700. Loss: 0.5050448775291443. Accuracy: 80.46181172291297\n",
            "Iteration: 44800. Loss: 0.4104194939136505. Accuracy: 81.78127378837858\n",
            "Iteration: 44900. Loss: 0.11149363964796066. Accuracy: 86.6785079928952\n",
            "Iteration: 45000. Loss: 0.1425689458847046. Accuracy: 85.33367165693986\n",
            "Iteration: 45100. Loss: 0.13854286074638367. Accuracy: 87.16061913219995\n",
            "Iteration: 45200. Loss: 0.26786482334136963. Accuracy: 85.07992895204264\n",
            "Iteration: 45300. Loss: 0.23313488066196442. Accuracy: 79.92895204262878\n",
            "Iteration: 45400. Loss: 0.3892093598842621. Accuracy: 79.90357777213904\n",
            "Iteration: 45500. Loss: 0.21795035898685455. Accuracy: 85.0545546815529\n",
            "Iteration: 45600. Loss: 0.11970147490501404. Accuracy: 87.31286475513829\n",
            "Iteration: 45700. Loss: 0.06343142688274384. Accuracy: 87.49048464856635\n",
            "Iteration: 45800. Loss: 0.19457267224788666. Accuracy: 87.26211621415884\n",
            "Iteration: 45900. Loss: 0.36537933349609375. Accuracy: 84.0649581324537\n",
            "Iteration: 46000. Loss: 0.21486392617225647. Accuracy: 84.77543770616595\n",
            "Iteration: 46100. Loss: 0.29682987928390503. Accuracy: 87.26211621415884\n",
            "Iteration: 46200. Loss: 0.1796054244041443. Accuracy: 86.42476528799797\n",
            "Iteration: 46300. Loss: 0.23567453026771545. Accuracy: 85.0545546815529\n",
            "Iteration: 46400. Loss: 0.24695561826229095. Accuracy: 86.45013955848769\n",
            "Iteration: 46500. Loss: 0.14445964992046356. Accuracy: 86.3486424765288\n",
            "Iteration: 46600. Loss: 0.6034419536590576. Accuracy: 72.6972849530576\n",
            "Iteration: 46700. Loss: 0.30157196521759033. Accuracy: 84.95305759959402\n",
            "Iteration: 46800. Loss: 0.17945079505443573. Accuracy: 87.66810454199442\n",
            "Iteration: 46900. Loss: 0.17749233543872833. Accuracy: 86.88150215681299\n",
            "Iteration: 47000. Loss: 0.13813799619674683. Accuracy: 87.41436183709719\n",
            "Iteration: 47100. Loss: 0.16449101269245148. Accuracy: 85.53666582085765\n",
            "Iteration: 47200. Loss: 0.1569649875164032. Accuracy: 87.38898756660745\n",
            "Iteration: 47300. Loss: 0.4398193359375. Accuracy: 77.36615072316671\n",
            "Iteration: 47400. Loss: 0.23841427266597748. Accuracy: 85.73965998477544\n",
            "Iteration: 47500. Loss: 0.2579650580883026. Accuracy: 85.89190560771378\n",
            "Iteration: 47600. Loss: 0.8343807458877563. Accuracy: 82.82161887845724\n",
            "Iteration: 47700. Loss: 0.33476483821868896. Accuracy: 81.34991119005329\n",
            "Iteration: 47800. Loss: 0.47638919949531555. Accuracy: 83.9634610504948\n",
            "Iteration: 47900. Loss: 0.4571534991264343. Accuracy: 80.96929713270744\n",
            "Iteration: 48000. Loss: 0.9814208149909973. Accuracy: 70.66734331387973\n",
            "Iteration: 48100. Loss: 0.6735265254974365. Accuracy: 80.25881755899518\n",
            "Iteration: 48200. Loss: 0.7158746123313904. Accuracy: 65.871606191322\n",
            "Iteration: 48300. Loss: 0.2166885882616043. Accuracy: 86.39939101750825\n",
            "Iteration: 48400. Loss: 0.3128392696380615. Accuracy: 83.60822126363867\n",
            "Iteration: 48500. Loss: 0.07856164127588272. Accuracy: 87.28749048464857\n",
            "Iteration: 48600. Loss: 0.48490428924560547. Accuracy: 80.63943161634103\n",
            "Iteration: 48700. Loss: 0.143075630068779. Accuracy: 86.06952550114184\n",
            "Iteration: 48800. Loss: 0.38758939504623413. Accuracy: 81.78127378837858\n",
            "Iteration: 48900. Loss: 0.24637576937675476. Accuracy: 83.9634610504948\n",
            "Iteration: 49000. Loss: 0.20665428042411804. Accuracy: 84.7246891651865\n",
            "Iteration: 49100. Loss: 0.12164784967899323. Accuracy: 86.45013955848769\n",
            "Iteration: 49200. Loss: 0.27141624689102173. Accuracy: 82.111139304745\n",
            "Iteration: 49300. Loss: 0.3574526607990265. Accuracy: 82.87236741943669\n",
            "Iteration: 49400. Loss: 0.0733712837100029. Accuracy: 84.64856635371733\n",
            "Iteration: 49500. Loss: 0.21965628862380981. Accuracy: 87.05912205024106\n",
            "Iteration: 49600. Loss: 0.22757437825202942. Accuracy: 75.97056584623192\n",
            "Iteration: 49700. Loss: 0.1716574728488922. Accuracy: 87.33823902562801\n",
            "Iteration: 49800. Loss: 0.12895894050598145. Accuracy: 87.49048464856635\n",
            "Iteration: 49900. Loss: 0.12049734592437744. Accuracy: 85.33367165693986\n",
            "Iteration: 50000. Loss: 0.2521374523639679. Accuracy: 85.33367165693986\n",
            "Iteration: 50100. Loss: 0.24365560710430145. Accuracy: 85.8157827962446\n",
            "Iteration: 50200. Loss: 0.3415546715259552. Accuracy: 83.9634610504948\n",
            "Iteration: 50300. Loss: 0.15537777543067932. Accuracy: 87.1098705912205\n",
            "Iteration: 50400. Loss: 0.06670763343572617. Accuracy: 88.25171276325806\n",
            "Iteration: 50500. Loss: 0.17976054549217224. Accuracy: 86.45013955848769\n",
            "Iteration: 50600. Loss: 0.14407773315906525. Accuracy: 87.36361329611773\n",
            "Iteration: 50700. Loss: 0.0701967254281044. Accuracy: 87.2113676731794\n",
            "Iteration: 50800. Loss: 0.07674970477819443. Accuracy: 86.83075361583354\n",
            "Iteration: 50900. Loss: 0.21298934519290924. Accuracy: 81.75589951788886\n",
            "Iteration: 51000. Loss: 0.14214906096458435. Accuracy: 87.23674194366912\n",
            "Iteration: 51100. Loss: 0.6325671076774597. Accuracy: 63.43567622430855\n",
            "Iteration: 51200. Loss: 0.18398526310920715. Accuracy: 85.99340268967268\n",
            "Iteration: 51300. Loss: 0.09318627417087555. Accuracy: 87.7696016239533\n",
            "Iteration: 51400. Loss: 0.8037835359573364. Accuracy: 75.33620908398883\n",
            "Iteration: 51500. Loss: 0.07266754657030106. Accuracy: 86.70388226338493\n",
            "Iteration: 51600. Loss: 0.21686625480651855. Accuracy: 82.89774168992642\n",
            "Iteration: 51700. Loss: 1.2280737161636353. Accuracy: 71.60619132199949\n",
            "Iteration: 51800. Loss: 0.09324903786182404. Accuracy: 87.66810454199442\n",
            "Iteration: 51900. Loss: 0.11374342441558838. Accuracy: 87.61735600101497\n",
            "Iteration: 52000. Loss: 0.07016302645206451. Accuracy: 87.69347881248414\n",
            "Iteration: 52100. Loss: 0.3972252905368805. Accuracy: 81.75589951788886\n",
            "Iteration: 52200. Loss: 0.14320194721221924. Accuracy: 86.85612788632326\n",
            "Iteration: 52300. Loss: 0.3618151843547821. Accuracy: 80.74092869829992\n",
            "Iteration: 52400. Loss: 0.07504300773143768. Accuracy: 87.31286475513829\n",
            "Iteration: 52500. Loss: 0.1171032264828682. Accuracy: 86.1202740421213\n",
            "Iteration: 52600. Loss: 0.1850917488336563. Accuracy: 86.0187769601624\n",
            "Iteration: 52700. Loss: 0.13029207289218903. Accuracy: 86.57701091093631\n",
            "Iteration: 52800. Loss: 0.06777141988277435. Accuracy: 87.89647297640192\n",
            "Iteration: 52900. Loss: 0.20264673233032227. Accuracy: 84.54706927175843\n",
            "Iteration: 53000. Loss: 0.7188848257064819. Accuracy: 73.50926160872875\n",
            "Iteration: 53100. Loss: 0.5130859613418579. Accuracy: 80.79167723927937\n",
            "Iteration: 53200. Loss: 0.44932422041893005. Accuracy: 82.0096422227861\n",
            "Iteration: 53300. Loss: 0.5893728137016296. Accuracy: 84.54706927175843\n",
            "Iteration: 53400. Loss: 0.4411452114582062. Accuracy: 82.0096422227861\n",
            "Iteration: 53500. Loss: 0.2935202419757843. Accuracy: 84.7246891651865\n",
            "Iteration: 53600. Loss: 0.437392920255661. Accuracy: 83.35447855874143\n",
            "Iteration: 53700. Loss: 0.202717125415802. Accuracy: 85.99340268967268\n",
            "Iteration: 53800. Loss: 0.33156105875968933. Accuracy: 86.42476528799797\n",
            "Iteration: 53900. Loss: 0.1932881623506546. Accuracy: 86.75463080436437\n",
            "Iteration: 54000. Loss: 0.7092954516410828. Accuracy: 60.213143872113676\n",
            "Iteration: 54100. Loss: 0.2618127167224884. Accuracy: 86.65313372240549\n",
            "Iteration: 54200. Loss: 0.39145514369010925. Accuracy: 79.59908652626237\n",
            "Iteration: 54300. Loss: 0.18514612317085266. Accuracy: 87.28749048464857\n",
            "Iteration: 54400. Loss: 0.24043840169906616. Accuracy: 87.51585891905607\n",
            "Iteration: 54500. Loss: 0.24222688376903534. Accuracy: 83.55747272265923\n",
            "Iteration: 54600. Loss: 0.3506334125995636. Accuracy: 83.5320984521695\n",
            "Iteration: 54700. Loss: 0.2385515570640564. Accuracy: 84.01420959147424\n",
            "Iteration: 54800. Loss: 0.14792177081108093. Accuracy: 85.0545546815529\n",
            "Iteration: 54900. Loss: 0.20446297526359558. Accuracy: 84.19182948490231\n",
            "Iteration: 55000. Loss: 0.09935182332992554. Accuracy: 85.58741436183709\n",
            "Iteration: 55100. Loss: 0.4316444993019104. Accuracy: 74.04212128901294\n",
            "Iteration: 55200. Loss: 0.3514425456523895. Accuracy: 85.46054300938847\n",
            "Iteration: 55300. Loss: 0.10373111069202423. Accuracy: 85.23217457498097\n",
            "Iteration: 55400. Loss: 0.18478503823280334. Accuracy: 88.22633849276833\n",
            "Iteration: 55500. Loss: 0.2781166434288025. Accuracy: 85.38442019791931\n",
            "Iteration: 55600. Loss: 0.06979291886091232. Accuracy: 86.75463080436437\n",
            "Iteration: 55700. Loss: 0.6726856827735901. Accuracy: 68.56127886323269\n",
            "Iteration: 55800. Loss: 0.21690888702869415. Accuracy: 86.72925653387465\n",
            "Iteration: 55900. Loss: 0.14027711749076843. Accuracy: 85.68891144379599\n",
            "Iteration: 56000. Loss: 0.1589314341545105. Accuracy: 87.99797005836082\n",
            "Iteration: 56100. Loss: 0.6667668223381042. Accuracy: 71.7330626744481\n",
            "Iteration: 56200. Loss: 0.14453044533729553. Accuracy: 82.64399898502919\n",
            "Iteration: 56300. Loss: 0.18392321467399597. Accuracy: 87.7696016239533\n",
            "Iteration: 56400. Loss: 0.38975289463996887. Accuracy: 81.88277087033748\n",
            "Iteration: 56500. Loss: 0.15441866219043732. Accuracy: 85.99340268967268\n",
            "Iteration: 56600. Loss: 0.4323427677154541. Accuracy: 78.63486424765289\n",
            "Iteration: 56700. Loss: 0.12324218451976776. Accuracy: 87.23674194366912\n",
            "Iteration: 56800. Loss: 0.22375689446926117. Accuracy: 86.09489977163156\n",
            "Iteration: 56900. Loss: 0.06271781772375107. Accuracy: 87.8710987059122\n",
            "Iteration: 57000. Loss: 0.26691934466362. Accuracy: 83.45597564070033\n",
            "Iteration: 57100. Loss: 0.14170041680335999. Accuracy: 86.29789393554935\n",
            "Iteration: 57200. Loss: 0.1959051489830017. Accuracy: 84.0649581324537\n",
            "Iteration: 57300. Loss: 0.1076175719499588. Accuracy: 86.22177112408018\n",
            "Iteration: 57400. Loss: 0.15879012644290924. Accuracy: 87.26211621415884\n",
            "Iteration: 57500. Loss: 0.07821542769670486. Accuracy: 86.29789393554935\n",
            "Iteration: 57600. Loss: 0.1358054280281067. Accuracy: 87.46511037807663\n",
            "Iteration: 57700. Loss: 0.13525083661079407. Accuracy: 87.7696016239533\n",
            "Iteration: 57800. Loss: 1.9429353475570679. Accuracy: 23.318954580055824\n",
            "Iteration: 57900. Loss: 0.22674298286437988. Accuracy: 86.0187769601624\n",
            "Iteration: 58000. Loss: 0.09067902714014053. Accuracy: 87.69347881248414\n",
            "Iteration: 58100. Loss: 0.11622583866119385. Accuracy: 85.89190560771378\n",
            "Iteration: 58200. Loss: 0.16534903645515442. Accuracy: 86.04415123065212\n",
            "Iteration: 58300. Loss: 0.1772170513868332. Accuracy: 86.32326820603907\n",
            "Iteration: 58400. Loss: 0.11162330210208893. Accuracy: 85.1560517635118\n",
            "Iteration: 58500. Loss: 0.5613082051277161. Accuracy: 78.83785841157066\n",
            "Iteration: 58600. Loss: 0.050162602216005325. Accuracy: 87.74422735346359\n",
            "Iteration: 58700. Loss: 0.18047356605529785. Accuracy: 87.56660746003553\n",
            "Iteration: 58800. Loss: 0.13791678845882416. Accuracy: 86.39939101750825\n",
            "Iteration: 58900. Loss: 0.035026129335165024. Accuracy: 86.17102258310074\n",
            "Iteration: 59000. Loss: 1.5106056928634644. Accuracy: 75.31083481349911\n",
            "Iteration: 59100. Loss: 0.058303102850914. Accuracy: 87.61735600101497\n",
            "Iteration: 59200. Loss: 0.2262718826532364. Accuracy: 86.19639685359046\n",
            "Iteration: 59300. Loss: 0.2460314929485321. Accuracy: 83.65896980461812\n",
            "Iteration: 59400. Loss: 0.07433612644672394. Accuracy: 87.82035016493276\n",
            "Iteration: 59500. Loss: 0.16736285388469696. Accuracy: 83.43060137021061\n",
            "Iteration: 59600. Loss: 0.10123778879642487. Accuracy: 87.08449632073078\n",
            "Iteration: 59700. Loss: 0.3215862512588501. Accuracy: 77.69601623953311\n",
            "Iteration: 59800. Loss: 0.06901136785745621. Accuracy: 87.46511037807663\n",
            "Iteration: 59900. Loss: 0.07375984638929367. Accuracy: 88.07409286983\n",
            "Iteration: 60000. Loss: 0.15343573689460754. Accuracy: 85.68891144379599\n",
            "Iteration: 60100. Loss: 0.05228247120976448. Accuracy: 87.66810454199442\n",
            "Iteration: 60200. Loss: 0.10939152538776398. Accuracy: 88.09946714031972\n",
            "Iteration: 60300. Loss: 0.13436834514141083. Accuracy: 86.17102258310074\n",
            "Iteration: 60400. Loss: 0.1750940978527069. Accuracy: 85.58741436183709\n",
            "Iteration: 60500. Loss: 0.06217935308814049. Accuracy: 87.59198173052525\n",
            "Iteration: 60600. Loss: 0.08503352105617523. Accuracy: 88.25171276325806\n",
            "Iteration: 60700. Loss: 0.25274181365966797. Accuracy: 84.11570667343314\n",
            "Iteration: 60800. Loss: 0.10476239770650864. Accuracy: 87.66810454199442\n",
            "Iteration: 60900. Loss: 0.5124396085739136. Accuracy: 85.35904592742959\n",
            "Iteration: 61000. Loss: 0.2012351006269455. Accuracy: 84.01420959147424\n",
            "Iteration: 61100. Loss: 0.2171684354543686. Accuracy: 87.5412331895458\n",
            "Iteration: 61200. Loss: 0.07870815694332123. Accuracy: 87.84572443542248\n",
            "Iteration: 61300. Loss: 0.10840056091547012. Accuracy: 86.55163664044659\n",
            "Iteration: 61400. Loss: 0.14863131940364838. Accuracy: 86.62775945191575\n",
            "Iteration: 61500. Loss: 0.034087736159563065. Accuracy: 86.88150215681299\n",
            "Iteration: 61600. Loss: 0.04467567801475525. Accuracy: 87.82035016493276\n",
            "Iteration: 61700. Loss: 0.1735462099313736. Accuracy: 86.45013955848769\n",
            "Iteration: 61800. Loss: 0.11251342296600342. Accuracy: 87.94722151738138\n",
            "Iteration: 61900. Loss: 0.18430937826633453. Accuracy: 87.05912205024106\n",
            "Iteration: 62000. Loss: 0.13857130706310272. Accuracy: 88.55620400913473\n",
            "Iteration: 62100. Loss: 0.13862130045890808. Accuracy: 86.06952550114184\n",
            "Iteration: 62200. Loss: 0.09305492043495178. Accuracy: 86.90687642730272\n",
            "Iteration: 62300. Loss: 0.07385271787643433. Accuracy: 86.7800050748541\n",
            "Iteration: 62400. Loss: 0.05837739259004593. Accuracy: 87.84572443542248\n",
            "Iteration: 62500. Loss: 0.07000530511140823. Accuracy: 86.7800050748541\n",
            "Iteration: 62600. Loss: 0.17933893203735352. Accuracy: 85.43516873889875\n",
            "Iteration: 62700. Loss: 0.2706831991672516. Accuracy: 82.92311596041614\n",
            "Iteration: 62800. Loss: 0.06526800245046616. Accuracy: 88.63232682060391\n",
            "Iteration: 62900. Loss: 0.11167369782924652. Accuracy: 88.07409286983\n",
            "Iteration: 63000. Loss: 0.1778428852558136. Accuracy: 86.85612788632326\n",
            "Iteration: 63100. Loss: 0.05359501764178276. Accuracy: 88.58157827962447\n",
            "Iteration: 63200. Loss: 0.06214774772524834. Accuracy: 88.53082973864501\n",
            "Iteration: 63300. Loss: 0.034473031759262085. Accuracy: 88.78457244354225\n",
            "Iteration: 63400. Loss: 0.08469744026660919. Accuracy: 88.27708703374778\n",
            "Iteration: 63500. Loss: 0.16621217131614685. Accuracy: 85.84115706673433\n",
            "Iteration: 63600. Loss: 0.16686424612998962. Accuracy: 86.27251966505963\n",
            "Iteration: 63700. Loss: 0.17983229458332062. Accuracy: 85.66353717330627\n",
            "Iteration: 63800. Loss: 0.045953039079904556. Accuracy: 87.9725957878711\n",
            "Iteration: 63900. Loss: 0.05533270910382271. Accuracy: 88.25171276325806\n",
            "Iteration: 64000. Loss: 0.072409488260746. Accuracy: 88.07409286983\n",
            "Iteration: 64100. Loss: 0.29486095905303955. Accuracy: 85.51129155036793\n",
            "Iteration: 64200. Loss: 0.1939079612493515. Accuracy: 87.99797005836082\n",
            "Iteration: 64300. Loss: 0.10877566784620285. Accuracy: 88.50545546815529\n",
            "Iteration: 64400. Loss: 0.24395348131656647. Accuracy: 84.77543770616595\n",
            "Iteration: 64500. Loss: 0.23360008001327515. Accuracy: 86.93225069779244\n",
            "Iteration: 64600. Loss: 0.05515017732977867. Accuracy: 87.13524486171022\n",
            "Iteration: 64700. Loss: 0.08070585131645203. Accuracy: 88.37858411570667\n",
            "Iteration: 64800. Loss: 0.08791092783212662. Accuracy: 87.08449632073078\n",
            "Iteration: 64900. Loss: 0.06001284345984459. Accuracy: 87.7696016239533\n",
            "Iteration: 65000. Loss: 0.06110770255327225. Accuracy: 87.89647297640192\n",
            "Iteration: 65100. Loss: 0.05128343775868416. Accuracy: 87.64273027150469\n",
            "Iteration: 65200. Loss: 0.09777316451072693. Accuracy: 87.56660746003553\n",
            "Iteration: 65300. Loss: 0.09355099499225616. Accuracy: 87.51585891905607\n",
            "Iteration: 65400. Loss: 0.07163810729980469. Accuracy: 88.58157827962447\n",
            "Iteration: 65500. Loss: 0.07125165313482285. Accuracy: 86.83075361583354\n",
            "Iteration: 65600. Loss: 0.15508656203746796. Accuracy: 86.17102258310074\n",
            "Iteration: 65700. Loss: 0.20379658043384552. Accuracy: 86.62775945191575\n",
            "Iteration: 65800. Loss: 0.10330567508935928. Accuracy: 87.31286475513829\n",
            "Iteration: 65900. Loss: 0.04821265488862991. Accuracy: 87.2113676731794\n",
            "Iteration: 66000. Loss: 0.05734192952513695. Accuracy: 88.25171276325806\n",
            "Iteration: 66100. Loss: 0.07909271866083145. Accuracy: 85.68891144379599\n",
            "Iteration: 66200. Loss: 0.450061172246933. Accuracy: 81.29916264907384\n",
            "Iteration: 66300. Loss: 0.16637329757213593. Accuracy: 83.48134991119005\n",
            "Iteration: 66400. Loss: 0.2867159843444824. Accuracy: 83.43060137021061\n",
            "Iteration: 66500. Loss: 0.07472722977399826. Accuracy: 87.94722151738138\n",
            "Iteration: 66600. Loss: 0.030185675248503685. Accuracy: 89.44430347627505\n",
            "Iteration: 66700. Loss: 0.08320305496454239. Accuracy: 86.1202740421213\n",
            "Iteration: 66800. Loss: 0.14864051342010498. Accuracy: 85.61278863232683\n",
            "Iteration: 66900. Loss: 0.20982280373573303. Accuracy: 86.24714539456991\n",
            "Iteration: 67000. Loss: 0.08927110582590103. Accuracy: 87.66810454199442\n",
            "Iteration: 67100. Loss: 0.30588632822036743. Accuracy: 82.69474752600863\n",
            "Iteration: 67200. Loss: 0.14917784929275513. Accuracy: 87.43973610758691\n",
            "Iteration: 67300. Loss: 0.12748563289642334. Accuracy: 86.83075361583354\n",
            "Iteration: 67400. Loss: 0.14809942245483398. Accuracy: 83.88733823902562\n",
            "Iteration: 67500. Loss: 0.02820301614701748. Accuracy: 88.22633849276833\n",
            "Iteration: 67600. Loss: 0.3577956557273865. Accuracy: 83.760466886577\n",
            "Iteration: 67700. Loss: 0.05224577710032463. Accuracy: 88.15021568129916\n",
            "Iteration: 67800. Loss: 0.07233091443777084. Accuracy: 88.35320984521695\n",
            "Iteration: 67900. Loss: 0.06910037249326706. Accuracy: 86.29789393554935\n",
            "Iteration: 68000. Loss: 0.033307287842035294. Accuracy: 88.42933265668611\n",
            "Iteration: 68100. Loss: 0.4412565231323242. Accuracy: 83.83658969804618\n",
            "Iteration: 68200. Loss: 0.05654255300760269. Accuracy: 87.8710987059122\n",
            "Iteration: 68300. Loss: 0.00847934652119875. Accuracy: 88.80994671403197\n",
            "Iteration: 68400. Loss: 0.02321290224790573. Accuracy: 88.35320984521695\n",
            "Iteration: 68500. Loss: 0.05214518681168556. Accuracy: 87.33823902562801\n",
            "Iteration: 68600. Loss: 0.05503082275390625. Accuracy: 88.42933265668611\n",
            "Iteration: 68700. Loss: 0.06600309908390045. Accuracy: 87.99797005836082\n",
            "Iteration: 68800. Loss: 0.0907900407910347. Accuracy: 87.23674194366912\n",
            "Iteration: 68900. Loss: 0.577866792678833. Accuracy: 74.44810961684851\n",
            "Iteration: 69000. Loss: 0.07965502142906189. Accuracy: 87.51585891905607\n",
            "Iteration: 69100. Loss: 0.06636957824230194. Accuracy: 88.50545546815529\n",
            "Iteration: 69200. Loss: 0.040630340576171875. Accuracy: 87.84572443542248\n",
            "Iteration: 69300. Loss: 0.32024386525154114. Accuracy: 74.9809692971327\n",
            "Iteration: 69400. Loss: 0.2151755392551422. Accuracy: 83.60822126363867\n",
            "Iteration: 69500. Loss: 0.014718168415129185. Accuracy: 89.24130931235727\n",
            "Iteration: 69600. Loss: 0.12304646521806717. Accuracy: 84.90230905861456\n",
            "Iteration: 69700. Loss: 0.10042602568864822. Accuracy: 87.33823902562801\n",
            "Iteration: 69800. Loss: 0.012306559830904007. Accuracy: 88.68307536158335\n",
            "Iteration: 69900. Loss: 0.17014382779598236. Accuracy: 84.3948236488201\n",
            "Iteration: 70000. Loss: 0.10854919999837875. Accuracy: 88.48008119766557\n",
            "Iteration: 70100. Loss: 0.15217521786689758. Accuracy: 87.08449632073078\n",
            "Iteration: 70200. Loss: 0.07850364595651627. Accuracy: 87.38898756660745\n",
            "Iteration: 70300. Loss: 0.056049130856990814. Accuracy: 88.15021568129916\n",
            "Iteration: 70400. Loss: 0.44754576683044434. Accuracy: 75.71682314133469\n",
            "Iteration: 70500. Loss: 1.551256775856018. Accuracy: 73.53463587921847\n",
            "Iteration: 70600. Loss: 0.12215732038021088. Accuracy: 86.70388226338493\n",
            "Iteration: 70700. Loss: 0.014326324686408043. Accuracy: 88.60695255011419\n",
            "Iteration: 70800. Loss: 0.3338322043418884. Accuracy: 81.02004567368688\n",
            "Iteration: 70900. Loss: 1.0454461574554443. Accuracy: 58.46231920832276\n",
            "Iteration: 71000. Loss: 0.042750194668769836. Accuracy: 88.37858411570667\n",
            "Iteration: 71100. Loss: 0.09303084760904312. Accuracy: 85.89190560771378\n",
            "Iteration: 71200. Loss: 0.1211099848151207. Accuracy: 87.03374777975134\n",
            "Iteration: 71300. Loss: 0.0873216763138771. Accuracy: 87.56660746003553\n",
            "Iteration: 71400. Loss: 0.038105662912130356. Accuracy: 88.50545546815529\n",
            "Iteration: 71500. Loss: 0.03865235671401024. Accuracy: 88.78457244354225\n",
            "Iteration: 71600. Loss: 0.029228804633021355. Accuracy: 86.83075361583354\n",
            "Iteration: 71700. Loss: 0.049377404153347015. Accuracy: 87.33823902562801\n",
            "Iteration: 71800. Loss: 0.07762163132429123. Accuracy: 86.3486424765288\n",
            "Iteration: 71900. Loss: 0.06704887747764587. Accuracy: 88.55620400913473\n",
            "Iteration: 72000. Loss: 0.3307999074459076. Accuracy: 80.74092869829992\n",
            "Iteration: 72100. Loss: 0.09903853386640549. Accuracy: 87.13524486171022\n",
            "Iteration: 72200. Loss: 0.033565789461135864. Accuracy: 87.03374777975134\n",
            "Iteration: 72300. Loss: 0.5435599088668823. Accuracy: 83.15148439482365\n",
            "Iteration: 72400. Loss: 0.0640624389052391. Accuracy: 88.25171276325806\n",
            "Iteration: 72500. Loss: 0.03440579026937485. Accuracy: 87.94722151738138\n",
            "Iteration: 72600. Loss: 0.05050928518176079. Accuracy: 87.61735600101497\n",
            "Iteration: 72700. Loss: 0.007502744439989328. Accuracy: 88.37858411570667\n",
            "Iteration: 72800. Loss: 0.10888568311929703. Accuracy: 87.26211621415884\n",
            "Iteration: 72900. Loss: 0.032483313232660294. Accuracy: 86.70388226338493\n",
            "Iteration: 73000. Loss: 0.18389949202537537. Accuracy: 86.39939101750825\n",
            "Iteration: 73100. Loss: 0.0664450079202652. Accuracy: 88.27708703374778\n",
            "Iteration: 73200. Loss: 0.03354562073945999. Accuracy: 88.45470692717585\n",
            "Iteration: 73300. Loss: 0.05983802676200867. Accuracy: 88.55620400913473\n",
            "Iteration: 73400. Loss: 0.05978132039308548. Accuracy: 88.86069525501142\n",
            "Iteration: 73500. Loss: 0.0901276171207428. Accuracy: 85.94265414869322\n",
            "Iteration: 73600. Loss: 0.03205356374382973. Accuracy: 88.20096422227861\n",
            "Iteration: 73700. Loss: 0.06885279715061188. Accuracy: 87.66810454199442\n",
            "Iteration: 73800. Loss: 0.15336833894252777. Accuracy: 85.48591727987821\n",
            "Iteration: 73900. Loss: 0.03355727717280388. Accuracy: 88.48008119766557\n",
            "Iteration: 74000. Loss: 0.016255151480436325. Accuracy: 88.63232682060391\n",
            "Iteration: 74100. Loss: 0.01247505471110344. Accuracy: 88.96219233697032\n",
            "Iteration: 74200. Loss: 0.024139219895005226. Accuracy: 88.60695255011419\n",
            "Iteration: 74300. Loss: 0.03318741172552109. Accuracy: 88.3024613042375\n",
            "Iteration: 74400. Loss: 0.03372194617986679. Accuracy: 88.53082973864501\n",
            "Iteration: 74500. Loss: 0.05947743356227875. Accuracy: 88.78457244354225\n",
            "Iteration: 74600. Loss: 0.03895270451903343. Accuracy: 86.55163664044659\n",
            "Iteration: 74700. Loss: 0.2064065784215927. Accuracy: 87.5412331895458\n",
            "Iteration: 74800. Loss: 0.9359699487686157. Accuracy: 72.01217964983506\n",
            "Iteration: 74900. Loss: 0.07567545771598816. Accuracy: 87.56660746003553\n",
            "Iteration: 75000. Loss: 0.09332933276891708. Accuracy: 82.13651357523472\n",
            "Iteration: 75100. Loss: 0.39670121669769287. Accuracy: 81.85739659984776\n",
            "Iteration: 75200. Loss: 0.02968248538672924. Accuracy: 88.86069525501142\n",
            "Iteration: 75300. Loss: 0.05434735491871834. Accuracy: 87.51585891905607\n",
            "Iteration: 75400. Loss: 0.049537744373083115. Accuracy: 89.31743212382644\n",
            "Iteration: 75500. Loss: 0.03907652199268341. Accuracy: 88.15021568129916\n",
            "Iteration: 75600. Loss: 0.04013656824827194. Accuracy: 87.41436183709719\n",
            "Iteration: 75700. Loss: 0.029975512996315956. Accuracy: 87.74422735346359\n",
            "Iteration: 75800. Loss: 0.1534227430820465. Accuracy: 85.61278863232683\n",
            "Iteration: 75900. Loss: 0.13271307945251465. Accuracy: 88.09946714031972\n",
            "Iteration: 76000. Loss: 0.03650064766407013. Accuracy: 85.43516873889875\n",
            "Iteration: 76100. Loss: 0.01299665030092001. Accuracy: 88.63232682060391\n",
            "Iteration: 76200. Loss: 0.08069396764039993. Accuracy: 88.27708703374778\n",
            "Iteration: 76300. Loss: 0.04936918243765831. Accuracy: 86.75463080436437\n",
            "Iteration: 76400. Loss: 0.06567061692476273. Accuracy: 88.15021568129916\n",
            "Iteration: 76500. Loss: 0.15266019105911255. Accuracy: 86.83075361583354\n",
            "Iteration: 76600. Loss: 0.06196468323469162. Accuracy: 86.60238518142603\n",
            "Iteration: 76700. Loss: 0.06668543070554733. Accuracy: 87.92184724689166\n",
            "Iteration: 76800. Loss: 0.04778202250599861. Accuracy: 87.38898756660745\n",
            "Iteration: 76900. Loss: 0.3651299774646759. Accuracy: 82.94849023090586\n",
            "Iteration: 77000. Loss: 0.031319279223680496. Accuracy: 88.45470692717585\n",
            "Iteration: 77100. Loss: 0.13190922141075134. Accuracy: 87.16061913219995\n",
            "Iteration: 77200. Loss: 0.04723528400063515. Accuracy: 87.89647297640192\n",
            "Iteration: 77300. Loss: 0.056648675352334976. Accuracy: 88.60695255011419\n",
            "Iteration: 77400. Loss: 0.010870409198105335. Accuracy: 88.75919817305252\n",
            "Iteration: 77500. Loss: 0.3244197964668274. Accuracy: 70.61659477290029\n",
            "Iteration: 77600. Loss: 0.27608054876327515. Accuracy: 81.42603400152245\n",
            "Iteration: 77700. Loss: 0.18282420933246613. Accuracy: 85.23217457498097\n",
            "Iteration: 77800. Loss: 0.090545654296875. Accuracy: 87.36361329611773\n",
            "Iteration: 77900. Loss: 0.11386200785636902. Accuracy: 87.0083735092616\n",
            "Iteration: 78000. Loss: 0.08862530440092087. Accuracy: 85.40979446840903\n",
            "Iteration: 78100. Loss: 0.034658681601285934. Accuracy: 87.51585891905607\n",
            "Iteration: 78200. Loss: 0.05646131932735443. Accuracy: 88.02334432885054\n",
            "Iteration: 78300. Loss: 0.08337070047855377. Accuracy: 88.42933265668611\n",
            "Iteration: 78400. Loss: 0.07881468534469604. Accuracy: 86.14564831261102\n",
            "Iteration: 78500. Loss: 0.044277388602495193. Accuracy: 87.59198173052525\n",
            "Iteration: 78600. Loss: 0.009281002916395664. Accuracy: 88.4039583861964\n",
            "Iteration: 78700. Loss: 0.02278577722609043. Accuracy: 87.05912205024106\n",
            "Iteration: 78800. Loss: 0.04138336703181267. Accuracy: 87.94722151738138\n",
            "Iteration: 78900. Loss: 0.009343348443508148. Accuracy: 87.94722151738138\n",
            "Iteration: 79000. Loss: 0.05318782478570938. Accuracy: 87.28749048464857\n",
            "Iteration: 79100. Loss: 0.18935845792293549. Accuracy: 87.33823902562801\n",
            "Iteration: 79200. Loss: 0.33849018812179565. Accuracy: 78.38112154275565\n",
            "Iteration: 79300. Loss: 0.016185352578759193. Accuracy: 88.48008119766557\n",
            "Iteration: 79400. Loss: 0.026441551744937897. Accuracy: 88.20096422227861\n",
            "Iteration: 79500. Loss: 0.162456214427948. Accuracy: 83.5320984521695\n",
            "Iteration: 79600. Loss: 0.010881883092224598. Accuracy: 88.48008119766557\n",
            "Iteration: 79700. Loss: 0.0163156446069479. Accuracy: 88.78457244354225\n",
            "Iteration: 79800. Loss: 0.008241386152803898. Accuracy: 89.266683582847\n",
            "Iteration: 79900. Loss: 0.10317546129226685. Accuracy: 85.00380614057346\n",
            "Iteration: 80000. Loss: 0.026946531608700752. Accuracy: 88.68307536158335\n",
            "Iteration: 80100. Loss: 0.01630316860973835. Accuracy: 88.04871859934026\n",
            "Iteration: 80200. Loss: 0.09852664917707443. Accuracy: 87.46511037807663\n",
            "Iteration: 80300. Loss: 0.08643341064453125. Accuracy: 86.72925653387465\n",
            "Iteration: 80400. Loss: 0.030303630977869034. Accuracy: 88.35320984521695\n",
            "Iteration: 80500. Loss: 0.03722282499074936. Accuracy: 88.25171276325806\n",
            "Iteration: 80600. Loss: 0.012697024270892143. Accuracy: 88.58157827962447\n",
            "Iteration: 80700. Loss: 0.04103609547019005. Accuracy: 88.27708703374778\n",
            "Iteration: 80800. Loss: 0.0142182856798172. Accuracy: 88.4039583861964\n",
            "Iteration: 80900. Loss: 0.059989720582962036. Accuracy: 86.45013955848769\n",
            "Iteration: 81000. Loss: 0.07730020582675934. Accuracy: 87.92184724689166\n",
            "Iteration: 81100. Loss: 0.09045058488845825. Accuracy: 83.40522709972089\n",
            "Iteration: 81200. Loss: 0.03500734269618988. Accuracy: 87.99797005836082\n",
            "Iteration: 81300. Loss: 0.06812018901109695. Accuracy: 87.59198173052525\n",
            "Iteration: 81400. Loss: 0.4824211299419403. Accuracy: 82.79624460796752\n",
            "Iteration: 81500. Loss: 0.01043914258480072. Accuracy: 88.3024613042375\n",
            "Iteration: 81600. Loss: 0.009153672493994236. Accuracy: 88.68307536158335\n",
            "Iteration: 81700. Loss: 1.0943326950073242. Accuracy: 79.01547830499874\n",
            "Iteration: 81800. Loss: 0.046484850347042084. Accuracy: 88.3024613042375\n",
            "Iteration: 81900. Loss: 0.017337724566459656. Accuracy: 88.04871859934026\n",
            "Iteration: 82000. Loss: 0.09185200184583664. Accuracy: 88.09946714031972\n",
            "Iteration: 82100. Loss: 0.20058630406856537. Accuracy: 86.39939101750825\n",
            "Iteration: 82200. Loss: 0.009645114652812481. Accuracy: 88.45470692717585\n",
            "Iteration: 82300. Loss: 0.07149849832057953. Accuracy: 86.04415123065212\n",
            "Iteration: 82400. Loss: 0.005626767873764038. Accuracy: 87.74422735346359\n",
            "Iteration: 82500. Loss: 0.044218458235263824. Accuracy: 88.50545546815529\n",
            "Iteration: 82600. Loss: 0.03502054512500763. Accuracy: 88.09946714031972\n",
            "Iteration: 82700. Loss: 0.01027732901275158. Accuracy: 88.55620400913473\n",
            "Iteration: 82800. Loss: 0.06598688662052155. Accuracy: 88.12484141080944\n",
            "Iteration: 82900. Loss: 0.1070224866271019. Accuracy: 86.50088809946715\n",
            "Iteration: 83000. Loss: 0.047684185206890106. Accuracy: 87.7696016239533\n",
            "Iteration: 83100. Loss: 0.019189367070794106. Accuracy: 88.86069525501142\n",
            "Iteration: 83200. Loss: 0.725485622882843. Accuracy: 81.83202232935803\n",
            "Iteration: 83300. Loss: 0.03128428757190704. Accuracy: 87.84572443542248\n",
            "Iteration: 83400. Loss: 0.010189379565417767. Accuracy: 88.91144379599086\n",
            "Iteration: 83500. Loss: 0.008708520792424679. Accuracy: 88.7338239025628\n",
            "Iteration: 83600. Loss: 0.0015095866983756423. Accuracy: 89.1651865008881\n",
            "Iteration: 83700. Loss: 0.019579481333494186. Accuracy: 88.4039583861964\n",
            "Iteration: 83800. Loss: 0.028660370036959648. Accuracy: 87.36361329611773\n",
            "Iteration: 83900. Loss: 0.016821976751089096. Accuracy: 88.88606952550114\n",
            "Iteration: 84000. Loss: 0.006798856891691685. Accuracy: 88.8353209845217\n",
            "Iteration: 84100. Loss: 0.2223787009716034. Accuracy: 83.81121542755646\n",
            "Iteration: 84200. Loss: 0.109348364174366. Accuracy: 85.66353717330627\n",
            "Iteration: 84300. Loss: 0.8927282691001892. Accuracy: 67.34331387972595\n",
            "Iteration: 84400. Loss: 0.030270220711827278. Accuracy: 87.99797005836082\n",
            "Iteration: 84500. Loss: 0.07541928440332413. Accuracy: 87.69347881248414\n",
            "Iteration: 84600. Loss: 0.08266863971948624. Accuracy: 86.57701091093631\n",
            "Iteration: 84700. Loss: 0.0748327225446701. Accuracy: 87.36361329611773\n",
            "Iteration: 84800. Loss: 0.032886117696762085. Accuracy: 88.32783557472723\n",
            "Iteration: 84900. Loss: 0.1763095259666443. Accuracy: 86.6785079928952\n",
            "Iteration: 85000. Loss: 0.008362255990505219. Accuracy: 88.58157827962447\n",
            "Iteration: 85100. Loss: 0.49528074264526367. Accuracy: 81.55290535397107\n",
            "Iteration: 85200. Loss: 0.03825531154870987. Accuracy: 86.6785079928952\n",
            "Iteration: 85300. Loss: 0.050655003637075424. Accuracy: 85.73965998477544\n",
            "Iteration: 85400. Loss: 0.034219756722450256. Accuracy: 88.45470692717585\n",
            "Iteration: 85500. Loss: 0.01320493221282959. Accuracy: 88.22633849276833\n",
            "Iteration: 85600. Loss: 0.01277871336787939. Accuracy: 89.29205785333671\n",
            "Iteration: 85700. Loss: 0.05054355785250664. Accuracy: 87.1098705912205\n",
            "Iteration: 85800. Loss: 0.05002331733703613. Accuracy: 85.84115706673433\n",
            "Iteration: 85900. Loss: 0.019045105203986168. Accuracy: 88.20096422227861\n",
            "Iteration: 86000. Loss: 0.047275360673666. Accuracy: 87.8710987059122\n",
            "Iteration: 86100. Loss: 0.631888747215271. Accuracy: 86.72925653387465\n",
            "Iteration: 86200. Loss: 0.009546367451548576. Accuracy: 88.20096422227861\n",
            "Iteration: 86300. Loss: 0.00481702946126461. Accuracy: 88.75919817305252\n",
            "Iteration: 86400. Loss: 0.004693272057920694. Accuracy: 88.80994671403197\n",
            "Iteration: 86500. Loss: 0.016453292220830917. Accuracy: 88.8353209845217\n",
            "Iteration: 86600. Loss: 0.06788966059684753. Accuracy: 86.80537934534382\n",
            "Iteration: 86700. Loss: 0.008476022630929947. Accuracy: 88.98756660746004\n",
            "Iteration: 86800. Loss: 0.005851622670888901. Accuracy: 89.08906368941894\n",
            "Iteration: 86900. Loss: 0.0019128724234178662. Accuracy: 89.11443795990866\n",
            "Iteration: 87000. Loss: 0.004591137170791626. Accuracy: 88.88606952550114\n",
            "Iteration: 87100. Loss: 0.0008333768346346915. Accuracy: 89.36818066480589\n",
            "Iteration: 87200. Loss: 0.010876458138227463. Accuracy: 88.60695255011419\n",
            "Iteration: 87300. Loss: 0.010673479177057743. Accuracy: 89.11443795990866\n",
            "Iteration: 87400. Loss: 0.001259261043742299. Accuracy: 89.13981223039838\n",
            "Iteration: 87500. Loss: 0.001627330668270588. Accuracy: 89.01294087794976\n",
            "Iteration: 87600. Loss: 0.0008447864674963057. Accuracy: 89.11443795990866\n",
            "Iteration: 87700. Loss: 0.00057798414491117. Accuracy: 89.266683582847\n",
            "Iteration: 87800. Loss: 0.0013987745624035597. Accuracy: 89.34280639431617\n",
            "Iteration: 87900. Loss: 0.0006504298071376979. Accuracy: 89.31743212382644\n",
            "Iteration: 88000. Loss: 0.00033851154148578644. Accuracy: 89.266683582847\n",
            "Iteration: 88100. Loss: 0.0018469853093847632. Accuracy: 89.36818066480589\n",
            "Iteration: 88200. Loss: 0.002219647401943803. Accuracy: 89.24130931235727\n",
            "Iteration: 88300. Loss: 0.0006693433970212936. Accuracy: 89.39355493529561\n",
            "Iteration: 88400. Loss: 0.00040847729542292655. Accuracy: 89.36818066480589\n",
            "Iteration: 88500. Loss: 0.0004011488053947687. Accuracy: 89.266683582847\n",
            "Iteration: 88600. Loss: 0.0003873807145282626. Accuracy: 89.34280639431617\n",
            "Iteration: 88700. Loss: 0.02207537367939949. Accuracy: 87.69347881248414\n",
            "Iteration: 88800. Loss: 0.08868211507797241. Accuracy: 86.24714539456991\n",
            "Iteration: 88900. Loss: 0.029995949938893318. Accuracy: 87.8710987059122\n",
            "Iteration: 89000. Loss: 0.01229733694344759. Accuracy: 88.68307536158335\n",
            "Iteration: 89100. Loss: 0.007286881096661091. Accuracy: 88.96219233697032\n",
            "Iteration: 89200. Loss: 0.0014358822954818606. Accuracy: 88.93681806648058\n",
            "Iteration: 89300. Loss: 0.004625939764082432. Accuracy: 89.21593504186755\n",
            "Iteration: 89400. Loss: 0.0008497023954987526. Accuracy: 88.75919817305252\n",
            "Iteration: 89500. Loss: 0.06534259766340256. Accuracy: 84.95305759959402\n",
            "Iteration: 89600. Loss: 0.18225139379501343. Accuracy: 86.24714539456991\n",
            "Iteration: 89700. Loss: 0.007637408562004566. Accuracy: 88.42933265668611\n",
            "Iteration: 89800. Loss: 0.015591060742735863. Accuracy: 88.53082973864501\n",
            "Iteration: 89900. Loss: 0.507677435874939. Accuracy: 79.92895204262878\n",
            "Iteration: 90000. Loss: 0.08377019315958023. Accuracy: 86.50088809946715\n",
            "Iteration: 90100. Loss: 0.011584838852286339. Accuracy: 88.17558995178888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI78mpcNij2e"
      },
      "source": [
        "##Saving data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKjBNFnrbrMl"
      },
      "source": [
        "s = 'model_exp21'\n",
        "with open(s,'wb') as f:\n",
        "  pickle.dump(model,f)\n",
        "\n",
        "s = 'iter_exp21'\n",
        "with open(s,'wb') as f:\n",
        "  pickle.dump(iter_,f)\n",
        "\n",
        "s = 'loss_exp21'\n",
        "with open(s,'wb') as f:\n",
        "  pickle.dump(loss_,f)\n",
        "\n",
        "s = 'acc_exp21'\n",
        "with open(s,'wb') as f:\n",
        "  pickle.dump(accuracy_,f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kOuLGsZPPiD"
      },
      "source": [
        "#Aproach 6\n",
        "batch size = 64<br>\n",
        "<font color = 'tiffani blue'> num_iters = 50000 </font><br>\n",
        "input_dim = 1 * 28 * 28 <br>\n",
        " <font color = 'tiffani blue'> num_hidden = 200  </font><br>\n",
        "output_dim = 10 <br>\n",
        "learning_rate = 0.1 <br>\n",
        "Activation = ReLU <br>\n",
        "Criterion = Cross Entropy <br>\n",
        "Optimizar = SGD <br>\n",
        "<font color = 'green'> Accuracy = 88.53% </font><br>\n",
        "Comment: I have reduced the iteration but increased the number of hidden nodes but the accuracy remains same.<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpTmkYYac8zT",
        "outputId": "0cea4d4e-e8fb-4b9b-a8bf-1a20f065cde7"
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "batch_size = 64\n",
        "num_iters = 50000\n",
        "input_dim = 1*28*28 # num_features = 784\n",
        "num_hidden = 200\n",
        "output_dim = 10\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle= False, batch_size=batch_size)\n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "\n",
        "        self.linear_4 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_4 = nn.ReLU()\n",
        "        \n",
        "        \n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_5 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_5 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_6 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_6 = nn.ReLU()\n",
        "        \n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_4(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_4(out)\n",
        "        \n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_5(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_5(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_6(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_6(out)\n",
        "        \n",
        "\n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "iter_ = []\n",
        "loss_ = []\n",
        "accuracy_ = []\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 1*28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 1*28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "            iter_.append(iter)\n",
        "            loss_.append(loss.item())\n",
        "            accuracy_.append(accuracy)\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500. Loss: 2.3051695823669434. Accuracy: 9.566099974625729\n",
            "Iteration: 1000. Loss: 2.2915890216827393. Accuracy: 16.721644252727735\n",
            "Iteration: 1500. Loss: 2.229556083679199. Accuracy: 10.68256787617356\n",
            "Iteration: 2000. Loss: 2.2459046840667725. Accuracy: 15.782796244607967\n",
            "Iteration: 2500. Loss: 2.283015489578247. Accuracy: 20.451662014717076\n",
            "Iteration: 3000. Loss: 2.081209897994995. Accuracy: 17.711240801826946\n",
            "Iteration: 3500. Loss: 2.2196500301361084. Accuracy: 17.635117990357777\n",
            "Iteration: 4000. Loss: 2.0253336429595947. Accuracy: 17.482872367419436\n",
            "Iteration: 4500. Loss: 2.075860023498535. Accuracy: 28.19081451408272\n",
            "Iteration: 5000. Loss: 2.0268683433532715. Accuracy: 28.292311596041614\n",
            "Iteration: 5500. Loss: 2.0625271797180176. Accuracy: 27.27734077645268\n",
            "Iteration: 6000. Loss: 1.9684134721755981. Accuracy: 22.583100735853844\n",
            "Iteration: 6500. Loss: 2.142596483230591. Accuracy: 17.45749809692971\n",
            "Iteration: 7000. Loss: 1.8472301959991455. Accuracy: 36.209083988835324\n",
            "Iteration: 7500. Loss: 1.9264297485351562. Accuracy: 24.94290789139812\n",
            "Iteration: 8000. Loss: 2.2445263862609863. Accuracy: 13.905100228368434\n",
            "Iteration: 8500. Loss: 1.936083436012268. Accuracy: 32.63131184978432\n",
            "Iteration: 9000. Loss: 1.8381779193878174. Accuracy: 39.17787363613296\n",
            "Iteration: 9500. Loss: 1.773193120956421. Accuracy: 44.58259325044405\n",
            "Iteration: 10000. Loss: 1.6100555658340454. Accuracy: 30.93123572697285\n",
            "Iteration: 10500. Loss: 1.980441927909851. Accuracy: 37.93453438213651\n",
            "Iteration: 11000. Loss: 1.8833562135696411. Accuracy: 37.83303730017762\n",
            "Iteration: 11500. Loss: 1.7753928899765015. Accuracy: 29.07891398122304\n",
            "Iteration: 12000. Loss: 1.8900066614151. Accuracy: 36.97031210352702\n",
            "Iteration: 12500. Loss: 1.393865704536438. Accuracy: 49.86044151230652\n",
            "Iteration: 13000. Loss: 1.9567986726760864. Accuracy: 19.106825678761737\n",
            "Iteration: 13500. Loss: 1.4603521823883057. Accuracy: 43.11088556204009\n",
            "Iteration: 14000. Loss: 1.2357290983200073. Accuracy: 55.39203247906622\n",
            "Iteration: 14500. Loss: 1.666810393333435. Accuracy: 34.838873382390254\n",
            "Iteration: 15000. Loss: 2.1372110843658447. Accuracy: 28.393808678000507\n",
            "Iteration: 15500. Loss: 1.276436686515808. Accuracy: 50.90078660238518\n",
            "Iteration: 16000. Loss: 1.6019543409347534. Accuracy: 51.12915503679269\n",
            "Iteration: 16500. Loss: 1.0682659149169922. Accuracy: 59.0459274295864\n",
            "Iteration: 17000. Loss: 1.4210039377212524. Accuracy: 48.56635371733063\n",
            "Iteration: 17500. Loss: 1.1436232328414917. Accuracy: 50.49479827454961\n",
            "Iteration: 18000. Loss: 1.4947330951690674. Accuracy: 60.31464095407257\n",
            "Iteration: 18500. Loss: 1.5035998821258545. Accuracy: 47.55138289774169\n",
            "Iteration: 19000. Loss: 1.1441820859909058. Accuracy: 63.258056330880486\n",
            "Iteration: 19500. Loss: 1.060094952583313. Accuracy: 61.07586906876427\n",
            "Iteration: 20000. Loss: 0.73185133934021. Accuracy: 61.35498604415123\n",
            "Iteration: 20500. Loss: 0.6876707673072815. Accuracy: 63.20730778990104\n",
            "Iteration: 21000. Loss: 1.158198595046997. Accuracy: 65.44024359299671\n",
            "Iteration: 21500. Loss: 1.0785644054412842. Accuracy: 59.35041867546308\n",
            "Iteration: 22000. Loss: 0.7499229907989502. Accuracy: 72.84953057599594\n",
            "Iteration: 22500. Loss: 0.7597756385803223. Accuracy: 74.7526008627252\n",
            "Iteration: 23000. Loss: 0.8378127813339233. Accuracy: 72.01217964983506\n",
            "Iteration: 23500. Loss: 0.6770777702331543. Accuracy: 71.50469424004059\n",
            "Iteration: 24000. Loss: 0.567518949508667. Accuracy: 70.61659477290029\n",
            "Iteration: 24500. Loss: 0.5033549070358276. Accuracy: 79.39609236234459\n",
            "Iteration: 25000. Loss: 0.9898394346237183. Accuracy: 72.41816797767065\n",
            "Iteration: 25500. Loss: 1.4257739782333374. Accuracy: 55.4681552905354\n",
            "Iteration: 26000. Loss: 0.663434624671936. Accuracy: 74.60035523978685\n",
            "Iteration: 26500. Loss: 0.74721759557724. Accuracy: 66.07460035523978\n",
            "Iteration: 27000. Loss: 0.5833913087844849. Accuracy: 80.66480588683075\n",
            "Iteration: 27500. Loss: 0.747525155544281. Accuracy: 74.34661253488962\n",
            "Iteration: 28000. Loss: 0.5360528230667114. Accuracy: 81.1469170261355\n",
            "Iteration: 28500. Loss: 0.31561559438705444. Accuracy: 83.50672418167977\n",
            "Iteration: 29000. Loss: 0.4118330776691437. Accuracy: 82.69474752600863\n",
            "Iteration: 29500. Loss: 0.46170586347579956. Accuracy: 80.79167723927937\n",
            "Iteration: 30000. Loss: 0.4411779046058655. Accuracy: 81.95889368180664\n",
            "Iteration: 30500. Loss: 0.6326714158058167. Accuracy: 72.59578787109871\n",
            "Iteration: 31000. Loss: 0.29843050241470337. Accuracy: 84.62319208322761\n",
            "Iteration: 31500. Loss: 0.5294741988182068. Accuracy: 75.20933773154022\n",
            "Iteration: 32000. Loss: 0.890195906162262. Accuracy: 71.834559756407\n",
            "Iteration: 32500. Loss: 0.36006882786750793. Accuracy: 84.31870083735093\n",
            "Iteration: 33000. Loss: 0.3017851412296295. Accuracy: 85.94265414869322\n",
            "Iteration: 33500. Loss: 0.46911200881004333. Accuracy: 81.32453691956356\n",
            "Iteration: 34000. Loss: 0.21239323914051056. Accuracy: 85.79040852575488\n",
            "Iteration: 34500. Loss: 0.31546664237976074. Accuracy: 87.1098705912205\n",
            "Iteration: 35000. Loss: 0.11580356955528259. Accuracy: 86.98299923877188\n",
            "Iteration: 35500. Loss: 0.14584596455097198. Accuracy: 87.13524486171022\n",
            "Iteration: 36000. Loss: 0.22599102556705475. Accuracy: 85.13067749302208\n",
            "Iteration: 36500. Loss: 0.20188163220882416. Accuracy: 87.66810454199442\n",
            "Iteration: 37000. Loss: 0.29886066913604736. Accuracy: 84.64856635371733\n",
            "Iteration: 37500. Loss: 0.1311250776052475. Accuracy: 87.26211621415884\n",
            "Iteration: 38000. Loss: 0.4988667964935303. Accuracy: 77.49302207561533\n",
            "Iteration: 38500. Loss: 0.40920206904411316. Accuracy: 83.37985282923115\n",
            "Iteration: 39000. Loss: 1.1708924770355225. Accuracy: 56.30550621669627\n",
            "Iteration: 39500. Loss: 0.18982765078544617. Accuracy: 86.80537934534382\n",
            "Iteration: 40000. Loss: 0.141696959733963. Accuracy: 87.36361329611773\n",
            "Iteration: 40500. Loss: 0.20371004939079285. Accuracy: 88.07409286983\n",
            "Iteration: 41000. Loss: 0.46163296699523926. Accuracy: 80.18269474752601\n",
            "Iteration: 41500. Loss: 0.26083916425704956. Accuracy: 78.7617356001015\n",
            "Iteration: 42000. Loss: 0.566898763179779. Accuracy: 77.31540218218726\n",
            "Iteration: 42500. Loss: 0.09416612982749939. Accuracy: 88.80994671403197\n",
            "Iteration: 43000. Loss: 0.08030074089765549. Accuracy: 88.91144379599086\n",
            "Iteration: 43500. Loss: 0.09533309191465378. Accuracy: 87.61735600101497\n",
            "Iteration: 44000. Loss: 0.08753780275583267. Accuracy: 88.4039583861964\n",
            "Iteration: 44500. Loss: 0.7543172240257263. Accuracy: 79.04085257548846\n",
            "Iteration: 45000. Loss: 0.1895795464515686. Accuracy: 85.1560517635118\n",
            "Iteration: 45500. Loss: 0.10396087169647217. Accuracy: 87.26211621415884\n",
            "Iteration: 46000. Loss: 0.09992460161447525. Accuracy: 88.68307536158335\n",
            "Iteration: 46500. Loss: 0.11348096281290054. Accuracy: 87.33823902562801\n",
            "Iteration: 47000. Loss: 0.19127030670642853. Accuracy: 86.93225069779244\n",
            "Iteration: 47500. Loss: 0.3604592978954315. Accuracy: 75.0824663790916\n",
            "Iteration: 48000. Loss: 0.07035569101572037. Accuracy: 87.84572443542248\n",
            "Iteration: 48500. Loss: 0.017944516614079475. Accuracy: 89.24130931235727\n",
            "Iteration: 49000. Loss: 0.020623406395316124. Accuracy: 88.78457244354225\n",
            "Iteration: 49500. Loss: 0.09508079290390015. Accuracy: 88.63232682060391\n",
            "Iteration: 50000. Loss: 0.023194504901766777. Accuracy: 88.53082973864501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF4tbXEwie-W"
      },
      "source": [
        "#Function for saving data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40yd2yOTeA6i"
      },
      "source": [
        "def sav(x,s):\n",
        "  with open(s,'wb') as f:\n",
        "    pickle.dump(x,f)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp_iSfbBidrH"
      },
      "source": [
        "#Saving data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hXOqunCd_Xi"
      },
      "source": [
        "sav(iter_,'iter_exp22')\n",
        "sav(loss_,'loss_exp22')\n",
        "sav(accuracy_,'acc_exp22')\n",
        "sav(model,'model_exp22')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYMepvYmQXmi"
      },
      "source": [
        "#Aproach 7\n",
        " <font color = 'tiffani blue'>batch size = 32 </font><br>\n",
        "num_iters = 50000<br>\n",
        "input_dim = 1 * 28 * 28 <br>\n",
        " num_hidden = 200  </font><br>\n",
        "output_dim = 10 <br>\n",
        "learning_rate = 0.1 <br>\n",
        "Activation = ReLU <br>\n",
        "Criterion = Cross Entropy <br>\n",
        "Optimizar = SGD <br>\n",
        "<font color = 'green'> Accuracy = 84.36% </font><br>\n",
        "Comment: I have reduced the batch size and the  accuracy has been decreased a little bit <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abhzESnwfOBk",
        "outputId": "77891130-fd05-4aed-c098-d6aa16bde1ff"
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "batch_size = 32\n",
        "num_iters = 50000\n",
        "input_dim = 1*28*28 # num_features = 784\n",
        "num_hidden = 200\n",
        "output_dim = 10\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle= False, batch_size=batch_size)\n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "\n",
        "        self.linear_4 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_4 = nn.ReLU()\n",
        "        \n",
        "        \n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_5 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_5 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_6 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_6 = nn.ReLU()\n",
        "        \n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_4(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_4(out)\n",
        "        \n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_5(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_5(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_6(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_6(out)\n",
        "        \n",
        "\n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "iter_ = []\n",
        "loss_ = []\n",
        "accuracy_ = []\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 1*28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 1*28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "            iter_.append(iter)\n",
        "            loss_.append(loss.item())\n",
        "            accuracy_.append(accuracy)\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500. Loss: 2.315453052520752. Accuracy: 9.819842679522964\n",
            "Iteration: 1000. Loss: 2.3062918186187744. Accuracy: 9.819842679522964\n",
            "Iteration: 1500. Loss: 2.253286123275757. Accuracy: 22.659223547323016\n",
            "Iteration: 2000. Loss: 2.4153308868408203. Accuracy: 9.79446840903324\n",
            "Iteration: 2500. Loss: 2.179927349090576. Accuracy: 12.61101243339254\n",
            "Iteration: 3000. Loss: 2.0415284633636475. Accuracy: 15.57980208069018\n",
            "Iteration: 3500. Loss: 2.2799205780029297. Accuracy: 10.048211113930474\n",
            "Iteration: 4000. Loss: 2.110630512237549. Accuracy: 20.730778990104035\n",
            "Iteration: 4500. Loss: 2.1695339679718018. Accuracy: 23.2935803095661\n",
            "Iteration: 5000. Loss: 2.1498544216156006. Accuracy: 18.117229129662523\n",
            "Iteration: 5500. Loss: 1.9397754669189453. Accuracy: 31.00735853844202\n",
            "Iteration: 6000. Loss: 2.589862823486328. Accuracy: 12.484141080943923\n",
            "Iteration: 6500. Loss: 1.9123402833938599. Accuracy: 30.27150469424004\n",
            "Iteration: 7000. Loss: 2.262211561203003. Accuracy: 21.238264399898505\n",
            "Iteration: 7500. Loss: 2.2698748111724854. Accuracy: 16.92463841664552\n",
            "Iteration: 8000. Loss: 1.9755213260650635. Accuracy: 21.36513575234712\n",
            "Iteration: 8500. Loss: 2.0795936584472656. Accuracy: 26.972849530575996\n",
            "Iteration: 9000. Loss: 1.869655966758728. Accuracy: 36.91956356254758\n",
            "Iteration: 9500. Loss: 2.5644257068634033. Accuracy: 20.52778482618625\n",
            "Iteration: 10000. Loss: 2.3054957389831543. Accuracy: 32.98655163664045\n",
            "Iteration: 10500. Loss: 1.7761071920394897. Accuracy: 42.12128901294088\n",
            "Iteration: 11000. Loss: 1.942237138748169. Accuracy: 43.51687388987567\n",
            "Iteration: 11500. Loss: 1.783172845840454. Accuracy: 41.94366911951281\n",
            "Iteration: 12000. Loss: 1.6545181274414062. Accuracy: 32.35219487439736\n",
            "Iteration: 12500. Loss: 1.6044071912765503. Accuracy: 40.37046434914996\n",
            "Iteration: 13000. Loss: 1.6261675357818604. Accuracy: 40.54808424257803\n",
            "Iteration: 13500. Loss: 1.8982325792312622. Accuracy: 37.807663029687895\n",
            "Iteration: 14000. Loss: 1.4848769903182983. Accuracy: 48.08424257802588\n",
            "Iteration: 14500. Loss: 1.4045628309249878. Accuracy: 41.639177873636136\n",
            "Iteration: 15000. Loss: 1.556251883506775. Accuracy: 38.82263384927683\n",
            "Iteration: 15500. Loss: 1.5692301988601685. Accuracy: 35.04186754630804\n",
            "Iteration: 16000. Loss: 1.2907556295394897. Accuracy: 54.96066988074093\n",
            "Iteration: 16500. Loss: 1.2269214391708374. Accuracy: 47.272265922354734\n",
            "Iteration: 17000. Loss: 1.3704516887664795. Accuracy: 54.47855874143618\n",
            "Iteration: 17500. Loss: 1.4308527708053589. Accuracy: 45.293072824156305\n",
            "Iteration: 18000. Loss: 2.0032083988189697. Accuracy: 37.45242324283177\n",
            "Iteration: 18500. Loss: 1.3275179862976074. Accuracy: 57.777213905100226\n",
            "Iteration: 19000. Loss: 1.8642245531082153. Accuracy: 37.35092616087287\n",
            "Iteration: 19500. Loss: 0.9682636857032776. Accuracy: 63.91778736361329\n",
            "Iteration: 20000. Loss: 1.0997540950775146. Accuracy: 62.16696269982238\n",
            "Iteration: 20500. Loss: 2.216864824295044. Accuracy: 43.288505455468155\n",
            "Iteration: 21000. Loss: 1.451775312423706. Accuracy: 40.29434153768079\n",
            "Iteration: 21500. Loss: 1.314528465270996. Accuracy: 66.09997462572952\n",
            "Iteration: 22000. Loss: 1.6540364027023315. Accuracy: 30.42375031717838\n",
            "Iteration: 22500. Loss: 1.2281882762908936. Accuracy: 60.771377822887594\n",
            "Iteration: 23000. Loss: 0.6092517375946045. Accuracy: 66.75970565846232\n",
            "Iteration: 23500. Loss: 0.9972915649414062. Accuracy: 68.23141334686628\n",
            "Iteration: 24000. Loss: 1.1852335929870605. Accuracy: 64.80588683075362\n",
            "Iteration: 24500. Loss: 0.7927175164222717. Accuracy: 62.44607967520934\n",
            "Iteration: 25000. Loss: 0.6301243305206299. Accuracy: 70.51509769094139\n",
            "Iteration: 25500. Loss: 1.2988556623458862. Accuracy: 69.95686374016746\n",
            "Iteration: 26000. Loss: 2.4355292320251465. Accuracy: 32.40294341537681\n",
            "Iteration: 26500. Loss: 0.7724637389183044. Accuracy: 69.82999238771885\n",
            "Iteration: 27000. Loss: 0.7074966430664062. Accuracy: 67.49555950266429\n",
            "Iteration: 27500. Loss: 0.593259334564209. Accuracy: 65.94772900279116\n",
            "Iteration: 28000. Loss: 0.6953688859939575. Accuracy: 68.81502156812992\n",
            "Iteration: 28500. Loss: 0.5172446966171265. Accuracy: 75.61532605937579\n",
            "Iteration: 29000. Loss: 0.9048978686332703. Accuracy: 74.6511037807663\n",
            "Iteration: 29500. Loss: 0.6634216904640198. Accuracy: 74.77797513321492\n",
            "Iteration: 30000. Loss: 0.5669763684272766. Accuracy: 78.30499873128647\n",
            "Iteration: 30500. Loss: 0.3605038523674011. Accuracy: 78.40649581324537\n",
            "Iteration: 31000. Loss: 0.9335494041442871. Accuracy: 68.53590459274295\n",
            "Iteration: 31500. Loss: 0.5543587803840637. Accuracy: 71.47931996955087\n",
            "Iteration: 32000. Loss: 0.6958684921264648. Accuracy: 79.44684090332403\n",
            "Iteration: 32500. Loss: 0.7984708547592163. Accuracy: 71.55544278102005\n",
            "Iteration: 33000. Loss: 0.48551973700523376. Accuracy: 80.00507485409794\n",
            "Iteration: 33500. Loss: 0.9552043080329895. Accuracy: 70.38822633849277\n",
            "Iteration: 34000. Loss: 0.4189228117465973. Accuracy: 76.55417406749557\n",
            "Iteration: 34500. Loss: 1.6350643634796143. Accuracy: 43.719868053793455\n",
            "Iteration: 35000. Loss: 0.3876044750213623. Accuracy: 78.63486424765289\n",
            "Iteration: 35500. Loss: 0.3445843458175659. Accuracy: 76.35117990357777\n",
            "Iteration: 36000. Loss: 0.5539040565490723. Accuracy: 79.1930981984268\n",
            "Iteration: 36500. Loss: 0.3690038025379181. Accuracy: 84.03958386196396\n",
            "Iteration: 37000. Loss: 0.30276525020599365. Accuracy: 82.79624460796752\n",
            "Iteration: 37500. Loss: 2.2064049243927. Accuracy: 53.33671656939863\n",
            "Iteration: 38000. Loss: 0.5201257467269897. Accuracy: 83.55747272265923\n",
            "Iteration: 38500. Loss: 0.6060344576835632. Accuracy: 84.3948236488201\n",
            "Iteration: 39000. Loss: 0.5402140617370605. Accuracy: 81.07079421466632\n",
            "Iteration: 39500. Loss: 0.3561120629310608. Accuracy: 83.30373001776199\n",
            "Iteration: 40000. Loss: 0.5863661766052246. Accuracy: 74.54960669880741\n",
            "Iteration: 40500. Loss: 0.47352251410484314. Accuracy: 84.16645521441258\n",
            "Iteration: 41000. Loss: 0.2011266052722931. Accuracy: 84.95305759959402\n",
            "Iteration: 41500. Loss: 0.04098178446292877. Accuracy: 86.90687642730272\n",
            "Iteration: 42000. Loss: 0.6074694395065308. Accuracy: 72.67191068256787\n",
            "Iteration: 42500. Loss: 0.5340447425842285. Accuracy: 85.66353717330627\n",
            "Iteration: 43000. Loss: 0.22126469016075134. Accuracy: 83.50672418167977\n",
            "Iteration: 43500. Loss: 0.2707199454307556. Accuracy: 86.1202740421213\n",
            "Iteration: 44000. Loss: 0.14395155012607574. Accuracy: 85.43516873889875\n",
            "Iteration: 44500. Loss: 0.3128056526184082. Accuracy: 82.82161887845724\n",
            "Iteration: 45000. Loss: 1.160314917564392. Accuracy: 73.48388733823903\n",
            "Iteration: 45500. Loss: 0.5622653961181641. Accuracy: 79.26922100989597\n",
            "Iteration: 46000. Loss: 0.17016710340976715. Accuracy: 83.93808678000508\n",
            "Iteration: 46500. Loss: 0.42476359009742737. Accuracy: 82.44100482111139\n",
            "Iteration: 47000. Loss: 0.2654695510864258. Accuracy: 85.94265414869322\n",
            "Iteration: 47500. Loss: 0.2679715156555176. Accuracy: 87.36361329611773\n",
            "Iteration: 48000. Loss: 0.18768814206123352. Accuracy: 87.2113676731794\n",
            "Iteration: 48500. Loss: 0.18649032711982727. Accuracy: 82.87236741943669\n",
            "Iteration: 49000. Loss: 0.826262354850769. Accuracy: 71.68231413346867\n",
            "Iteration: 49500. Loss: 0.5498960018157959. Accuracy: 84.36944937833037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXNyFCcTRF5E"
      },
      "source": [
        "#Aproach 8\n",
        " <font color = 'tiffani blue'>batch size = 128 </font><br>\n",
        "num_iters = 50000<br>\n",
        "input_dim = 1 * 28 * 28 <br>\n",
        " num_hidden = 200  </font><br>\n",
        "output_dim = 10 <br>\n",
        "learning_rate = 0.1 <br>\n",
        "Activation = ReLU <br>\n",
        "Criterion = Cross Entropy <br>\n",
        "Optimizar = SGD <br>\n",
        "<font color = 'red'> Accuracy = 10.55% </font><br>\n",
        "Comment: I have increased the batch size a lot and the accuracy has become worst. Moreover, the loss function is returning nan values.<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RG_4W-YSf5TK",
        "outputId": "ee397fdb-ddf7-4009-93d3-3d9d94c73430"
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "batch_size = 128\n",
        "num_iters = 50000\n",
        "input_dim = 1*28*28 # num_features = 784\n",
        "num_hidden = 200\n",
        "output_dim = 10\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle= False, batch_size=batch_size)\n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "\n",
        "        self.linear_4 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_4 = nn.ReLU()\n",
        "        \n",
        "        \n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_5 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_5 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_6 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_6 = nn.ReLU()\n",
        "        \n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_4(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_4(out)\n",
        "        \n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_5(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_5(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_6(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_6(out)\n",
        "        \n",
        "\n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "iter_ = []\n",
        "loss_ = []\n",
        "accuracy_ = []\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 1*28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 1*28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "            iter_.append(iter)\n",
        "            loss_.append(loss.item())\n",
        "            accuracy_.append(accuracy)\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500. Loss: 2.2974448204040527. Accuracy: 9.71834559756407\n",
            "Iteration: 1000. Loss: 2.285680055618286. Accuracy: 15.35143364628267\n",
            "Iteration: 1500. Loss: 2.23923921585083. Accuracy: 22.050241055569654\n",
            "Iteration: 2000. Loss: 2.1790060997009277. Accuracy: 16.594772900279118\n",
            "Iteration: 2500. Loss: 2.1314876079559326. Accuracy: 17.43212382643999\n",
            "Iteration: 3000. Loss: 2.240400791168213. Accuracy: 14.412585638162902\n",
            "Iteration: 3500. Loss: 2.155710458755493. Accuracy: 14.15884293326567\n",
            "Iteration: 4000. Loss: 2.0161266326904297. Accuracy: 21.162141588429332\n",
            "Iteration: 4500. Loss: 2.0235605239868164. Accuracy: 19.715808170515096\n",
            "Iteration: 5000. Loss: 2.0699820518493652. Accuracy: 24.384673940624207\n",
            "Iteration: 5500. Loss: 2.34660005569458. Accuracy: 10.60644506470439\n",
            "Iteration: 6000. Loss: 1.9266142845153809. Accuracy: 24.689165186500887\n",
            "Iteration: 6500. Loss: 1.9958410263061523. Accuracy: 29.789393554935295\n",
            "Iteration: 7000. Loss: 1.7450532913208008. Accuracy: 28.825171276325804\n",
            "Iteration: 7500. Loss: 1.9205690622329712. Accuracy: 27.657954833798527\n",
            "Iteration: 8000. Loss: 1.6760895252227783. Accuracy: 38.49276833291043\n",
            "Iteration: 8500. Loss: 1.9341107606887817. Accuracy: 36.03146409540726\n",
            "Iteration: 9000. Loss: 2.014234781265259. Accuracy: 31.97158081705151\n",
            "Iteration: 9500. Loss: 1.8189440965652466. Accuracy: 42.62877442273535\n",
            "Iteration: 10000. Loss: 1.6701871156692505. Accuracy: 23.902562801319462\n",
            "Iteration: 10500. Loss: 1.8306337594985962. Accuracy: 47.50063435676224\n",
            "Iteration: 11000. Loss: 2.142092704772949. Accuracy: 28.596802841918294\n",
            "Iteration: 11500. Loss: 1.683030128479004. Accuracy: 44.07510784064958\n",
            "Iteration: 12000. Loss: 2.5221478939056396. Accuracy: 32.42831768586653\n",
            "Iteration: 12500. Loss: 1.8332678079605103. Accuracy: 40.6242070540472\n",
            "Iteration: 13000. Loss: 1.3249433040618896. Accuracy: 46.40954072570413\n",
            "Iteration: 13500. Loss: 2.0638139247894287. Accuracy: 38.315148439482364\n",
            "Iteration: 14000. Loss: 1.382035255432129. Accuracy: 49.226084750063436\n",
            "Iteration: 14500. Loss: 1.2078437805175781. Accuracy: 53.56508500380614\n",
            "Iteration: 15000. Loss: 0.9635791182518005. Accuracy: 57.54884547069272\n",
            "Iteration: 15500. Loss: 0.9982307553291321. Accuracy: 45.216950012687136\n",
            "Iteration: 16000. Loss: 1.1180015802383423. Accuracy: 54.50393301192591\n",
            "Iteration: 16500. Loss: 1.0267174243927002. Accuracy: 55.11291550367927\n",
            "Iteration: 17000. Loss: 0.9274876117706299. Accuracy: 65.79548337985283\n",
            "Iteration: 17500. Loss: 1.3742560148239136. Accuracy: 59.223547323014465\n",
            "Iteration: 18000. Loss: 1.002994418144226. Accuracy: 60.46688657701091\n",
            "Iteration: 18500. Loss: 0.8298976421356201. Accuracy: 66.12534889621924\n",
            "Iteration: 19000. Loss: 0.6396431922912598. Accuracy: 69.24638416645521\n",
            "Iteration: 19500. Loss: 0.8140299320220947. Accuracy: 74.62572951027659\n",
            "Iteration: 20000. Loss: 0.8243729472160339. Accuracy: 75.79294595280386\n",
            "Iteration: 20500. Loss: 1.0620307922363281. Accuracy: 68.84039583861964\n",
            "Iteration: 21000. Loss: 0.7634395360946655. Accuracy: 71.60619132199949\n",
            "Iteration: 21500. Loss: 0.905642569065094. Accuracy: 67.57168231413347\n",
            "Iteration: 22000. Loss: 0.7327459454536438. Accuracy: 72.51966505962953\n",
            "Iteration: 22500. Loss: 0.5575591921806335. Accuracy: 81.62902816544025\n",
            "Iteration: 23000. Loss: 0.6817225813865662. Accuracy: 79.24384673940624\n",
            "Iteration: 23500. Loss: 0.7072785496711731. Accuracy: 76.32580563308805\n",
            "Iteration: 24000. Loss: 0.5859978199005127. Accuracy: 75.64070032986551\n",
            "Iteration: 24500. Loss: 1.1554478406906128. Accuracy: 55.747272265922355\n",
            "Iteration: 25000. Loss: 0.36492276191711426. Accuracy: 82.61862471453945\n",
            "Iteration: 25500. Loss: 1.3503718376159668. Accuracy: 63.232682060390765\n",
            "Iteration: 26000. Loss: 0.7009444236755371. Accuracy: 74.95559502664298\n",
            "Iteration: 26500. Loss: 0.390415221452713. Accuracy: 83.37985282923115\n",
            "Iteration: 27000. Loss: 0.552861750125885. Accuracy: 82.23801065719361\n",
            "Iteration: 27500. Loss: 0.201399028301239. Accuracy: 83.81121542755646\n",
            "Iteration: 28000. Loss: 0.5479209423065186. Accuracy: 60.13702106064451\n",
            "Iteration: 28500. Loss: nan. Accuracy: 10.555696523724944\n",
            "Iteration: 29000. Loss: nan. Accuracy: 10.555696523724944\n",
            "Iteration: 29500. Loss: nan. Accuracy: 10.555696523724944\n",
            "Iteration: 30000. Loss: nan. Accuracy: 10.555696523724944\n",
            "Iteration: 30500. Loss: nan. Accuracy: 10.555696523724944\n",
            "Iteration: 31000. Loss: nan. Accuracy: 10.555696523724944\n",
            "Iteration: 31500. Loss: nan. Accuracy: 10.555696523724944\n",
            "Iteration: 32000. Loss: nan. Accuracy: 10.555696523724944\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-2390356ce2a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# Forward pass to get output/logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# Calculate Loss: softmax --> cross entropy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-2390356ce2a9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m### 1st hidden layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mout\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;31m### Non-linearity in 1st hidden layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrH8tInyRvEL"
      },
      "source": [
        "#Aproach 9\n",
        " <font color = 'tiffani blue'>batch size = 64 </font><br>\n",
        "num_iters = 50000<br>\n",
        "input_dim = 1 * 28 * 28 <br>\n",
        "<font color = 'tiffani blue'> num_hidden = 300  </font><br>\n",
        "output_dim = 10 <br>\n",
        "learning_rate = 0.1 <br>\n",
        "Activation = ReLU <br>\n",
        "Criterion = Cross Entropy <br>\n",
        "Optimizar = SGD <br>\n",
        "<font color = 'green'> Accuracy = 89.06% </font><br>\n",
        "Comment: I have made the batch size 64 again and increased the hidden nodes. The accuracy is good but it has not crossed 90% yet.<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoVGr4KPgwpy",
        "outputId": "c57a6c2a-9799-4d28-9918-c0f544dc5ff3"
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "batch_size = 64\n",
        "num_iters = 50000\n",
        "input_dim = 1*28*28 # num_features = 784\n",
        "num_hidden = 300\n",
        "output_dim = 10\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle= False, batch_size=batch_size)\n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "\n",
        "        self.linear_4 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_4 = nn.ReLU()\n",
        "        \n",
        "        \n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_5 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_5 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_6 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_6 = nn.ReLU()\n",
        "        \n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_4(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_4(out)\n",
        "        \n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_5(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_5(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_6(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_6(out)\n",
        "        \n",
        "\n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "iter_ = []\n",
        "loss_ = []\n",
        "accuracy_ = []\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 1*28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 1*28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "            iter_.append(iter)\n",
        "            loss_.append(loss.item())\n",
        "            accuracy_.append(accuracy)\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500. Loss: 2.3029913902282715. Accuracy: 9.591474245115453\n",
            "Iteration: 1000. Loss: 2.290576934814453. Accuracy: 17.6604922608475\n",
            "Iteration: 1500. Loss: 2.2106099128723145. Accuracy: 13.296117736615072\n",
            "Iteration: 2000. Loss: 2.1146953105926514. Accuracy: 13.600608982491753\n",
            "Iteration: 2500. Loss: 2.2204041481018066. Accuracy: 12.712509515351433\n",
            "Iteration: 3000. Loss: 2.243915557861328. Accuracy: 9.921339761481857\n",
            "Iteration: 3500. Loss: 2.2032008171081543. Accuracy: 18.77696016239533\n",
            "Iteration: 4000. Loss: 2.043666124343872. Accuracy: 20.451662014717076\n",
            "Iteration: 4500. Loss: 2.420156955718994. Accuracy: 10.8348134991119\n",
            "Iteration: 5000. Loss: 1.9441732168197632. Accuracy: 20.78152753108348\n",
            "Iteration: 5500. Loss: 1.978839635848999. Accuracy: 21.821872621162143\n",
            "Iteration: 6000. Loss: 2.401482343673706. Accuracy: 11.342298908906368\n",
            "Iteration: 6500. Loss: 1.8496659994125366. Accuracy: 31.413346866277596\n",
            "Iteration: 7000. Loss: 1.7686654329299927. Accuracy: 35.57472722659224\n",
            "Iteration: 7500. Loss: 1.7752457857131958. Accuracy: 23.648820096422227\n",
            "Iteration: 8000. Loss: 1.9745787382125854. Accuracy: 29.96701344836336\n",
            "Iteration: 8500. Loss: 2.0566446781158447. Accuracy: 12.864755138289775\n",
            "Iteration: 9000. Loss: 1.9998173713684082. Accuracy: 26.33849276833291\n",
            "Iteration: 9500. Loss: 2.079242706298828. Accuracy: 33.82390256280132\n",
            "Iteration: 10000. Loss: 1.598425269126892. Accuracy: 39.304744988581575\n",
            "Iteration: 10500. Loss: 1.489388346672058. Accuracy: 40.97944684090332\n",
            "Iteration: 11000. Loss: 1.8087579011917114. Accuracy: 28.444557218979956\n",
            "Iteration: 11500. Loss: 1.8456507921218872. Accuracy: 27.96244607967521\n",
            "Iteration: 12000. Loss: 1.9522840976715088. Accuracy: 29.205785333671656\n",
            "Iteration: 12500. Loss: 1.7329988479614258. Accuracy: 48.2111139304745\n",
            "Iteration: 13000. Loss: 1.3721740245819092. Accuracy: 45.69906115199188\n",
            "Iteration: 13500. Loss: 1.3061772584915161. Accuracy: 51.763511799035776\n",
            "Iteration: 14000. Loss: 2.727426290512085. Accuracy: 30.246130423750316\n",
            "Iteration: 14500. Loss: 1.1737114191055298. Accuracy: 49.200710479573715\n",
            "Iteration: 15000. Loss: 1.2215882539749146. Accuracy: 57.52347120020299\n",
            "Iteration: 15500. Loss: 0.9755810499191284. Accuracy: 47.93199695508754\n",
            "Iteration: 16000. Loss: 1.415820598602295. Accuracy: 60.59375792945953\n",
            "Iteration: 16500. Loss: 1.1335481405258179. Accuracy: 47.93199695508754\n",
            "Iteration: 17000. Loss: 1.3956811428070068. Accuracy: 41.740674955595026\n",
            "Iteration: 17500. Loss: 1.4989229440689087. Accuracy: 51.205277848261865\n",
            "Iteration: 18000. Loss: 0.9095765352249146. Accuracy: 67.34331387972595\n",
            "Iteration: 18500. Loss: 1.4352587461471558. Accuracy: 59.07130170007612\n",
            "Iteration: 19000. Loss: 1.0198007822036743. Accuracy: 65.69398629789393\n",
            "Iteration: 19500. Loss: 0.7549778819084167. Accuracy: 68.0284191829485\n",
            "Iteration: 20000. Loss: 0.6462647914886475. Accuracy: 68.9165186500888\n",
            "Iteration: 20500. Loss: 1.0769450664520264. Accuracy: 64.27302715046942\n",
            "Iteration: 21000. Loss: 0.5230759978294373. Accuracy: 76.52879979700583\n",
            "Iteration: 21500. Loss: 0.6989279389381409. Accuracy: 75.0824663790916\n",
            "Iteration: 22000. Loss: 1.4288451671600342. Accuracy: 60.31464095407257\n",
            "Iteration: 22500. Loss: 0.6330941319465637. Accuracy: 71.0733316417153\n",
            "Iteration: 23000. Loss: 0.6784827709197998. Accuracy: 77.44227353463587\n",
            "Iteration: 23500. Loss: 0.4701286554336548. Accuracy: 77.67064196904339\n",
            "Iteration: 24000. Loss: 0.6392908692359924. Accuracy: 80.86780005074854\n",
            "Iteration: 24500. Loss: 0.33470064401626587. Accuracy: 80.3856889114438\n",
            "Iteration: 25000. Loss: 0.6010005474090576. Accuracy: 74.70185232174575\n",
            "Iteration: 25500. Loss: 0.5069437026977539. Accuracy: 74.27048972342045\n",
            "Iteration: 26000. Loss: 0.3241758644580841. Accuracy: 79.57371225577265\n",
            "Iteration: 26500. Loss: 0.4873752295970917. Accuracy: 76.35117990357777\n",
            "Iteration: 27000. Loss: 0.27419549226760864. Accuracy: 84.87693478812484\n",
            "Iteration: 27500. Loss: 0.26709750294685364. Accuracy: 85.33367165693986\n",
            "Iteration: 28000. Loss: 0.24224601686000824. Accuracy: 81.47678254250191\n",
            "Iteration: 28500. Loss: 0.15652622282505035. Accuracy: 85.18142603400152\n",
            "Iteration: 29000. Loss: 0.229216068983078. Accuracy: 85.10530322253236\n",
            "Iteration: 29500. Loss: 0.23326626420021057. Accuracy: 80.86780005074854\n",
            "Iteration: 30000. Loss: 0.2690330147743225. Accuracy: 83.12611012433392\n",
            "Iteration: 30500. Loss: 0.2525577247142792. Accuracy: 83.45597564070033\n",
            "Iteration: 31000. Loss: 0.24062471091747284. Accuracy: 86.04415123065212\n",
            "Iteration: 31500. Loss: 0.2646060287952423. Accuracy: 86.24714539456991\n",
            "Iteration: 32000. Loss: 0.13581091165542603. Accuracy: 86.70388226338493\n",
            "Iteration: 32500. Loss: 0.25492963194847107. Accuracy: 85.48591727987821\n",
            "Iteration: 33000. Loss: 0.17231306433677673. Accuracy: 86.95762496828216\n",
            "Iteration: 33500. Loss: 0.18384715914726257. Accuracy: 84.80081197665567\n",
            "Iteration: 34000. Loss: 0.5755106806755066. Accuracy: 85.0545546815529\n",
            "Iteration: 34500. Loss: 0.2872416377067566. Accuracy: 82.97386450139558\n",
            "Iteration: 35000. Loss: 0.3332775831222534. Accuracy: 78.91398122303984\n",
            "Iteration: 35500. Loss: 0.17227868735790253. Accuracy: 82.21263638670388\n",
            "Iteration: 36000. Loss: 0.1613243967294693. Accuracy: 84.57244354224817\n",
            "Iteration: 36500. Loss: 0.20119744539260864. Accuracy: 84.87693478812484\n",
            "Iteration: 37000. Loss: 0.16836893558502197. Accuracy: 86.1202740421213\n",
            "Iteration: 37500. Loss: 0.2061876505613327. Accuracy: 82.111139304745\n",
            "Iteration: 38000. Loss: 0.17826570570468903. Accuracy: 81.75589951788886\n",
            "Iteration: 38500. Loss: 0.1548437774181366. Accuracy: 85.63816290281655\n",
            "Iteration: 39000. Loss: 0.28385308384895325. Accuracy: 86.70388226338493\n",
            "Iteration: 39500. Loss: 0.08948071300983429. Accuracy: 87.5412331895458\n",
            "Iteration: 40000. Loss: 0.08749964088201523. Accuracy: 86.75463080436437\n",
            "Iteration: 40500. Loss: 0.5248422026634216. Accuracy: 81.09616848515606\n",
            "Iteration: 41000. Loss: 0.05103766545653343. Accuracy: 88.80994671403197\n",
            "Iteration: 41500. Loss: 0.18236809968948364. Accuracy: 85.0545546815529\n",
            "Iteration: 42000. Loss: 0.04534922167658806. Accuracy: 88.48008119766557\n",
            "Iteration: 42500. Loss: 0.04081306979060173. Accuracy: 87.26211621415884\n",
            "Iteration: 43000. Loss: 0.1825065314769745. Accuracy: 87.31286475513829\n",
            "Iteration: 43500. Loss: 0.12394925206899643. Accuracy: 84.95305759959402\n",
            "Iteration: 44000. Loss: 0.04441206157207489. Accuracy: 87.94722151738138\n",
            "Iteration: 44500. Loss: 0.04202215373516083. Accuracy: 89.44430347627505\n",
            "Iteration: 45000. Loss: 0.009039279073476791. Accuracy: 89.13981223039838\n",
            "Iteration: 45500. Loss: 0.026315107941627502. Accuracy: 88.65770109109363\n",
            "Iteration: 46000. Loss: 0.02268839254975319. Accuracy: 89.11443795990866\n",
            "Iteration: 46500. Loss: 0.06821849942207336. Accuracy: 88.48008119766557\n",
            "Iteration: 47000. Loss: 0.057711392641067505. Accuracy: 88.78457244354225\n",
            "Iteration: 47500. Loss: 0.026827068999409676. Accuracy: 88.78457244354225\n",
            "Iteration: 48000. Loss: 0.009634568355977535. Accuracy: 89.11443795990866\n",
            "Iteration: 48500. Loss: 0.017505478113889694. Accuracy: 88.80994671403197\n",
            "Iteration: 49000. Loss: 0.8389192223548889. Accuracy: 82.03501649327582\n",
            "Iteration: 49500. Loss: 0.28022539615631104. Accuracy: 83.81121542755646\n",
            "Iteration: 50000. Loss: 0.003263136837631464. Accuracy: 89.0636894189292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibBHRIpJSsi0"
      },
      "source": [
        "#Aproach 10\n",
        "batch size = 64 <br>\n",
        "num_iters = 50000<br>\n",
        "input_dim = 1 * 28 * 28 <br>\n",
        "<font color = 'tiffani blue'> num_hidden = 500  </font><br>\n",
        "output_dim = 10 <br>\n",
        "learning_rate = 0.1 <br>\n",
        "Activation = ReLU <br>\n",
        "Criterion = Cross Entropy <br>\n",
        "Optimizar = SGD <br>\n",
        "<font color = 'green'> Accuracy = 90.23% </font><br>\n",
        "Comment: I have made the hidden nodes equal to 500 and finaly the accuracy has become 90%<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRqRN2ZHiVpL",
        "outputId": "7c0d6962-4e7c-4e26-892b-9a5fcb7b2c1f"
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "batch_size = 64\n",
        "num_iters = 50000\n",
        "input_dim = 1*28*28 # num_features = 784\n",
        "num_hidden = 500\n",
        "output_dim = 10\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle= False, batch_size=batch_size)\n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "\n",
        "        self.linear_4 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_4 = nn.ReLU()\n",
        "        \n",
        "        \n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_5 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_5 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_6 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_6 = nn.ReLU()\n",
        "        \n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_4(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_4(out)\n",
        "        \n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_5(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_5(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_6(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_6(out)\n",
        "        \n",
        "\n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "iter_ = []\n",
        "loss_ = []\n",
        "accuracy_ = []\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 1*28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 1*28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "            iter_.append(iter)\n",
        "            loss_.append(loss.item())\n",
        "            accuracy_.append(accuracy)\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500. Loss: 2.31158185005188. Accuracy: 10.352702359807155\n",
            "Iteration: 1000. Loss: 2.2788424491882324. Accuracy: 14.006597310327328\n",
            "Iteration: 1500. Loss: 2.2318954467773438. Accuracy: 16.873889875666073\n",
            "Iteration: 2000. Loss: 2.30698823928833. Accuracy: 9.87059122050241\n",
            "Iteration: 2500. Loss: 2.1633262634277344. Accuracy: 17.102258310073584\n",
            "Iteration: 3000. Loss: 2.1641061305999756. Accuracy: 15.935041867546309\n",
            "Iteration: 3500. Loss: 1.9975718259811401. Accuracy: 23.775691448870845\n",
            "Iteration: 4000. Loss: 1.980688452720642. Accuracy: 24.435422481603656\n",
            "Iteration: 4500. Loss: 2.1889350414276123. Accuracy: 20.451662014717076\n",
            "Iteration: 5000. Loss: 1.935383677482605. Accuracy: 29.53565085003806\n",
            "Iteration: 5500. Loss: 2.1198651790618896. Accuracy: 26.211621415884295\n",
            "Iteration: 6000. Loss: 2.377007246017456. Accuracy: 33.874651103780764\n",
            "Iteration: 6500. Loss: 1.9361240863800049. Accuracy: 25.095153514336463\n",
            "Iteration: 7000. Loss: 1.749233365058899. Accuracy: 31.387972595787872\n",
            "Iteration: 7500. Loss: 1.77293860912323. Accuracy: 26.49073839127125\n",
            "Iteration: 8000. Loss: 1.8183890581130981. Accuracy: 34.05227099720883\n",
            "Iteration: 8500. Loss: 1.7658101320266724. Accuracy: 33.51941131692464\n",
            "Iteration: 9000. Loss: 2.0164988040924072. Accuracy: 14.920071047957371\n",
            "Iteration: 9500. Loss: 1.464815378189087. Accuracy: 44.15123065211875\n",
            "Iteration: 10000. Loss: 1.3751351833343506. Accuracy: 47.272265922354734\n",
            "Iteration: 10500. Loss: 1.6498504877090454. Accuracy: 48.92159350418675\n",
            "Iteration: 11000. Loss: 1.5537010431289673. Accuracy: 56.00101497081959\n",
            "Iteration: 11500. Loss: 1.5108366012573242. Accuracy: 45.90205531590967\n",
            "Iteration: 12000. Loss: 1.2176742553710938. Accuracy: 57.218979954326315\n",
            "Iteration: 12500. Loss: 2.525519609451294. Accuracy: 26.592235473230144\n",
            "Iteration: 13000. Loss: 1.3455684185028076. Accuracy: 53.742704897234205\n",
            "Iteration: 13500. Loss: 1.4071767330169678. Accuracy: 55.92489215935042\n",
            "Iteration: 14000. Loss: 1.3196247816085815. Accuracy: 58.53844201979193\n",
            "Iteration: 14500. Loss: 0.9536185264587402. Accuracy: 60.796752093377314\n",
            "Iteration: 15000. Loss: 1.198140263557434. Accuracy: 50.97690941385435\n",
            "Iteration: 15500. Loss: 1.1165214776992798. Accuracy: 49.70819588936818\n",
            "Iteration: 16000. Loss: 1.407459020614624. Accuracy: 66.70895711748287\n",
            "Iteration: 16500. Loss: 1.3920986652374268. Accuracy: 41.639177873636136\n",
            "Iteration: 17000. Loss: 1.068192481994629. Accuracy: 62.217711240801826\n",
            "Iteration: 17500. Loss: 1.1473139524459839. Accuracy: 60.771377822887594\n",
            "Iteration: 18000. Loss: 1.0272585153579712. Accuracy: 65.69398629789393\n",
            "Iteration: 18500. Loss: 1.1710647344589233. Accuracy: 49.96193859426541\n",
            "Iteration: 19000. Loss: 0.8256001472473145. Accuracy: 76.32580563308805\n",
            "Iteration: 19500. Loss: 1.8752042055130005. Accuracy: 55.18903831514844\n",
            "Iteration: 20000. Loss: 0.5689778923988342. Accuracy: 78.2288759198173\n",
            "Iteration: 20500. Loss: 0.7701503038406372. Accuracy: 74.77797513321492\n",
            "Iteration: 21000. Loss: 0.5592726469039917. Accuracy: 80.69018015732048\n",
            "Iteration: 21500. Loss: 0.8150100111961365. Accuracy: 64.57751839634611\n",
            "Iteration: 22000. Loss: 0.47238853573799133. Accuracy: 72.0375539203248\n",
            "Iteration: 22500. Loss: 0.642326295375824. Accuracy: 82.0096422227861\n",
            "Iteration: 23000. Loss: 0.865342915058136. Accuracy: 74.219741182441\n",
            "Iteration: 23500. Loss: 0.4852030277252197. Accuracy: 78.55874143618371\n",
            "Iteration: 24000. Loss: 0.610130250453949. Accuracy: 75.53920324790663\n",
            "Iteration: 24500. Loss: 0.9094181060791016. Accuracy: 69.95686374016746\n",
            "Iteration: 25000. Loss: 0.4386025369167328. Accuracy: 75.10784064958132\n",
            "Iteration: 25500. Loss: 0.3781670033931732. Accuracy: 80.69018015732048\n",
            "Iteration: 26000. Loss: 0.6478241682052612. Accuracy: 78.96472976401928\n",
            "Iteration: 26500. Loss: 0.8588032126426697. Accuracy: 80.48718599340269\n",
            "Iteration: 27000. Loss: 0.2944333851337433. Accuracy: 83.73509261608729\n",
            "Iteration: 27500. Loss: 0.21636202931404114. Accuracy: 85.63816290281655\n",
            "Iteration: 28000. Loss: 0.2949453294277191. Accuracy: 85.61278863232683\n",
            "Iteration: 28500. Loss: 0.3250696659088135. Accuracy: 84.01420959147424\n",
            "Iteration: 29000. Loss: 0.28310608863830566. Accuracy: 87.0083735092616\n",
            "Iteration: 29500. Loss: 0.16287259757518768. Accuracy: 87.18599340268968\n",
            "Iteration: 30000. Loss: 0.1650126576423645. Accuracy: 87.89647297640192\n",
            "Iteration: 30500. Loss: 0.2020713984966278. Accuracy: 87.2113676731794\n",
            "Iteration: 31000. Loss: 0.22185254096984863. Accuracy: 87.03374777975134\n",
            "Iteration: 31500. Loss: 0.20614036917686462. Accuracy: 87.51585891905607\n",
            "Iteration: 32000. Loss: 0.14432291686534882. Accuracy: 85.76503425526516\n",
            "Iteration: 32500. Loss: 0.1071043610572815. Accuracy: 87.31286475513829\n",
            "Iteration: 33000. Loss: 0.12454481422901154. Accuracy: 87.18599340268968\n",
            "Iteration: 33500. Loss: 0.21060383319854736. Accuracy: 85.51129155036793\n",
            "Iteration: 34000. Loss: 0.16352877020835876. Accuracy: 86.7800050748541\n",
            "Iteration: 34500. Loss: 2.1924734115600586. Accuracy: 26.871352448617102\n",
            "Iteration: 35000. Loss: 0.24277901649475098. Accuracy: 86.29789393554935\n",
            "Iteration: 35500. Loss: 0.1912783533334732. Accuracy: 88.07409286983\n",
            "Iteration: 36000. Loss: 0.18109768629074097. Accuracy: 87.92184724689166\n",
            "Iteration: 36500. Loss: 0.8229323625564575. Accuracy: 79.92895204262878\n",
            "Iteration: 37000. Loss: 0.23726308345794678. Accuracy: 85.10530322253236\n",
            "Iteration: 37500. Loss: 0.19439834356307983. Accuracy: 80.43643745242325\n",
            "Iteration: 38000. Loss: 0.27247288823127747. Accuracy: 82.18726211621416\n",
            "Iteration: 38500. Loss: 0.07860369980335236. Accuracy: 88.48008119766557\n",
            "Iteration: 39000. Loss: 0.1437234729528427. Accuracy: 87.94722151738138\n",
            "Iteration: 39500. Loss: 0.0725313201546669. Accuracy: 88.22633849276833\n",
            "Iteration: 40000. Loss: 0.06911104172468185. Accuracy: 87.1098705912205\n",
            "Iteration: 40500. Loss: 0.0993703082203865. Accuracy: 86.60238518142603\n",
            "Iteration: 41000. Loss: 0.5097048282623291. Accuracy: 76.30043136259833\n",
            "Iteration: 41500. Loss: 0.10365629941225052. Accuracy: 86.57701091093631\n",
            "Iteration: 42000. Loss: 0.027115391567349434. Accuracy: 88.55620400913473\n",
            "Iteration: 42500. Loss: 0.03258570283651352. Accuracy: 89.49505201725451\n",
            "Iteration: 43000. Loss: 0.013813326135277748. Accuracy: 89.34280639431617\n",
            "Iteration: 43500. Loss: 0.08940061181783676. Accuracy: 86.72925653387465\n",
            "Iteration: 44000. Loss: 0.047094520181417465. Accuracy: 88.37858411570667\n",
            "Iteration: 44500. Loss: 0.01443561166524887. Accuracy: 88.7338239025628\n",
            "Iteration: 45000. Loss: 0.6266366839408875. Accuracy: 74.90484648566354\n",
            "Iteration: 45500. Loss: 0.05751460790634155. Accuracy: 89.01294087794976\n",
            "Iteration: 46000. Loss: 0.007993364706635475. Accuracy: 89.90104034509008\n",
            "Iteration: 46500. Loss: 0.03729132190346718. Accuracy: 88.53082973864501\n",
            "Iteration: 47000. Loss: 0.0056356266140937805. Accuracy: 90.40852575488455\n",
            "Iteration: 47500. Loss: 0.015829753130674362. Accuracy: 90.00253742704898\n",
            "Iteration: 48000. Loss: 0.0021695843897759914. Accuracy: 90.63689418929206\n",
            "Iteration: 48500. Loss: 0.00961365457624197. Accuracy: 89.24130931235727\n",
            "Iteration: 49000. Loss: 0.012310930527746677. Accuracy: 88.93681806648058\n",
            "Iteration: 49500. Loss: 0.0010604738490656018. Accuracy: 90.05328596802842\n",
            "Iteration: 50000. Loss: 0.0006770900217816234. Accuracy: 90.23090586145648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJxEHA3LjQde"
      },
      "source": [
        "#Saving data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm2gweUQlecC"
      },
      "source": [
        "sav(iter_,'iter_exp23')\n",
        "sav(loss_,'loss_exp23')\n",
        "sav(accuracy_,'acc_exp23')\n",
        "sav(model,'model_exp23')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkAE2EC4TZIq"
      },
      "source": [
        "#Aproach 11\n",
        "batch size = 64 <br>\n",
        "<font color = 'tiffani blue'> num_iters = 90000 </font><br>\n",
        "input_dim = 1 * 28 * 28 <br>\n",
        "num_hidden = 500  <br>\n",
        "output_dim = 10 <br>\n",
        "learning_rate = 0.1 <br>\n",
        "Activation = ReLU <br>\n",
        "Criterion = Cross Entropy <br>\n",
        "Optimizar = SGD <br>\n",
        "<font color = 'green'> Accuracy = 90.58% </font><br>\n",
        "Comment: I have increased the number of iteration but the accuracy remains the same.<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SYIeiUsJmJkq",
        "outputId": "ab512209-140e-4ed3-f266-33541de4b537"
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "batch_size = 64\n",
        "num_iters = 90000\n",
        "input_dim = 1*28*28 # num_features = 784\n",
        "num_hidden = 500\n",
        "output_dim = 10\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle= False, batch_size=batch_size)\n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "\n",
        "        self.linear_4 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_4 = nn.ReLU()\n",
        "        \n",
        "        \n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_5 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_5 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_6 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_6 = nn.ReLU()\n",
        "        \n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_4(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_4(out)\n",
        "        \n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_5(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_5(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_6(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_6(out)\n",
        "        \n",
        "\n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "iter_ = []\n",
        "loss_ = []\n",
        "accuracy_ = []\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 1*28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 1*28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "            iter_.append(iter)\n",
        "            loss_.append(loss.item())\n",
        "            accuracy_.append(accuracy)\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500. Loss: 2.302003860473633. Accuracy: 10.301953818827709\n",
            "Iteration: 1000. Loss: 2.2807137966156006. Accuracy: 17.330626744481098\n",
            "Iteration: 1500. Loss: 2.2042603492736816. Accuracy: 10.555696523724944\n",
            "Iteration: 2000. Loss: 2.31062650680542. Accuracy: 13.625983252981477\n",
            "Iteration: 2500. Loss: 2.2349321842193604. Accuracy: 12.179649835067242\n",
            "Iteration: 3000. Loss: 2.155787467956543. Accuracy: 16.290281654402435\n",
            "Iteration: 3500. Loss: 2.218829870223999. Accuracy: 18.573965998477544\n",
            "Iteration: 4000. Loss: 1.9955153465270996. Accuracy: 28.77442273534636\n",
            "Iteration: 4500. Loss: 2.119797945022583. Accuracy: 27.50570921086019\n",
            "Iteration: 5000. Loss: 1.9499543905258179. Accuracy: 28.013194620654655\n",
            "Iteration: 5500. Loss: 2.0392327308654785. Accuracy: 28.596802841918294\n",
            "Iteration: 6000. Loss: 1.8973468542099. Accuracy: 20.98452169500127\n",
            "Iteration: 6500. Loss: 1.8630198240280151. Accuracy: 37.04643491499619\n",
            "Iteration: 7000. Loss: 2.028902292251587. Accuracy: 25.95787871098706\n",
            "Iteration: 7500. Loss: 2.453139066696167. Accuracy: 15.57980208069018\n",
            "Iteration: 8000. Loss: 1.7159814834594727. Accuracy: 31.032732808931744\n",
            "Iteration: 8500. Loss: 2.484886646270752. Accuracy: 12.408018269474752\n",
            "Iteration: 9000. Loss: 1.7144839763641357. Accuracy: 39.304744988581575\n",
            "Iteration: 9500. Loss: 1.677270531654358. Accuracy: 21.56812991626491\n",
            "Iteration: 10000. Loss: 1.4076335430145264. Accuracy: 26.896726719106827\n",
            "Iteration: 10500. Loss: 1.8511570692062378. Accuracy: 26.99822380106572\n",
            "Iteration: 11000. Loss: 1.4960793256759644. Accuracy: 35.65085003806141\n",
            "Iteration: 11500. Loss: 1.2067242860794067. Accuracy: 52.01725450393301\n",
            "Iteration: 12000. Loss: 1.483441710472107. Accuracy: 51.230652118751586\n",
            "Iteration: 12500. Loss: 2.699125051498413. Accuracy: 29.713270743466126\n",
            "Iteration: 13000. Loss: 1.3557593822479248. Accuracy: 43.212382643998986\n",
            "Iteration: 13500. Loss: 1.4911667108535767. Accuracy: 47.37376300431362\n",
            "Iteration: 14000. Loss: 1.8998510837554932. Accuracy: 39.203247906622686\n",
            "Iteration: 14500. Loss: 1.2898340225219727. Accuracy: 46.20654656178635\n",
            "Iteration: 15000. Loss: 1.28810715675354. Accuracy: 63.790916011164676\n",
            "Iteration: 15500. Loss: 1.3805583715438843. Accuracy: 37.55392032479066\n",
            "Iteration: 16000. Loss: 1.424664855003357. Accuracy: 51.96650596295356\n",
            "Iteration: 16500. Loss: 1.225426435470581. Accuracy: 56.178634864247655\n",
            "Iteration: 17000. Loss: 1.46651291847229. Accuracy: 62.04009134737376\n",
            "Iteration: 17500. Loss: 0.7180060148239136. Accuracy: 66.9626998223801\n",
            "Iteration: 18000. Loss: 1.2085932493209839. Accuracy: 50.44404973357016\n",
            "Iteration: 18500. Loss: 1.1315809488296509. Accuracy: 53.79345343821365\n",
            "Iteration: 19000. Loss: 0.8645238280296326. Accuracy: 65.38949505201725\n",
            "Iteration: 19500. Loss: 1.377901554107666. Accuracy: 66.93732555189038\n",
            "Iteration: 20000. Loss: 0.5333775877952576. Accuracy: 76.78254250190307\n",
            "Iteration: 20500. Loss: 0.6061796545982361. Accuracy: 76.0466886577011\n",
            "Iteration: 21000. Loss: 0.9635292291641235. Accuracy: 66.91195128140066\n",
            "Iteration: 21500. Loss: 0.647642970085144. Accuracy: 55.315909667597055\n",
            "Iteration: 22000. Loss: 0.5643860697746277. Accuracy: 81.50215681299163\n",
            "Iteration: 22500. Loss: 0.5746195316314697. Accuracy: 81.83202232935803\n",
            "Iteration: 23000. Loss: 0.37874913215637207. Accuracy: 80.69018015732048\n",
            "Iteration: 23500. Loss: 1.0294475555419922. Accuracy: 65.9731032732809\n",
            "Iteration: 24000. Loss: 0.32796940207481384. Accuracy: 81.02004567368688\n",
            "Iteration: 24500. Loss: 0.369963139295578. Accuracy: 83.35447855874143\n",
            "Iteration: 25000. Loss: 0.26745325326919556. Accuracy: 86.19639685359046\n",
            "Iteration: 25500. Loss: 0.1844167411327362. Accuracy: 83.73509261608729\n",
            "Iteration: 26000. Loss: 0.3388931453227997. Accuracy: 85.40979446840903\n",
            "Iteration: 26500. Loss: 0.38859620690345764. Accuracy: 82.6693732555189\n",
            "Iteration: 27000. Loss: 0.43313246965408325. Accuracy: 70.92108601877696\n",
            "Iteration: 27500. Loss: 0.28015947341918945. Accuracy: 85.99340268967268\n",
            "Iteration: 28000. Loss: 0.14210517704486847. Accuracy: 87.69347881248414\n",
            "Iteration: 28500. Loss: 0.21180325746536255. Accuracy: 87.13524486171022\n",
            "Iteration: 29000. Loss: 0.1889098584651947. Accuracy: 87.03374777975134\n",
            "Iteration: 29500. Loss: 0.2393147498369217. Accuracy: 86.70388226338493\n",
            "Iteration: 30000. Loss: 0.4684692621231079. Accuracy: 82.21263638670388\n",
            "Iteration: 30500. Loss: 0.1798582524061203. Accuracy: 88.63232682060391\n",
            "Iteration: 31000. Loss: 0.1297837197780609. Accuracy: 87.94722151738138\n",
            "Iteration: 31500. Loss: 0.35629844665527344. Accuracy: 79.62446079675209\n",
            "Iteration: 32000. Loss: 0.22525769472122192. Accuracy: 87.49048464856635\n",
            "Iteration: 32500. Loss: 0.14811192452907562. Accuracy: 87.23674194366912\n",
            "Iteration: 33000. Loss: 0.12682819366455078. Accuracy: 87.56660746003553\n",
            "Iteration: 33500. Loss: 0.14234185218811035. Accuracy: 87.69347881248414\n",
            "Iteration: 34000. Loss: 0.09546193480491638. Accuracy: 89.46967774676477\n",
            "Iteration: 34500. Loss: 0.1344113051891327. Accuracy: 87.89647297640192\n",
            "Iteration: 35000. Loss: 0.4638994038105011. Accuracy: 84.59781781273789\n",
            "Iteration: 35500. Loss: 2.1315042972564697. Accuracy: 66.55671149454453\n",
            "Iteration: 36000. Loss: 0.10225334763526917. Accuracy: 89.39355493529561\n",
            "Iteration: 36500. Loss: 0.422015517950058. Accuracy: 83.8619639685359\n",
            "Iteration: 37000. Loss: 0.141209676861763. Accuracy: 89.67267191068257\n",
            "Iteration: 37500. Loss: 0.5238094329833984. Accuracy: 84.09033240294342\n",
            "Iteration: 38000. Loss: 0.11049766838550568. Accuracy: 88.7338239025628\n",
            "Iteration: 38500. Loss: 0.2086830884218216. Accuracy: 86.04415123065212\n",
            "Iteration: 39000. Loss: 0.05721846595406532. Accuracy: 88.12484141080944\n",
            "Iteration: 39500. Loss: 0.09675228595733643. Accuracy: 88.17558995178888\n",
            "Iteration: 40000. Loss: 0.06386595964431763. Accuracy: 88.65770109109363\n",
            "Iteration: 40500. Loss: 0.06088027358055115. Accuracy: 87.33823902562801\n",
            "Iteration: 41000. Loss: 0.030393652617931366. Accuracy: 89.72342045166201\n",
            "Iteration: 41500. Loss: 0.07316173613071442. Accuracy: 89.46967774676477\n",
            "Iteration: 42000. Loss: 0.198679119348526. Accuracy: 85.51129155036793\n",
            "Iteration: 42500. Loss: 0.028914455324411392. Accuracy: 90.23090586145648\n",
            "Iteration: 43000. Loss: 0.018965700641274452. Accuracy: 89.36818066480589\n",
            "Iteration: 43500. Loss: 0.03729384392499924. Accuracy: 89.72342045166201\n",
            "Iteration: 44000. Loss: 0.0021747841965407133. Accuracy: 91.1190053285968\n",
            "Iteration: 44500. Loss: 0.003031856846064329. Accuracy: 90.56077137782289\n",
            "Iteration: 45000. Loss: 0.03303900361061096. Accuracy: 89.77416899264146\n",
            "Iteration: 45500. Loss: 0.007749120704829693. Accuracy: 89.44430347627505\n",
            "Iteration: 46000. Loss: 0.0017380795907229185. Accuracy: 90.30702867292565\n",
            "Iteration: 46500. Loss: 0.01731482893228531. Accuracy: 89.49505201725451\n",
            "Iteration: 47000. Loss: 0.3741435408592224. Accuracy: 75.03171783811216\n",
            "Iteration: 47500. Loss: 0.026939960196614265. Accuracy: 87.71885308297387\n",
            "Iteration: 48000. Loss: 0.002788921818137169. Accuracy: 90.48464856635371\n",
            "Iteration: 48500. Loss: 0.02382586896419525. Accuracy: 89.8249175336209\n",
            "Iteration: 49000. Loss: 0.0012548597296699882. Accuracy: 89.77416899264146\n",
            "Iteration: 49500. Loss: 0.0006311031756922603. Accuracy: 90.30702867292565\n",
            "Iteration: 50000. Loss: 0.0007639543619006872. Accuracy: 90.51002283684345\n",
            "Iteration: 50500. Loss: 0.00015864234592299908. Accuracy: 90.45927429586399\n",
            "Iteration: 51000. Loss: 0.00010161117825191468. Accuracy: 90.45927429586399\n",
            "Iteration: 51500. Loss: 0.0001497319753980264. Accuracy: 90.58614564831261\n",
            "Iteration: 52000. Loss: 0.0001931483275257051. Accuracy: 90.53539710733317\n",
            "Iteration: 52500. Loss: 0.0007423444767482579. Accuracy: 90.66226845978179\n",
            "Iteration: 53000. Loss: 8.12021316960454e-05. Accuracy: 90.6876427302715\n",
            "Iteration: 53500. Loss: 0.00017636080156080425. Accuracy: 90.63689418929206\n",
            "Iteration: 54000. Loss: 0.00018068250210490078. Accuracy: 90.56077137782289\n",
            "Iteration: 54500. Loss: 0.00011942681157961488. Accuracy: 90.66226845978179\n",
            "Iteration: 55000. Loss: 0.0001622279523871839. Accuracy: 90.63689418929206\n",
            "Iteration: 55500. Loss: 0.0003601880162023008. Accuracy: 90.63689418929206\n",
            "Iteration: 56000. Loss: 0.0004264163435436785. Accuracy: 90.63689418929206\n",
            "Iteration: 56500. Loss: 0.00013647714513354003. Accuracy: 90.76376554174068\n",
            "Iteration: 57000. Loss: 8.849896403262392e-05. Accuracy: 90.63689418929206\n",
            "Iteration: 57500. Loss: 0.0001428307732567191. Accuracy: 90.58614564831261\n",
            "Iteration: 58000. Loss: 5.329232953954488e-05. Accuracy: 90.73839127125095\n",
            "Iteration: 58500. Loss: 0.0002458348753862083. Accuracy: 90.61151991880233\n",
            "Iteration: 59000. Loss: 2.3483966288040392e-05. Accuracy: 90.6876427302715\n",
            "Iteration: 59500. Loss: 5.2236788178561255e-05. Accuracy: 90.6876427302715\n",
            "Iteration: 60000. Loss: 9.918773866957054e-05. Accuracy: 90.63689418929206\n",
            "Iteration: 60500. Loss: 2.60762626567157e-05. Accuracy: 90.6876427302715\n",
            "Iteration: 61000. Loss: 6.641168874921277e-05. Accuracy: 90.6876427302715\n",
            "Iteration: 61500. Loss: 5.9498794144019485e-05. Accuracy: 90.6876427302715\n",
            "Iteration: 62000. Loss: 5.442865221993998e-05. Accuracy: 90.6876427302715\n",
            "Iteration: 62500. Loss: 9.562604827806354e-05. Accuracy: 90.61151991880233\n",
            "Iteration: 63000. Loss: 7.935566827654839e-05. Accuracy: 90.71301700076123\n",
            "Iteration: 63500. Loss: 5.709485412808135e-05. Accuracy: 90.6876427302715\n",
            "Iteration: 64000. Loss: 5.259752651909366e-05. Accuracy: 90.6876427302715\n",
            "Iteration: 64500. Loss: 7.495590398320928e-05. Accuracy: 90.58614564831261\n",
            "Iteration: 65000. Loss: 3.82968682970386e-05. Accuracy: 90.63689418929206\n",
            "Iteration: 65500. Loss: 6.666549597866833e-05. Accuracy: 90.56077137782289\n",
            "Iteration: 66000. Loss: 1.0910893252003007e-05. Accuracy: 90.66226845978179\n",
            "Iteration: 66500. Loss: 3.263193502789363e-05. Accuracy: 90.63689418929206\n",
            "Iteration: 67000. Loss: 4.821488255402073e-05. Accuracy: 90.58614564831261\n",
            "Iteration: 67500. Loss: 1.563730438647326e-05. Accuracy: 90.66226845978179\n",
            "Iteration: 68000. Loss: 2.1415591618278995e-05. Accuracy: 90.63689418929206\n",
            "Iteration: 68500. Loss: 8.349490963155404e-05. Accuracy: 90.63689418929206\n",
            "Iteration: 69000. Loss: 1.904387681861408e-05. Accuracy: 90.61151991880233\n",
            "Iteration: 69500. Loss: 4.081732913618907e-05. Accuracy: 90.61151991880233\n",
            "Iteration: 70000. Loss: 7.962471863720566e-05. Accuracy: 90.63689418929206\n",
            "Iteration: 70500. Loss: 1.555195194669068e-05. Accuracy: 90.66226845978179\n",
            "Iteration: 71000. Loss: 2.9465556508512236e-05. Accuracy: 90.66226845978179\n",
            "Iteration: 71500. Loss: 2.8982231015106663e-06. Accuracy: 90.66226845978179\n",
            "Iteration: 72000. Loss: 1.7347732864436693e-05. Accuracy: 90.66226845978179\n",
            "Iteration: 72500. Loss: 1.786402572179213e-05. Accuracy: 90.61151991880233\n",
            "Iteration: 73000. Loss: 4.764141340274364e-05. Accuracy: 90.63689418929206\n",
            "Iteration: 73500. Loss: 3.0226750823203474e-05. Accuracy: 90.61151991880233\n",
            "Iteration: 74000. Loss: 1.6810499801067635e-05. Accuracy: 90.63689418929206\n",
            "Iteration: 74500. Loss: 3.303176708868705e-05. Accuracy: 90.63689418929206\n",
            "Iteration: 75000. Loss: 1.4709328752360307e-05. Accuracy: 90.63689418929206\n",
            "Iteration: 75500. Loss: 3.701699461089447e-05. Accuracy: 90.58614564831261\n",
            "Iteration: 76000. Loss: 5.75241501792334e-05. Accuracy: 90.58614564831261\n",
            "Iteration: 76500. Loss: 1.2466929547372274e-05. Accuracy: 90.61151991880233\n",
            "Iteration: 77000. Loss: 5.113755469210446e-05. Accuracy: 90.61151991880233\n",
            "Iteration: 77500. Loss: 1.862014687503688e-05. Accuracy: 90.61151991880233\n",
            "Iteration: 78000. Loss: 0.00018758887017611414. Accuracy: 90.61151991880233\n",
            "Iteration: 78500. Loss: 1.8512815586291254e-05. Accuracy: 90.58614564831261\n",
            "Iteration: 79000. Loss: 1.667198921495583e-05. Accuracy: 90.58614564831261\n",
            "Iteration: 79500. Loss: 2.8674196073552594e-05. Accuracy: 90.61151991880233\n",
            "Iteration: 80000. Loss: 1.2613718354259618e-05. Accuracy: 90.58614564831261\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-aa4b58c41a9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Updating parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0miter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                     \u001b[0md_p_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"retains_grad\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leaf\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n\u001b[1;32m    981\u001b[0m                           \u001b[0;34m\"attribute won't be populated during autograd.backward(). If you indeed want the gradient \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU4wIAYKUfxr"
      },
      "source": [
        "#Aproach 12\n",
        "batch size = 64 <br>\n",
        "<font color = 'tiffani blue'> num_iters = 55000 </font><br>\n",
        "input_dim = 1 * 28 * 28 <br>\n",
        "num_hidden = 500  <br>\n",
        "output_dim = 10 <br>\n",
        "learning_rate = 0.1 <br>\n",
        "Activation = ReLU <br>\n",
        "Criterion = Cross Entropy <br>\n",
        "Optimizar = SGD <br>\n",
        "<font color = 'green'> Accuracy = 90.96% </font><br>\n",
        "Comment: I have reduced the iteration again as the accuracy remains the same but the learning rate is very high. High learning rate can cause problems in different datasets such as loss function returning nan values etc.<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdLkdmwoq2bi",
        "outputId": "bbb53442-7e3a-424e-cf75-9b3b619b6bf9"
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "batch_size = 64\n",
        "num_iters = 55000\n",
        "input_dim = 1*28*28 # num_features = 784\n",
        "num_hidden = 500\n",
        "output_dim = 10\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle= False, batch_size=batch_size)\n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "\n",
        "        self.linear_4 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_4 = nn.ReLU()\n",
        "        \n",
        "        \n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_5 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_5 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_6 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_6 = nn.ReLU()\n",
        "        \n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_4(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_4(out)\n",
        "        \n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_5(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_5(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_6(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_6(out)\n",
        "        \n",
        "\n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "iter_ = []\n",
        "loss_ = []\n",
        "accuracy_ = []\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 1*28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 1*28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "            iter_.append(iter)\n",
        "            loss_.append(loss.item())\n",
        "            accuracy_.append(accuracy)\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500. Loss: 2.303018093109131. Accuracy: 9.464602892666836\n",
            "Iteration: 1000. Loss: 2.2658426761627197. Accuracy: 12.966252220248668\n",
            "Iteration: 1500. Loss: 2.4994924068450928. Accuracy: 10.149708195889367\n",
            "Iteration: 2000. Loss: 2.4161176681518555. Accuracy: 12.433392539964476\n",
            "Iteration: 2500. Loss: 2.127840280532837. Accuracy: 15.630550621669627\n",
            "Iteration: 3000. Loss: 2.1346325874328613. Accuracy: 26.33849276833291\n",
            "Iteration: 3500. Loss: 2.238792896270752. Accuracy: 16.97538695762497\n",
            "Iteration: 4000. Loss: 2.0196211338043213. Accuracy: 24.384673940624207\n",
            "Iteration: 4500. Loss: 2.3463308811187744. Accuracy: 28.647551382897742\n",
            "Iteration: 5000. Loss: 2.0136022567749023. Accuracy: 17.609743719868053\n",
            "Iteration: 5500. Loss: 1.8230302333831787. Accuracy: 19.030702867292565\n",
            "Iteration: 6000. Loss: 2.1039929389953613. Accuracy: 38.162902816544026\n",
            "Iteration: 6500. Loss: 1.8400754928588867. Accuracy: 16.366404465871607\n",
            "Iteration: 7000. Loss: 1.6842751502990723. Accuracy: 43.06013702106065\n",
            "Iteration: 7500. Loss: 2.0485541820526123. Accuracy: 30.981984267952296\n",
            "Iteration: 8000. Loss: 1.9694929122924805. Accuracy: 37.249429078913984\n",
            "Iteration: 8500. Loss: 1.931793451309204. Accuracy: 28.85054554681553\n",
            "Iteration: 9000. Loss: 1.578020691871643. Accuracy: 45.97817812737884\n",
            "Iteration: 9500. Loss: 1.4789040088653564. Accuracy: 51.61126617609744\n",
            "Iteration: 10000. Loss: 1.8058964014053345. Accuracy: 40.218218726211624\n",
            "Iteration: 10500. Loss: 1.5289943218231201. Accuracy: 46.739406242070544\n",
            "Iteration: 11000. Loss: 1.5265541076660156. Accuracy: 49.91119005328597\n",
            "Iteration: 11500. Loss: 1.7198127508163452. Accuracy: 48.9723420451662\n",
            "Iteration: 12000. Loss: 1.7947990894317627. Accuracy: 18.523217457498095\n",
            "Iteration: 12500. Loss: 1.3213249444961548. Accuracy: 56.02638924130931\n",
            "Iteration: 13000. Loss: 1.0264772176742554. Accuracy: 60.59375792945953\n",
            "Iteration: 13500. Loss: 0.810000479221344. Accuracy: 65.08500380614058\n",
            "Iteration: 14000. Loss: 1.4647783041000366. Accuracy: 47.14539456990612\n",
            "Iteration: 14500. Loss: 1.2038463354110718. Accuracy: 64.78051256026389\n",
            "Iteration: 15000. Loss: 1.4522751569747925. Accuracy: 57.80258817558995\n",
            "Iteration: 15500. Loss: 1.4666365385055542. Accuracy: 42.47652879979701\n",
            "Iteration: 16000. Loss: 1.6500253677368164. Accuracy: 50.36792692210099\n",
            "Iteration: 16500. Loss: 1.0306994915008545. Accuracy: 66.60746003552399\n",
            "Iteration: 17000. Loss: 0.6620174646377563. Accuracy: 73.15402182187262\n",
            "Iteration: 17500. Loss: 1.0977606773376465. Accuracy: 64.65364120781527\n",
            "Iteration: 18000. Loss: 0.6953014135360718. Accuracy: 72.87490484648566\n",
            "Iteration: 18500. Loss: 0.4207606911659241. Accuracy: 77.77213905100228\n",
            "Iteration: 19000. Loss: 0.4354057013988495. Accuracy: 78.88860695255012\n",
            "Iteration: 19500. Loss: 0.7427238821983337. Accuracy: 70.10910936310582\n",
            "Iteration: 20000. Loss: 1.2324974536895752. Accuracy: 59.52803856889114\n",
            "Iteration: 20500. Loss: 0.511580228805542. Accuracy: 79.62446079675209\n",
            "Iteration: 21000. Loss: 0.6137312650680542. Accuracy: 76.30043136259833\n",
            "Iteration: 21500. Loss: 0.4575555920600891. Accuracy: 73.15402182187262\n",
            "Iteration: 22000. Loss: 0.37226152420043945. Accuracy: 76.63029687896473\n",
            "Iteration: 22500. Loss: 0.6962928175926208. Accuracy: 78.20350164932758\n",
            "Iteration: 23000. Loss: 0.44847291707992554. Accuracy: 81.37528546054301\n",
            "Iteration: 23500. Loss: 0.8589553833007812. Accuracy: 80.0558233950774\n",
            "Iteration: 24000. Loss: 0.44804683327674866. Accuracy: 81.1469170261355\n",
            "Iteration: 24500. Loss: 0.26949113607406616. Accuracy: 84.44557218979955\n",
            "Iteration: 25000. Loss: 0.17561954259872437. Accuracy: 82.21263638670388\n",
            "Iteration: 25500. Loss: 0.22513535618782043. Accuracy: 85.1560517635118\n",
            "Iteration: 26000. Loss: 0.23699809610843658. Accuracy: 87.31286475513829\n",
            "Iteration: 26500. Loss: 0.2623588442802429. Accuracy: 86.52626236995687\n",
            "Iteration: 27000. Loss: 0.1938781589269638. Accuracy: 87.18599340268968\n",
            "Iteration: 27500. Loss: 0.15166880190372467. Accuracy: 86.57701091093631\n",
            "Iteration: 28000. Loss: 0.38625475764274597. Accuracy: 73.8898756660746\n",
            "Iteration: 28500. Loss: 0.2651973366737366. Accuracy: 86.04415123065212\n",
            "Iteration: 29000. Loss: 0.17176544666290283. Accuracy: 84.57244354224817\n",
            "Iteration: 29500. Loss: 0.3088209331035614. Accuracy: 81.9081451408272\n",
            "Iteration: 30000. Loss: 0.2493806928396225. Accuracy: 85.9172798782035\n",
            "Iteration: 30500. Loss: 0.21686571836471558. Accuracy: 85.48591727987821\n",
            "Iteration: 31000. Loss: 0.7136380076408386. Accuracy: 71.45394569906115\n",
            "Iteration: 31500. Loss: 0.08665813505649567. Accuracy: 88.55620400913473\n",
            "Iteration: 32000. Loss: 0.34761953353881836. Accuracy: 83.30373001776199\n",
            "Iteration: 32500. Loss: 0.2513042390346527. Accuracy: 84.92768332910428\n",
            "Iteration: 33000. Loss: 0.07795044779777527. Accuracy: 88.80994671403197\n",
            "Iteration: 33500. Loss: 0.12030324339866638. Accuracy: 87.18599340268968\n",
            "Iteration: 34000. Loss: 0.09905184060335159. Accuracy: 87.99797005836082\n",
            "Iteration: 34500. Loss: 0.1746523231267929. Accuracy: 86.70388226338493\n",
            "Iteration: 35000. Loss: 0.7055640816688538. Accuracy: 41.740674955595026\n",
            "Iteration: 35500. Loss: 0.07820364087820053. Accuracy: 88.15021568129916\n",
            "Iteration: 36000. Loss: 0.05310743674635887. Accuracy: 87.38898756660745\n",
            "Iteration: 36500. Loss: 0.19737674295902252. Accuracy: 83.58284699314895\n",
            "Iteration: 37000. Loss: 0.026981472969055176. Accuracy: 89.41892920578533\n",
            "Iteration: 37500. Loss: 3.667990207672119. Accuracy: 29.45952803856889\n",
            "Iteration: 38000. Loss: 0.042573489248752594. Accuracy: 88.93681806648058\n",
            "Iteration: 38500. Loss: 0.04268503934144974. Accuracy: 87.71885308297387\n",
            "Iteration: 39000. Loss: 0.01571199856698513. Accuracy: 88.7338239025628\n",
            "Iteration: 39500. Loss: 0.061348602175712585. Accuracy: 88.25171276325806\n",
            "Iteration: 40000. Loss: 0.04794062674045563. Accuracy: 88.45470692717585\n",
            "Iteration: 40500. Loss: 0.12066981941461563. Accuracy: 86.75463080436437\n",
            "Iteration: 41000. Loss: 0.10727854073047638. Accuracy: 87.74422735346359\n",
            "Iteration: 41500. Loss: 0.10581915080547333. Accuracy: 86.37401674701853\n",
            "Iteration: 42000. Loss: 0.15066593885421753. Accuracy: 86.06952550114184\n",
            "Iteration: 42500. Loss: 0.0908268466591835. Accuracy: 87.82035016493276\n",
            "Iteration: 43000. Loss: 0.12087030708789825. Accuracy: 86.88150215681299\n",
            "Iteration: 43500. Loss: 0.04080374166369438. Accuracy: 88.93681806648058\n",
            "Iteration: 44000. Loss: 0.08929431438446045. Accuracy: 87.92184724689166\n",
            "Iteration: 44500. Loss: 0.05665387585759163. Accuracy: 89.72342045166201\n",
            "Iteration: 45000. Loss: 0.01868460513651371. Accuracy: 89.31743212382644\n",
            "Iteration: 45500. Loss: 0.01727435737848282. Accuracy: 89.21593504186755\n",
            "Iteration: 46000. Loss: 0.011989574879407883. Accuracy: 88.7338239025628\n",
            "Iteration: 46500. Loss: 0.0025774629320949316. Accuracy: 90.45927429586399\n",
            "Iteration: 47000. Loss: 0.0026981376577168703. Accuracy: 90.56077137782289\n",
            "Iteration: 47500. Loss: 0.0003844822058454156. Accuracy: 90.58614564831261\n",
            "Iteration: 48000. Loss: 0.0008076340891420841. Accuracy: 90.99213397614818\n",
            "Iteration: 48500. Loss: 0.00024001735437195748. Accuracy: 90.81451408272012\n",
            "Iteration: 49000. Loss: 0.0003693410544656217. Accuracy: 90.89063689418929\n",
            "Iteration: 49500. Loss: 0.0003143325448036194. Accuracy: 90.89063689418929\n",
            "Iteration: 50000. Loss: 0.0002651439281180501. Accuracy: 90.73839127125095\n",
            "Iteration: 50500. Loss: 0.00013698299881070852. Accuracy: 90.86526262369956\n",
            "Iteration: 51000. Loss: 0.0002653161354828626. Accuracy: 90.73839127125095\n",
            "Iteration: 51500. Loss: 0.0003063396434299648. Accuracy: 90.73839127125095\n",
            "Iteration: 52000. Loss: 0.00010932673467323184. Accuracy: 90.73839127125095\n",
            "Iteration: 52500. Loss: 3.7852343666600063e-05. Accuracy: 90.76376554174068\n",
            "Iteration: 53000. Loss: 4.886191527475603e-05. Accuracy: 90.76376554174068\n",
            "Iteration: 53500. Loss: 0.0002880065585486591. Accuracy: 90.94138543516874\n",
            "Iteration: 54000. Loss: 3.305844802525826e-05. Accuracy: 90.81451408272012\n",
            "Iteration: 54500. Loss: 4.852813435718417e-05. Accuracy: 90.89063689418929\n",
            "Iteration: 55000. Loss: 0.0001398788735968992. Accuracy: 90.96675970565846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR1MIWqNiU0P"
      },
      "source": [
        "#Saving data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUzWzU3HuBNQ"
      },
      "source": [
        "sav(iter_,'best_iter_exp24')\n",
        "sav(loss_,'best_loss_exp24')\n",
        "sav(accuracy_,'best_acc_exp24')\n",
        "sav(model,'best_model_exp24')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTwHI8ndVM0h"
      },
      "source": [
        "#Aproach 13\n",
        "batch size = 64 <br>\n",
        " num_iters = 55000 <br>\n",
        "input_dim = 1 * 28 * 28 <br>\n",
        "num_hidden = 500  <br>\n",
        "output_dim = 10 <br>\n",
        "<font color = 'tiffani blue'> learning_rate = 0.05</font> <br>\n",
        "Activation = ReLU <br>\n",
        "Criterion = Cross Entropy <br>\n",
        "Optimizar = SGD <br>\n",
        "<font color = 'green'> Accuracy = 91.27% </font><br>\n",
        "Comment: I have reduced the learning rate to 0.05. The accuracy looks handsome. The iteration and the learning rate are also optimal. This is my best model.<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKmyzVQB3zym",
        "outputId": "b527041c-8eaa-467a-fe70-b62144c05a61"
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "batch_size = 64\n",
        "num_iters = 55000\n",
        "input_dim = 1*28*28 # num_features = 784\n",
        "num_hidden = 500\n",
        "output_dim = 10\n",
        "learning_rate = 0.05\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle= False, batch_size=batch_size)\n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "\n",
        "        self.linear_4 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_4 = nn.ReLU()\n",
        "        \n",
        "        \n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_5 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_5 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_6 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_6 = nn.ReLU()\n",
        "        \n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_4(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_4(out)\n",
        "        \n",
        "        \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_5(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_5(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_6(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_6(out)\n",
        "        \n",
        "\n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "iter_ = []\n",
        "loss_ = []\n",
        "accuracy_ = []\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 1*28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 1*28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "            iter_.append(iter)\n",
        "            loss_.append(loss.item())\n",
        "            accuracy_.append(accuracy)\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500. Loss: 2.3025546073913574. Accuracy: 9.692971327074346\n",
            "Iteration: 1000. Loss: 2.300694227218628. Accuracy: 10.352702359807155\n",
            "Iteration: 1500. Loss: 2.303300380706787. Accuracy: 10.403450900786602\n",
            "Iteration: 2000. Loss: 2.286489248275757. Accuracy: 12.712509515351433\n",
            "Iteration: 2500. Loss: 2.2294678688049316. Accuracy: 15.123065211875158\n",
            "Iteration: 3000. Loss: 2.212395668029785. Accuracy: 20.451662014717076\n",
            "Iteration: 3500. Loss: 2.610283613204956. Accuracy: 19.918802334432886\n",
            "Iteration: 4000. Loss: 2.148385524749756. Accuracy: 17.35600101497082\n",
            "Iteration: 4500. Loss: 2.159085512161255. Accuracy: 17.838112154275564\n",
            "Iteration: 5000. Loss: 2.165522575378418. Accuracy: 17.55899517888861\n",
            "Iteration: 5500. Loss: 2.0874454975128174. Accuracy: 24.76528799797006\n",
            "Iteration: 6000. Loss: 2.1730692386627197. Accuracy: 19.182948490230906\n",
            "Iteration: 6500. Loss: 1.9378986358642578. Accuracy: 28.216188784572445\n",
            "Iteration: 7000. Loss: 2.0553455352783203. Accuracy: 32.732808931743214\n",
            "Iteration: 7500. Loss: 1.891570806503296. Accuracy: 29.434153768079167\n",
            "Iteration: 8000. Loss: 2.1332547664642334. Accuracy: 18.370971834559757\n",
            "Iteration: 8500. Loss: 1.899168610572815. Accuracy: 22.912966252220247\n",
            "Iteration: 9000. Loss: 1.9442672729492188. Accuracy: 37.274803349403705\n",
            "Iteration: 9500. Loss: 1.914443016052246. Accuracy: 29.332656686120274\n",
            "Iteration: 10000. Loss: 1.9111515283584595. Accuracy: 33.44328850545547\n",
            "Iteration: 10500. Loss: 1.9851330518722534. Accuracy: 37.883785841157064\n",
            "Iteration: 11000. Loss: 1.4602700471878052. Accuracy: 48.84547069271758\n",
            "Iteration: 11500. Loss: 1.645995020866394. Accuracy: 28.647551382897742\n",
            "Iteration: 12000. Loss: 1.957706093788147. Accuracy: 38.06140573458513\n",
            "Iteration: 12500. Loss: 1.5622479915618896. Accuracy: 36.741943669119514\n",
            "Iteration: 13000. Loss: 1.5587300062179565. Accuracy: 48.56635371733063\n",
            "Iteration: 13500. Loss: 2.5961849689483643. Accuracy: 27.708703374777976\n",
            "Iteration: 14000. Loss: 1.8324813842773438. Accuracy: 42.19741182441005\n",
            "Iteration: 14500. Loss: 1.4325754642486572. Accuracy: 60.61913219994925\n",
            "Iteration: 15000. Loss: 1.673929214477539. Accuracy: 44.43034762750571\n",
            "Iteration: 15500. Loss: 1.607041597366333. Accuracy: 55.34128393808678\n",
            "Iteration: 16000. Loss: 0.9424832463264465. Accuracy: 65.18650088809947\n",
            "Iteration: 16500. Loss: 1.654720664024353. Accuracy: 47.93199695508754\n",
            "Iteration: 17000. Loss: 0.8151199817657471. Accuracy: 68.89114437959908\n",
            "Iteration: 17500. Loss: 1.4091076850891113. Accuracy: 46.02892666835829\n",
            "Iteration: 18000. Loss: 1.1326169967651367. Accuracy: 57.320477036285205\n",
            "Iteration: 18500. Loss: 0.9476039409637451. Accuracy: 65.69398629789393\n",
            "Iteration: 19000. Loss: 1.166224479675293. Accuracy: 66.3029687896473\n",
            "Iteration: 19500. Loss: 1.2373360395431519. Accuracy: 59.781781273788376\n",
            "Iteration: 20000. Loss: 0.5864057540893555. Accuracy: 66.93732555189038\n",
            "Iteration: 20500. Loss: 0.8313615918159485. Accuracy: 73.027150469424\n",
            "Iteration: 21000. Loss: 0.8220521807670593. Accuracy: 70.46434914996193\n",
            "Iteration: 21500. Loss: 0.9185487031936646. Accuracy: 65.49099213397615\n",
            "Iteration: 22000. Loss: 0.6032130122184753. Accuracy: 74.57498096929713\n",
            "Iteration: 22500. Loss: 0.43643859028816223. Accuracy: 76.47805125602639\n",
            "Iteration: 23000. Loss: 0.6710128784179688. Accuracy: 76.75716823141335\n",
            "Iteration: 23500. Loss: 0.6781941652297974. Accuracy: 66.68358284699315\n",
            "Iteration: 24000. Loss: 0.968417763710022. Accuracy: 65.41486932250697\n",
            "Iteration: 24500. Loss: 0.7433973550796509. Accuracy: 52.90535397107333\n",
            "Iteration: 25000. Loss: 0.4256851375102997. Accuracy: 83.30373001776199\n",
            "Iteration: 25500. Loss: 0.5401632785797119. Accuracy: 78.25425019030703\n",
            "Iteration: 26000. Loss: 0.5112601518630981. Accuracy: 82.44100482111139\n",
            "Iteration: 26500. Loss: 0.5596218705177307. Accuracy: 74.27048972342045\n",
            "Iteration: 27000. Loss: 0.8682196140289307. Accuracy: 60.59375792945953\n",
            "Iteration: 27500. Loss: 0.39302361011505127. Accuracy: 81.22303983760467\n",
            "Iteration: 28000. Loss: 0.5500597357749939. Accuracy: 74.82872367419436\n",
            "Iteration: 28500. Loss: 0.3141789436340332. Accuracy: 85.51129155036793\n",
            "Iteration: 29000. Loss: 0.3031448423862457. Accuracy: 82.6693732555189\n",
            "Iteration: 29500. Loss: 0.34896859526634216. Accuracy: 82.46637909160111\n",
            "Iteration: 30000. Loss: 0.49154171347618103. Accuracy: 66.83582846993149\n",
            "Iteration: 30500. Loss: 0.4166679084300995. Accuracy: 82.21263638670388\n",
            "Iteration: 31000. Loss: 0.3421022295951843. Accuracy: 80.43643745242325\n",
            "Iteration: 31500. Loss: 0.29903843998908997. Accuracy: 82.89774168992642\n",
            "Iteration: 32000. Loss: 0.27516812086105347. Accuracy: 84.75006343567622\n",
            "Iteration: 32500. Loss: 0.34674519300460815. Accuracy: 86.29789393554935\n",
            "Iteration: 33000. Loss: 0.2284071147441864. Accuracy: 88.58157827962447\n",
            "Iteration: 33500. Loss: 0.381832093000412. Accuracy: 79.8528292311596\n",
            "Iteration: 34000. Loss: 0.16056418418884277. Accuracy: 89.49505201725451\n",
            "Iteration: 34500. Loss: 0.5130910277366638. Accuracy: 71.35244861710225\n",
            "Iteration: 35000. Loss: 0.13529032468795776. Accuracy: 88.07409286983\n",
            "Iteration: 35500. Loss: 0.18962211906909943. Accuracy: 82.94849023090586\n",
            "Iteration: 36000. Loss: 0.5678631067276001. Accuracy: 80.48718599340269\n",
            "Iteration: 36500. Loss: 0.14859288930892944. Accuracy: 88.42933265668611\n",
            "Iteration: 37000. Loss: 0.1793220341205597. Accuracy: 89.19056077137782\n",
            "Iteration: 37500. Loss: 0.12212176620960236. Accuracy: 88.93681806648058\n",
            "Iteration: 38000. Loss: 0.14172908663749695. Accuracy: 90.20553159096676\n",
            "Iteration: 38500. Loss: 0.18065404891967773. Accuracy: 85.46054300938847\n",
            "Iteration: 39000. Loss: 0.20857664942741394. Accuracy: 85.86653133722406\n",
            "Iteration: 39500. Loss: 0.11232329159975052. Accuracy: 87.03374777975134\n",
            "Iteration: 40000. Loss: 0.4956783950328827. Accuracy: 66.15072316670896\n",
            "Iteration: 40500. Loss: 0.11618674546480179. Accuracy: 87.26211621415884\n",
            "Iteration: 41000. Loss: 0.8135616779327393. Accuracy: 81.40065973103273\n",
            "Iteration: 41500. Loss: 0.05009010061621666. Accuracy: 90.73839127125095\n",
            "Iteration: 42000. Loss: 0.6048223376274109. Accuracy: 76.78254250190307\n",
            "Iteration: 42500. Loss: 0.02518020011484623. Accuracy: 90.30702867292565\n",
            "Iteration: 43000. Loss: 0.049306586384773254. Accuracy: 90.43390002537427\n",
            "Iteration: 43500. Loss: 0.06508559733629227. Accuracy: 89.44430347627505\n",
            "Iteration: 44000. Loss: 0.03211754932999611. Accuracy: 90.20553159096676\n",
            "Iteration: 44500. Loss: 0.027537185698747635. Accuracy: 90.45927429586399\n",
            "Iteration: 45000. Loss: 0.019359827041625977. Accuracy: 90.7891398122304\n",
            "Iteration: 45500. Loss: 0.2863854765892029. Accuracy: 85.18142603400152\n",
            "Iteration: 46000. Loss: 0.03134914115071297. Accuracy: 89.90104034509008\n",
            "Iteration: 46500. Loss: 0.0322643406689167. Accuracy: 90.25628013194621\n",
            "Iteration: 47000. Loss: 0.0438072495162487. Accuracy: 90.05328596802842\n",
            "Iteration: 47500. Loss: 0.03354635834693909. Accuracy: 88.75919817305252\n",
            "Iteration: 48000. Loss: 0.017014438286423683. Accuracy: 90.76376554174068\n",
            "Iteration: 48500. Loss: 0.04283852130174637. Accuracy: 89.79954326313118\n",
            "Iteration: 49000. Loss: 0.02065294422209263. Accuracy: 90.25628013194621\n",
            "Iteration: 49500. Loss: 0.006979816127568483. Accuracy: 91.2205024105557\n",
            "Iteration: 50000. Loss: 0.024165906012058258. Accuracy: 90.20553159096676\n",
            "Iteration: 50500. Loss: 0.004353982862085104. Accuracy: 90.86526262369956\n",
            "Iteration: 51000. Loss: 0.08864705264568329. Accuracy: 89.1651865008881\n",
            "Iteration: 51500. Loss: 0.00315768551081419. Accuracy: 90.76376554174068\n",
            "Iteration: 52000. Loss: 0.028133584186434746. Accuracy: 91.2205024105557\n",
            "Iteration: 52500. Loss: 0.054548829793930054. Accuracy: 90.89063689418929\n",
            "Iteration: 53000. Loss: 0.011206703260540962. Accuracy: 90.73839127125095\n",
            "Iteration: 53500. Loss: 0.01267788466066122. Accuracy: 90.51002283684345\n",
            "Iteration: 54000. Loss: 0.0013567451387643814. Accuracy: 91.27125095153514\n",
            "Iteration: 54500. Loss: 0.0005080365808680654. Accuracy: 91.37274803349403\n",
            "Iteration: 55000. Loss: 0.0024312965106219053. Accuracy: 91.27125095153514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWc0oh5EiNyO"
      },
      "source": [
        "#Saving data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8XHBwDQ8ACF"
      },
      "source": [
        "sav(iter_,'final_best_iter_exp25')\n",
        "sav(loss_,'final_best_loss_exp25')\n",
        "sav(accuracy_,'final_best_acc_exp25')\n",
        "sav(model,'final_best_model_exp25')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-OWEopajZ4m"
      },
      "source": [
        "#Function for loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld4ZggBAHBou"
      },
      "source": [
        "def lod(s):\n",
        "  with open(s,'rb') as f:\n",
        "    a = pickle.load(f)  \n",
        "  return a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaxvmqQYjcqI"
      },
      "source": [
        "#Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfYepXxcH0Rp"
      },
      "source": [
        "iteration = lod('final_best_iter_exp25')\n",
        "loss = lod('final_best_loss_exp25')\n",
        "acc = lod('final_best_acc_exp25')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1BLZlWNjgbn"
      },
      "source": [
        "#Plotting loss vs iteration curve for experiment 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "1l5ZVardH3J5",
        "outputId": "41428b9d-a8b2-437b-8959-ce20ed63696c"
      },
      "source": [
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.plot(iteration, loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fbc99bf6510>]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfCliaIaj2f6"
      },
      "source": [
        "#Function for plotting table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI_l2qSpwQSA"
      },
      "source": [
        "def table(l1, l2, l3, columns):\n",
        "  plt.rcParams[\"figure.figsize\"] = [10, 5]\n",
        "  plt.rcParams[\"figure.autolayout\"] = True\n",
        "  \n",
        "  fig, axs = plt.subplots(1, 1)\n",
        "  data = np.column_stack((l1, l2, l3))\n",
        "  axs.axis('tight')\n",
        "  axs.axis('off')\n",
        "  the_table = axs.table(cellText=data, colLabels=columns, loc='center', cellLoc = 'center')\n",
        "  the_table.auto_set_font_size(False)\n",
        "  the_table.set_fontsize(18)\n",
        "  the_table.scale(1.5, 1.5)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0_5H_f51mQS"
      },
      "source": [
        "#table data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai2AQepcxG8T"
      },
      "source": [
        "l1= ['Hidden Layer', 'Nodes in Hidden Layer', 'Iteration', 'Learning Rate', 'Bath size', \n",
        "           'Optimizer', 'Loss Function', 'Activation Function', 'Loss', 'Accuracy (%)']\n",
        "l2 = [6,200,20000,.01,20,'SGD','Cross Entropy', 'ReLU', 2.1379,22.2786]\n",
        "l3 = [6,500,55000,.05,64,'SGD','Cross Entropy', 'ReLU', 0.0024,91.2712]\n",
        "columns = ['','Experiment 1', 'Experiment 2']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZ64A8mS1vea"
      },
      "source": [
        "#The comparison between two experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "Oxp8W95hxJIj",
        "outputId": "c24e0e61-6e32-4fcc-a51a-82c0386bc083"
      },
      "source": [
        "table(l1, l2,l3,columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3I10_kAj7BH"
      },
      "source": [
        "#Function for plotting table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxkJ0GxNUiGJ"
      },
      "source": [
        "def table2(l1, l2,l3,l4, columns):\n",
        "  plt.rcParams[\"figure.figsize\"] = [15, 2]\n",
        "  plt.rcParams[\"figure.autolayout\"] = True\n",
        "  \n",
        "  fig, axs = plt.subplots(1, 1)\n",
        "  data = np.row_stack((l1, l2,l3,l4))\n",
        "  axs.axis('tight')\n",
        "  axs.axis('off')\n",
        "  the_table = axs.table(cellText=data, colLabels=columns, loc='center', cellLoc = 'center')\n",
        "  the_table.auto_set_font_size(False)\n",
        "  the_table.set_fontsize(18)\n",
        "  the_table.scale(1.5, 1.5)\n",
        "  plt.show()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvvsA1sCj9_-"
      },
      "source": [
        "#Table data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3AzM1ECUGCq"
      },
      "source": [
        "l1 = ['1', 1, 2.1379, 22.27]\n",
        "l2 = ['1', 2, 0.0024, 91.27]\n",
        "l3 = ['2', 1, 0.4234, 86.78]\n",
        "l4 = ['2', 2, 0.0946, 89.94]\n",
        "\n",
        "columns = [ 'Dataset', 'Experiment', 'Loss', 'Accuracy (%)']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZtnJW_tj_rm"
      },
      "source": [
        "#Comparison of the results between the first dataset and second dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "Ui0LDvC4VtXO",
        "outputId": "5c79a8b5-99f2-485e-e9dd-b012036441f5"
      },
      "source": [
        "table2(l1,l2,l3,l4,columns)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x144 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}